
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>pcaexample2</title><meta name="generator" content="MATLAB 7.13"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2011-11-14"><meta name="DC.source" content="pcaexample2.m"><style type="text/css">

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head><body><div class="content"><h2>Contents</h2><div><ul><li><a href="#1">pcaexample2.m</a></li><li><a href="#2">Generate the data</a></li><li><a href="#3">Plot the original data</a></li><li><a href="#4">Do the PCA</a></li><li><a href="#5">Plot the first two components on to the original data</a></li><li><a href="#6">Bar plot of the eigenvalues</a></li><li><a href="#7">Plot the data projected into the first two dimensions</a></li></ul></div><h2>pcaexample2.m<a name="1"></a></h2><p>From A First Course in Machine Learning, Chapter 6. Simon Rogers, 01/11/11 [simon.rogers@glasgow.ac.uk] Second PCA example</p><pre class="codeinput">clear <span class="string">all</span>;close <span class="string">all</span>;
</pre><h2>Generate the data<a name="2"></a></h2><pre class="codeinput">Y = [randn(20,2);randn(20,2)+5;randn(20,2)+repmat([5 0],20,1);randn(20,2)+repmat([0 5],20,1)];
<span class="comment">% Add 5 random dimensions</span>
N = size(Y,1);
Y = [Y randn(N,5)];
<span class="comment">% labels - just used for plotting</span>
t = [repmat(1,20,1);repmat(2,20,1);repmat(3,20,1);repmat(4,20,1)];
</pre><h2>Plot the original data<a name="3"></a></h2><pre class="codeinput">symbs = {<span class="string">'ro'</span>,<span class="string">'gs'</span>,<span class="string">'b^'</span>,<span class="string">'kd'</span>};
figure(1);hold <span class="string">off</span>
<span class="keyword">for</span> k = 1:4
    pos = find(t==k);
    plot(Y(pos,1),Y(pos,2),symbs{k});
    hold <span class="string">on</span>
<span class="keyword">end</span>
</pre><img vspace="5" hspace="5" src="pcaexample2_01.png" alt=""> <h2>Do the PCA<a name="4"></a></h2><p>Subtract the means</p><pre class="codeinput">Y = Y - repmat(mean(Y,1),N,1);
<span class="comment">% Compute covariance matrix</span>
C = (1/N)*Y'*Y;
<span class="comment">% Find the eigen-vectors/values</span>
<span class="comment">% columns of w correspond to the projection directions</span>
[w,lam] = eig(C);
</pre><h2>Plot the first two components on to the original data<a name="5"></a></h2><pre class="codeinput">figure(1);hold <span class="string">off</span>
<span class="keyword">for</span> k = 1:4
    pos = find(t==k);
    plot(Y(pos,1),Y(pos,2),symbs{k});
    hold <span class="string">on</span>
<span class="keyword">end</span>
xl = xlim;
<span class="keyword">for</span> k = 1:2
    plot(xl,xl*w(1,k)/w(2,k),<span class="string">'k'</span>);
<span class="keyword">end</span>
ylim(xl);
</pre><img vspace="5" hspace="5" src="pcaexample2_02.png" alt=""> <h2>Bar plot of the eigenvalues<a name="6"></a></h2><pre class="codeinput">figure(1);
hold <span class="string">off</span>
bar(diag(lam));
xlabel(<span class="string">'Projection dimension'</span>);
ylabel(<span class="string">'Variance'</span>);
</pre><img vspace="5" hspace="5" src="pcaexample2_03.png" alt=""> <h2>Plot the data projected into the first two dimensions<a name="7"></a></h2><pre class="codeinput">X = Y*w(:,1:2);
figure(1);hold <span class="string">off</span>
<span class="keyword">for</span> k = 1:4
    pos = find(t==k);
    plot(X(pos,1),X(pos,2),symbs{k});
    hold <span class="string">on</span>
<span class="keyword">end</span>
</pre><img vspace="5" hspace="5" src="pcaexample2_04.png" alt=""> <p class="footer"><br>
      Published with MATLAB&reg; 7.13<br></p></div><!--
##### SOURCE BEGIN #####
%% pcaexample2.m
% From A First Course in Machine Learning, Chapter 6.
% Simon Rogers, 01/11/11 [simon.rogers@glasgow.ac.uk]
% Second PCA example
clear all;close all;

%% Generate the data
Y = [randn(20,2);randn(20,2)+5;randn(20,2)+repmat([5 0],20,1);randn(20,2)+repmat([0 5],20,1)];
% Add 5 random dimensions
N = size(Y,1);
Y = [Y randn(N,5)];
% labels - just used for plotting
t = [repmat(1,20,1);repmat(2,20,1);repmat(3,20,1);repmat(4,20,1)];
%% Plot the original data
symbs = {'ro','gs','b^','kd'};
figure(1);hold off
for k = 1:4
    pos = find(t==k);
    plot(Y(pos,1),Y(pos,2),symbs{k});
    hold on
end

%% Do the PCA
% Subtract the means
Y = Y - repmat(mean(Y,1),N,1);
% Compute covariance matrix
C = (1/N)*Y'*Y;
% Find the eigen-vectors/values
% columns of w correspond to the projection directions
[w,lam] = eig(C);

%% Plot the first two components on to the original data
figure(1);hold off
for k = 1:4
    pos = find(t==k);
    plot(Y(pos,1),Y(pos,2),symbs{k});
    hold on
end
xl = xlim;
for k = 1:2
    plot(xl,xl*w(1,k)/w(2,k),'k');
end
ylim(xl);
%% Bar plot of the eigenvalues
figure(1);
hold off
bar(diag(lam));
xlabel('Projection dimension');
ylabel('Variance');

%% Plot the data projected into the first two dimensions
X = Y*w(:,1:2);
figure(1);hold off
for k = 1:4
    pos = find(t==k);
    plot(X(pos,1),X(pos,2),symbs{k});
    hold on
end

##### SOURCE END #####
--></body></html>