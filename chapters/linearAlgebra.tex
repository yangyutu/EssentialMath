\begin{refsection}
\startcontents[chapters]
\chapter{Linear Algebra \& Matrix Analysis}\label{ch:linearalgebra}
%\minitoc

\printcontents[chapters]{}{1}{}
\section{Linear equation theory}\label{ch:linearalgebra:sec:linearequation}
\subsection{Homogeneous equations}
\begin{lemma}[solutions to homogeneous equations]\index{linear equation}
Given a system of linear equations given by $Ax=0,A\in \R^{m\times n}$, there are exactly two possibilities:
\begin{enumerate}
    \item unique solution of zero.
    \item infinitely many solutions.
\end{enumerate}
Moreover, if $m<n$, there are exactly one possibilities:
\begin{enumerate}
    \item infinitely many solutions.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) 0 vector is always one solution; (2) For the proof of infinitely many solutions, it can be showed using linear map theory(rank-nullity theorems) later. $A$ can be viewed as a linear map from larger space to smaller space, and the null space of $A$ has dimensionality greater than 0.
\end{proof}


\begin{lemma}[subspace nature of solutions]
Consider a system of linear equations given by $Ax=0,A\in \R^{m\times n}$. if it has more than one solutions, then it has infinitely many solutions and all the solutions form a subspace, called null space.
\end{lemma}
\begin{proof}
use linearity of $A$.
\end{proof}





\subsection{Non-homogeneous equations}
\begin{definition}[consistence]\index{consistence}
A system of $m$ linear equations in $n$ unknowns,i.e., $Ax=b$ is said to be a \textbf{consistent} system if it possesses at least one solution; If there are no solutions, the system is said to be \textbf{inconsistent} system. 
\end{definition}

\begin{lemma}[consistence criterion] The system of linear equations $Ax=b,A\in \R^{m\times n}$ is consistent if one of the following is satisfied:
\begin{itemize}
    \item $rank[A|b]=rank[A]$. 
    \item $A$ is full row rank, then $rank[A|b] = rank[A] = m$.
    \item $b$ can be constructed via a linear combination of basic columns in $A$.
\end{itemize}
\end{lemma}




\begin{theorem}[solutions to non-homogeneous equation]
Consider a system of linear equations given by $Ax=b,A\in \R^{m\times n}$, there are exactly three possibilities:\cite{meyer2000matrix}
\begin{enumerate}
    \item one unique solution if $Ax=0$ only has zero vector as the solution.
    \item no solutions if inconsistent.
    \item infinitely many solutions if $Ax = 0$ has infinitely many solutions; that is, the solution will form an \textbf{affine set/space}.
\end{enumerate}
\end{theorem}
\begin{proof}
directly from above theorems.
\end{proof}



\begin{corollary}[uniqueness criterion]
The system of linear equations $Ax=b,A\in \R^{m\times n}$ has a unique solution if  the following conditions are satisfied:
\begin{itemize}
    \item $rank[A|b]=rank[A] = n$. 
    \item the homogeneous system $Ax=0$ only have the trivial solution.
\end{itemize}
\end{corollary}

\subsection{Overdetermined vs. underdetermined}
\begin{definition}
In $Ax = b,A\in \R^{m\times n}$, it is \textbf{overdetermined} if $m > n$; it is underdetermined if $m < n$. 
Note that \textbf{overdetermined/underdetermined is not related to consistence}. Therefore, usually, there are six types of linear equation systems:
\begin{enumerate}
    \item underdetermined and consistent
    \item underdetermined and inconsistent
    \item exactly determined and consistent
    \item exactly determined and consistent
    \item overdetermined and consistent
    \item overdetermined and inconsistent
\end{enumerate}
\end{definition}

\begin{lemma}[solution properties of overdetermined system]
An overdetermined system $Ax = b$ will have three possibilities:
\begin{enumerate}
\item no solution if inconsistent;
\item unique solution if consistent $Ax = 0$ only has the trivial  solution;
\item infinitely many solutions if consistent $Ax = 0$ has infinitely many solutions.
\end{enumerate}
\end{lemma}

\begin{lemma}[solution properties of underdetermined system]\label{ch:linearalgebra:th:solutionunderterminedsystem}
An underdetermined system $Ax = b$ will have twe possibilities:
\begin{enumerate}
\item no solution if inconsistent;
\item infinitely many solutions if consistent 
\end{enumerate}
\end{lemma}
\begin{proof}
note that $Ax = 0, m < n$ has infinitely many solution.
\end{proof}

\subsection{Solution methods}\label{ch:linearalgebra:sec:linearEquationSolutionMethod}
\begin{lemma}[orthogonal projection and rank-nullity decomposition]
	Let $A\in \R^{m\times n},rank(A) = n\leq m$. Define
	$$P \triangleq  A(A^TA)^{-1}A^T.$$
It follows that 
\begin{itemize}
	\item $P\in \R^{m\times m}$ and $rank(P) = n$.
	\item $P$ is an orthogonal projection matrix and $Px \in \cR(A)$.
	\item $I- P$ is an orthogonal projection matrix, $rank(I-P) = m-n$, and $(I-P)x \in \cR(A)$.
	\item consider $b\in \R^m$, and let $b = b_\cR + b_\cN$ be the rank-nullity decomposition(\autoref{ch:linearalgebra:th:RangeNullVectorDecomposition}) such that $b_\R \perp b_{\cN}$. Then $$b_\cR = Pb, b_\cN = (I-P)b.$$
\end{itemize}
\end{lemma}
\begin{proof}
See \autoref{ch:linearalgebra:sec:OrthogonalComplementarySubspacesProjections},\autoref{ch:linearalgebra:th:ProjectorPropertyFromPseudoInverse}.
\end{proof}



\begin{lemma}[tall thin matrix, full column rank]\label{ch:linearalgebra:th:linearEquationSolutionMethodTallThinMatrixFullColumnRank}
Let $A\in \R^{m\times n},rank(A) = n\leq m$ and $b\in \R^m$. The minimum 2-norm error solution to $Ax = b$ is given by 
	$$x^* = (A^TA)^{-1}A^Tb.$$
\begin{itemize}
	\item If $b\in \cR(A)$, then $\min_{x\in\R^n} \norm{Ax - b}_2^2 = 0$;that is $Ax^* = b$.
	\item If $b\notin \cR(A)$, then $\min_{x\in\R^n} \norm{Ax - b}_2^2 = \norm{b_\cN}_2^2$;that is $Ax^* = b_\cR \neq b$.
\end{itemize}
\end{lemma}
\begin{proof}
(1) Note that $x^*$ can be obtained by using the first-order optimality condition of $\min_{x\in\R^n} \norm{Ax - b}_2^2$. Note that when $b\in\cR(A)$, $Pb = b$.
(2) When $b\notin\cR(A)$, we have
$$\norm{Ax - b}_2^2 = \norm{Ax - b_\cR - b_\cN}_2^2 = \norm{Ax - b_\cR}_2^2 + \norm{b_\cN}_2^2$$
where we have used the property that $b_\cN\in\cN(A^T)$, and
$\cN(A^T) \perp \cR(A)$.
\end{proof}


\begin{lemma}[fat short matrix, full row rank]\label{ch:linearalgebra:th:linearEquationSolutionMethodFatShortMatrixFullRowRank}
	Let $A\in \R^{m\times n},rank(A) = m\leq n$ and $b\in \R^m$. Further assumes that $A$ can be partitioned as $A = [A_1 A_2]$ such that $A_1$ contains $m$ linearly independent columns.
\begin{itemize}
	\item The solution set to $Ax = b$ is given by 
	$$x = x_p + Ny.$$
	where $N\in \R^{m\times (n-m)}$ with columns being the basis of $\cN(A)$ and $x_p$ a particular solution given by
	$$x_p = \begin{bmatrix}
	x_p^1 \\ 0
	\end{bmatrix},x_p^1 = (A_1^TA_1)^{-1}A_1^Tb.$$
	\item The solution with the minimum 2-norm length $x_m$ is given by
	$$x_m = (I - P_\cN)x_p = P_\cR x, $$
	where $x$ is an arbitrary element in the solution set, $P_\cN = N(N^TN)^{-1}N^Tx, P_\cR$ is the projection matrix onto $\cR(A^T)$. Note that $P_\cR \neq A_1(A_1^TA_1)^{-1}A_1^T.$
	This solution is equivalent to the unique minimizer of 
	$$\min_{x\in\R^n} \norm{x}_2^2, ~subject~to~ Ax = b.$$
	\item \textbf{(Psedudoinverse:)}The solution with the minimum 2-norm length $x_m$ is given by
	$$x_m = A^T(AA^T)^{-1}b.$$
\end{itemize}	
\end{lemma}
\begin{proof}
(1)Partition the original linear equation as
$$[A_1 ~ A_2] \begin{bmatrix}
x_1 \\
x_2
\end{bmatrix} = b.$$
Then $A_1x_1 = b - A_2x_2$. Set $x_2 = 0$ and we use the full column rank thin tall linear equation result to solve $x_1$.
(2) To calculate the minimum length element, we seek the solution to 
$$\min_{y} \frac{1}{2}\norm{x + Ny}_2^2.$$
The first order condition gives
$$y^* =-(N^TN)^{-1}N^Tx.$$
(3) It can be showed that $x_m$ is the solution by
$$Ax_m = AA^T(AA^T)^{-1}b = b.$$
To show $x_m$ is the solution with minimum length, we can solve
$$\min_{y\in\R^n} f(y) = \norm{A^T(AA^T)^{-1}b + Ny}^2.$$
The objective function given by (let $R = A^T(AA^T)^{-1}$)
\begin{align*}
f &= p^TR^TRp + y^TN^TNy + 2y^TN^TRp \\
&=p^TR^TRp + y^TN^TNy 
\end{align*}
which will achieve minimum value at $y = 0$. That is, $x_m$ is the minimum length solution.
\end{proof}



\begin{corollary}[tall thin matrix, not full column rank]
	Let $A\in \R^{m\times n},rank(A) = r < n \leq m$ and $b\in \R^m$. Further assumes that $A$ can be partitioned as $A = [A_1 ~ A_2]$ such that $A_1$ contains $r$ linearly independent columns. The minimum 2-norm error solution to $Ax = b$ has the following properties:
\begin{itemize}
	\item If $b\in \cR(A)$, the the solution to $Ax = b$ exists but always not unique.
	\item If $b\in \cR(A)$, then $\min_{x\in\R^n} \norm{Ax - b}_2^2 = 0$;there exists a set of minimizers $x^*$ such that $Ax^* = b$. The full set of solution/minimizers are $x = x_p + Ny,$ where $$x_p = \begin{bmatrix}
	x_p^1 \\ 0
	\end{bmatrix},x_p^1 = (A_1^TA_1)^{-1}A_1^Tb.$$ 
	where $N\in \R^{m\times (n-r)}$ with columns being the basis of $\cN(A)$.
\end{itemize}
\end{corollary}



\begin{corollary}[fat short matrix, not full row rank]
	Let $A\in \R^{m\times n},rank(A) =r < m\leq n$ and $b\in \R^m$. Further assumes that $A$ can be partitioned as $A = [A_1 ~ A_2]$ such that $A_1$ contains $r$ linearly independent columns. The minimum 2-norm error solution to $Ax = b$ has the following properties:
	\begin{itemize}
		\item If $b\in \cR(A)$, the the solution to $Ax = b$ exists but always not unique.
		\item If $b\in \cR(A)$, then $\min_{x\in\R^n} \norm{Ax - b}_2^2 = 0$;there exists a set of minimizers $x^*$ such that $Ax^* = b$. The full set of solution/minimizers are $x = x_p + Ny,$ where $$x_p = \begin{bmatrix}
		x_p^1 \\ 0
		\end{bmatrix},x_p^1 = (A_1^TA_1)^{-1}A_1^Tb.$$  

		\item If $b\notin \cR(A)$, then $\min_{x\in\R^n} \norm{Ax - b}_2^2 = \norm{b_\cN}_2^2$;there exists a set of minimizer $x^*$ such that $Ax^* = b_\cR \neq b$. The full set of minimizers are $x = x_p + Ny,$ where $$x_p = \begin{bmatrix}
		x_p^1 \\ 0
		\end{bmatrix},x_p^1 = (A_1^TA_1)^{-1}A_1^Tb_\cR.$$  
	\end{itemize}
\end{corollary}

\begin{lemma}[minimum error minimum length solution via SVD theory]\label{ch:linearalgebra:linearEquationSolutionSVDmethod}
Let $A\in \R^{m\times n}$  and $b\in \R^m$. Let $A$ have SVD decomposition \autoref{ch:linearalgebra:th:SVD} given by
$$
A = [U_1 ~ U_2]\begin{bmatrix}
\Sigma ~ 0\\
0 ~ 0
\end{bmatrix} \begin{bmatrix}
V_1^T\\
V_2^T
\end{bmatrix} 
.$$

Then the minimizers of 
$$\min_{x\in\R^n} \norm{Ax - b}_2^2$$	
is a set
$$x^* = V_1\Sigma^{-1}U_1^T b + V_2y.$$
Among the set, the element with the minimum 2-norm length is
$$x^*_m = V_1\Sigma^{-1}U_1^Tb.$$	
\end{lemma}
\begin{proof}
(1)
\begin{align*}
\norm{Ax - b}_2^2 &= \norm{[U_1~U_2]Ax - [U_1~U_2]b}_2^2 \\
&=\norm{\begin{bmatrix}
	\Sigma ~ 0\\
	0 ~ 0
	\end{bmatrix} \begin{bmatrix}
	V_1^T\\
	V_2^T\end{bmatrix} x - \begin{bmatrix}
	U_1^Tb\\
	U_2^Tb
	\end{bmatrix}}_2^2 \\
&= \norm{\begin{bmatrix}
	\Sigma V_1^T x - U_1^Tb \\
	-U_2^Tb
	\end{bmatrix}}_2^2
\end{align*}	
We can solve $x$ from 	$\Sigma V_1^T x = U_1^Tb$. 
Also note that $V_2y$ does not contribute the objective function.
(2) Use the minimum length result in \autoref{ch:linearalgebra:th:linearEquationSolutionMethodFatShortMatrixFullRowRank}, the minimum length is given by
$$x^*_m = V_1V_1^Tx^* = V_1V_1^TV_1\Sigma^{-1}U_1^Tb + V_1V_1^TV_2y = V_1\Sigma^{-1}U_1^Tb,$$
where we use the fact that $V_1V_1^T$ is the orthogonal projection matrix to $\cR(A^T)$, and $V_1^TV_2 = 0$.
\end{proof}


\begin{theorem}[solution for general linear system, recap]
	Let $A \in \R^{m\times n}$ with SVD $A = U\Lambda V^T$ and $rank(A) = r$. Let $A^+  = V \Lambda^+ U^T$ be its pseudoinverse. If the linear system $Ax = b$ has solution, then the solution is given by
	$$x = A^+b + (I_n - A^+A)z, z\in \R^n.$$
	where $I_n - A^+A$ being the $\cN(A)$ basis matrix.
	Among all solutions, the minimum norm/length solution is $A^+b$.
	
	If $Ax = b$ does not have a solution, then
	$$x = A^+b + (I_n - A^+A)z, z\in \R^n.$$
	is the solution set of minimum error, with $A^+b$ being the minimum norm/length solution.
\end{theorem}
\begin{proof}
\autoref{ch:linearalgebra:th:solutionForGeneralLinearSystemPseudoinverseMethod}.
\end{proof}


\subsection{Error bounds in numerical solutions}
\subsection{Condition number}
\begin{definition}\index{condition number}
(condition number) For a square matrix $A$, we define the condition number as
$$cond(A) = \begin{cases}
        \norm{A}\norm{A^{-1}}\\
        \infty, A ~\textbf{is singular}
\end{cases}$$
\end{definition}

\begin{lemma}[basic properties of condition number]\cite{Robinson2015nonlinear}
The condition number has the following properties:
\begin{itemize}
    \item $cond(I) = 1$
    \item $cond(A) \geq 1$
    \item $cond(\alpha A) = cond(A), \forall \alpha \neq 0$
    \item If $D$ is diagnoal, then
    $$cond(D)=\frac{\max{\abs{d_{ii}}}}{\min{\abs{d_{ii}}}}$$
\end{itemize}
\end{lemma}

\subsection{Error bounds}
\begin{theorem}
If $A$ is nonsingular, $Ax=b$, and $A\tilde{x}=\tilde{b}$, then \cite{Robinson2015nonlinear}
$$\frac{\norm{\tilde{x}-x}}{x} \leq cond(A)\frac{\norm{\tilde{b}-b}}{b}$$
\end{theorem}
\begin{proof}
 Let $\Delta x = \tilde{x}-x,\Delta b = \tilde{b}-b$, then 
$$\frac{\norm{\Delta x}}{\norm{x}} = \frac{A^{-1}\Delta b}{b} \leq \frac{\norm{\Delta b}\norm{A^{-1}}}{\norm{x}} \leq cond(A)\frac{\norm{\tilde{b}-b}}{b}$$
where we use $\norm{b}\leq \norm{A}\norm{x}$ in the last step.	
\end{proof}


\section{Vector space theory}\label{ch:linearalgebra:sec:vector-space-theory}
\subsection{Vector space}\index{vector space}
\begin{definition}[vector space]
Let $\F$ be a field. \textbf{A vector space over a field} $\F$ is a set $V$ together with two operation called addition and scalar multiplication  The addition is a function $+: V\times V\to V $ such that $x + y = x+y\in V, x,y\in V$; the scalar multiplication is a function $\times: \F\times V \to V$ such that $\lambda\times x = \lambda x, \lambda\in F, x\in V$. The addition and the scalar multiplication are required to satisfy the following axioms.
\begin{enumerate}
\item (addition associativity) $(u+v)+w = u + (v+w), \forall u,v,w\in V$
\item (addition community) $u+v = v + u, \forall u,v \in V$
\item (additive identity) there exist an element $0 \in V$ such that $0+v=v,\forall v\in V$
\item (additive inverse element) for each $v\in V$, there exists a $u\in V$, such that $u+v=0$
\item (scalar multiplication identity) $1 u = u,\forall u\in V$
\item (associativity of scalar multiplication) $r(su)=(rs)u, \forall r,s\in \F,u\in V$ 
\item (distributivity of scalar sums) $(r+s)u = ru+su, \forall r,s\in \F,u\in V$
\item (distributivity of vector sums) $r(u+v)=ru+rv,, \forall r\in \F,u,v\in V$
\end{enumerate}

Note that elements of $V$ are called vectors and elements of $\F$ are called scalars.
\end{definition}


\begin{example}[common vector space examples]\hfill
\begin{itemize}
	\item Let $F$ be a field, then $F$ is a vector space over $F$ with addition and multiplication defined in $F$. Therefore, $\R$ is a vector space over $R$; $\Q$ is a vector space over $\Q$.
	\item Let $F$ be a field, and $F'\subseteq F$ is a vector space over $F'$. Therefore, $\R$ is a vector space over $\Q$; $\C$ is a vector space over $\R$.
	\item $\R^n$ is vector space over $\R$.
	\item The set $$M_{m\times n} F = \{m\times n ~matrices~with~entries~in~F\}$$
is an $F$-vector space equipped with component-wise addition and common scalar multiplication between scalar and matrices.
  
\end{itemize}	
\end{example}




\begin{example}[function spaces as vector space]
Note that the first two are infinite dimensional vector space.\cite{krim2015geometric}
\begin{itemize}
    \item Let $p>=1$ be a real number. The function space $L^p([a,b])$ of all real or complex-valued measurable functions defined by
    $$L^p([a,b])=\{f:[a,b]\rightarrow \mathbb{F} | \int_a^b|f(t)|^p dt < \infty\}$$
    is a vector space with point-wise addition and scalar multiplication.
    \item The function space $\mathcal{C}([a,b])$ of all continuous, real- or complex-valued functions defined on the interval $[a,b]:$
    $$\mathcal{C}([a,b])=\{f:[a,b]\rightarrow \mathbb{F}|f is continuous\}$$
    is a vector space with point-wise addition and scalar multiplication.
    
    \item Let $p>=1$ be a real number. The function space $l^p(\mathbb{Z})$ of infinite, $p$th order summable sequences or discrete function $f[n]$ (i.e. $f$ only take discrete integer as argument) defined by 
    $$l^p(\mathcal{Z})=\{f([n])|\sum_{n=-\infty}^{n=+\infty}|f[n]^p<\infty\}$$
    $$L^p([a,b])=\{f:[a,b]\rightarrow \mathbb{F} | \int_a^b|f(t)|^p dt < \infty\}$$
    is a vector space with component-wise addition and scalar multiplication.
\end{itemize}
Other similar examples include:
    \begin{itemize}
        \item $\mathcal{C}^k(\Omega)$ of all $k$ times continuously differentiable functions
        \item $\mathcal{C}^\infty(\Omega)$ of all smooth functions
    \end{itemize}
\end{example}

\subsection{Subspace}\index{subspace}
\begin{definition}[closed]
A subset $U$ of a vector space $V$ is said to be \textbf{closed under addition and scalar multiplication} if \begin{enumerate}
\item $u_1+u_2 \in U, \forall u_1,u_2 \in U$
\item $\lambda u \in U, \forall u\in U, \lambda \in F$
\end{enumerate}
\end{definition}

\begin{definition}[subspace]
\cite[64]{Howlett2001@vector}A subset $U$ of a vector space $V$ is called a subspace of $V$ if $U$ is itself a vector space relative to addition and scalar multiplication inherited from $V$.
\end{definition}

\begin{theorem}[subspace]
If $V$ is a vector space and $U$ a subset of $V$ which is \textbf{nonempty, and closed} under addition and scalar multiplication, the $U$ is subspace of $V$.
\end{theorem}
\begin{proof}
The key is to prove the existence of additive identity and inverse. Since the set $U$ is nonempty and closed, then $0 u = 0 \Rightarrow 0$ exists. The additive inverse: $-1 u$. For other properties of the addition and multiplication operation, they will inherit from $V$.
\end{proof}


\begin{theorem}[subspace condition, alternative]
\cite[18]{axler2015linear}
If $V$ is a vector space and $U$ a subset of $V$, then $U$ is the subspace if and only if it is closed under addition and scalar multiplication and $U$ contains the additive identity $0$ of $V$.
\end{theorem}

\begin{example}\hfill
\begin{itemize}
	\item If $V$ is a vector space, then $V$ is a subspace of $V$. The set $\{0\}$ is called the \textbf{zero subspace} of $V$.
	\item $\R^2$ is vector space over $\R$. The set $L$ defined by
	$$L = \{(x,y)\in\R^2|y = mx\},m\in \R^{++}$$
	is a subspace of $\R^2$.
	\item $\R^2$ is vector space over $\R$. The set $L$ defined by
	$$L = \{(x,y)\in\R^2|y = x + 1\}$$
	is not a subspace of $\R^2$. Take $(x_1,y_1)\in L, (x_2,y_2)\in L$, then
	$$(x_1,y_1)+ (x_2,y_2) = (x_1+x_2,y_1+y_2) \notin L,$$
	since $y_1+y_2\neq x_1+x_2+1$.
\end{itemize}	
\end{example}


\subsection{Sum and direct sum}\index{sum}\index{direct sum}\label{ch:linearalgebra:sec:vectorspace:subsec:sumdirectsum}
\begin{definition}[sum of subsets]
\cite[20]{axler2015linear} Suppose $U_1,U_2,...,U_m$ are subsets of $V$. The \textbf{sum} of $U_1,U_2,...,U_m$ is the set
$$U_1+U_2++...+U_m = \{u_1+u_2+...+u_m:u_1\in U_1,u_2 \in U_2,...,u_m\in U_m\}$$
\end{definition}

\begin{lemma}[sum of subspace is the smallest containing subspace]
\cite[20]{axler2015linear} Suppose $U_1,U_2,...,U_m$ are subspaces of $V$.
Then sum $U_1+U_2+,...,U_m$ is the smallest subspace of $V$ containing  $U_1,U_2,...,U_m$.
\end{lemma}
Proof: First, $U_1+U_2+,...,U_m$ is a subspace; second, every subspace in $V$ containing $U_1,U_2,...,U_m$ will contain $U_1+U_2+,...,U_m$.


\begin{definition}[direct sum]
\cite[21]{axler2015linear}\index{direct sum}
Suppose $U_1,U_2,...,U_m$ are subspaces of $V$:
The sum $U_1+U_2+,...,U_m$ is called a direct sum if each element of $U_1+U_2+,...,U_m$ can be written \textbf{uniquely} as $u_1+u_2+...+u_m, u_i\in U_i,\forall i$. Then $U_1+U_2+,...,U_m$ will be written as $U_1\oplus U_2\oplus,...,U_m$.
\end{definition}

\begin{remark}[not every sum is direct sum]
Not every sum of subspaces are direct sum due to the possible linear dependence of basis of subspaces.
\end{remark}

\begin{lemma}[direct sum criterion for sum to be direct sum]\label{ch:linearalgebra:th:SubspaceDirectSumCriterion}
\cite[23]{axler2015linear}Suppose $U_1,U_2,...,U_m$ are subspaces of $V$.
Then sum $U_1+U_2+,...,U_m$ is a direct sum if and only if the only way to write $0$ as a sum $u_1+u_2+...+u_m, u_i\in U_i$ is to set each $u_j = 0$.

Or equivalently, sum $U_1+U_2+,...,U_m$ is a direct sum if and only if $u_1,u_2,...,u_m, u_i\in U_i$ are linearly independent.
\end{lemma}
\begin{proof}
(1) suppose $U_1+U_2+,...,U_m$ is a direct sum, then the definition of direct sum implies that there is an unique way to write 0 as a sum of $u_1+u_2+...+u_m$, since $u_i=0$ will satisfy, then this is the unique way.
(2) Let $u \in U_1+U_2+,...,U_m$ be expressed as $u=u_1+u_2+...+u_m = v_1+v_2+...+v_m$(i.e. two ways). Now we prove that if the only way to write $0$ as a sum $u_1+u_2+...+u_m$ is to set each $u_j = 0$ will imply that $u_i=v_i$ in the above expression. We have $0=\sum(u_i-v_i),(u_i-v_i)\in U_i$, because the only way is to set $u_i-v_i = 0$, then we have $u_i=v_i$.	
\end{proof}

\begin{corollary}
Suppose $U$ and $W$ are subspaces of $V$. Then $V$ and $W$ is a direct sum if and only if $U\cap W = \{0\}$.
\end{corollary}


\begin{theorem}[dimensions of a sum]\label{ch:linearalgebra:th:dimensionofsum}
\cite[47]{axler2015linear}\cite[214]{meyer2000matrix} If $U_1$ and $U_2$ are subspaces of a finite-dimensional vector space, then
$$dim(U_1+U_2) = dim(U_1)+dim(U_2)-dim(U_1\cap U_2)$$
Moreover, if it is a direct sum,
$$dim(U_1\oplus U_2) = dim(U_1)+dim(U_2)$$
\end{theorem}
\begin{proof}
Let $\{z_1,z_2,...,z_t\}$ be the basis of $U_1\cap U_2$, let $B_X = \{z_1,...,z_t,x_1,...,x_m\}$ be the basis of $U_1$, let $B_Y = \{z_1,...,z_t,y_1,...,y_m\}$ be the basis of $U_2$. Then $B_X\cup B_Y$ will span $U_1+U_2$. It can be shown that $B_X\cup B_Y$ are linearly independent
\end{proof}


\subsection{Basis and dimensions}\index{basis}\index{dimensionality}
\begin{definition}[linear independence]\index{linear independence}
We say that vectors $v_1,v_2,...,v_n \in V$ are linearly independent if the \textbf{only} solution of $\sum_{i=1}^n c_i v_i = 0$ is $c_i = 0, \forall i$.
\end{definition}

\begin{definition}[span]\index{span}
The set of all linear combinations of a list of vectors $v_1,v_2,...,v_n$ in $V$ is called the \textbf{span} of $v_1,v_2,...,v_n$, denoted as $span(v_1,v_2,...,v_n)$, given as
$$span(v_1,v_2,...,v_n) = \{a_1v_1+a_2v_2+...+a_nv_n:a_1,a_2,..a_n \in \F\}$$
\end{definition}

\begin{lemma}[polynomials are linear independent]\label{ch:linearalgebra:th:LinearIndependenceOfPolynomials}\hfill
\begin{itemize}
	\item The polynomials $1,t,t^2,...,t^n$ are linearly independent.
	\item The matrix 
	$$P = \begin{bmatrix}
	1 & x_1 & x_1^2 & \cdots & x_1^n\\ 
	1 & x_2 & x_2^2 & \cdots & x_2^n \\ 
	\vdots & \vdots & \ddots & \vdots\\ 
	1 & x_m & x_m^2 & \cdots & x_m^n
	\end{bmatrix}, m \geq (n+1).$$
	and the matrix $P$ has independent columns.
\end{itemize}	
\end{lemma}
\begin{proof}
(1)	
Suppose we have some weights $C_0,C_1,...,C_n\in \R$ such that
$$C_0 + C_1 t + C_2 t^2 + \cdots + C_n t^n = 0, \forall t$$
If some of the coefficients are non-zero, then this is a polynomial with degree $\leq n$, therefore has at most $n$ roots(based on the fundamental theorem of algebra(\autoref{ch:topics-in-abstract-algebra:th:fundamentalthalgebra})). However, here has $\infty$ roots(the above equation holds for all $t$). Therefore, all coefficients have to be 0. 
(2) Suppose we have some weights $C_0,C_1,...,C_n\in \R$ such that
$$C_0 + C_1 t + C_2 t^2 + \cdots + C_n t^n = 0, t=x_1,x_2,...,x_m.$$If some of the coefficients are non-zero, then this is a polynomial with degree $\leq n$, therefore has at most $n$ roots(based on the fundamental theorem of algebra(\autoref{ch:topics-in-abstract-algebra:th:fundamentalthalgebra})). However, here has $m\geq n+1$ roots(the above equation holds for $t=x_1,x_2,...,x_m$). Therefore, all coefficients have to be 0. 
\end{proof}


\begin{definition}[basis and dimension]\hfill
\begin{itemize}
	\item A linearly independent set of vectors that span the vector space $V$ is called the \textbf{basis} for $V$. 
	\item The cardinality of the basis is called the \textbf{dimension} of the vector space. 
\end{itemize}	
\end{definition}



\begin{definition}[finite and infinite dimensional vector space] If a vector space has a finite-sized basis, then it is a \textbf{finite dimensional vector space}; otherwise it is a infinite dimensional vector space,i.e., no finite-sized basis can be found to span the vector space. 
\end{definition}


\begin{example}\hfill
	\begin{itemize}
		\item $\R^n$ has dimension $n$ since it has standard basis $\{e_1,e_2,...,e_n\}.$
		\item When $\C$ is a vector space over $\R$, it has basis $\{1,i\}$ and dimension 2; When $\C$ is a vector space over $\C$, it has basis $\{1\}$ and dimension 1; When $\C$ is a vector space over $\Q$, it has dimension $\infty$(we can finite-sized basis to span $C$ when the scalars are taken from $\Q$).  
	\end{itemize}	
\end{example}
\begin{lemma}[Basis is maximum linearly independent set]
Suppose that $S=\{v_1,v_2,v_3,â€¦,v_t\}$ is a finite set of vectors which spans the vector space $V$. Then any set of $t+1$ or more vectors from $V$ is linearly dependent.
\end{lemma}
\begin{proof}
Let $A$ be the basis of size $m$, let $B$ be the basis of size $n$. Suppose $m>n$, we have $A = BM, M \in \F^{n,m}$ since $A$ can be spanned by $B$. Consider the linear equation $Mx = 0$, it will has a non-trivial solution since there are more variables than equations. Then $Ax = BMx = 0$, that is a non-trival linear combination of $A$ columns leads to  0. Therefore, columns in $A$ are linearly dependent.	
\end{proof}
 

\begin{theorem}[All basis have the same length]
Let $V$ be a finite-dimensional vector space, then all its basis, that is, linearly independent set that span the space, will have the same length.
\end{theorem}
\begin{proof}
Let $A$ be the basis of size $m$, let $B$ be the basis of size $n$. Suppose $m>n$, then based on above lemma, $A$ cannot be an independent set.
\end{proof}
 

\begin{theorem}[extending linearly independent set]
Let $V$ be a vector space, let $S=\{v_1,v_2,...,v_m\}$ be a linearly independent set. Then if $v\in V$, but $v\notin span(S)$, then the set $S\cup v$ is linearly independent. 
\end{theorem}

\begin{proof}
Suppose linearly dependent, then $v$ can be expressed as linear combination, which contradicts $v \notin span(S)$.
\end{proof}


\begin{theorem}[proper subspace has smaller dimension]
If $U,W$ are subspaces of vector space $V$, and $U\subsetneq W$, then $dim(U) < dim(W)$.
\end{theorem}
\begin{proof}
First, we must have $dim(U)\leq dim(W)$ since basis of $W$ will span $U$. To show $dim(U)<dim(W)$, suppose $dim(U)=dim(W)=t$, then there exist $t$ linearly independent vectors in $U$ and therefore in $W$, therefore spanning $W$, therefore $U=W$, which is a contradiction.
\end{proof}



\begin{theorem}[\textbf{equal dimension implies equal subspace}]\label{ch:linearalgebra:th:equaldimensionimpliesequalsubspace}
If $U,W$ are subspaces of vector space $V$, and $U\subseteq W$, if $dim(U) = dim(W)$, then $U = W$.
\end{theorem}
\begin{proof}
suppose $U\neq W$, then there exists $w\in W$ such that $w\notin U$, let $B$ be the basis of $U$, then $B\cup w (\in W)$ will form a linear independent set(from above theorem), then $dim(W) > dim(U)$, which is contradiction. 
\end{proof}
\begin{corollary}
If $U$ is a subspace of vector space $V$, and $dim(U) = dim(V)$, then $U = V$.
\end{corollary}

\begin{remark}
The above two theorem will be important in proving 'onto' property of linear maps. 
\end{remark}


\subsection{Complex vector space vs. real vector space}

\begin{lemma}[same dimensionality of real and complex vector space]
The complex vector space $\C^n$ has a basis $\{v_i\}$ of size $n$ with all real entries. In other words, $\C^n$ and $\R^n$ have the same dimensionality.
\end{lemma}
\begin{proof}
Consider a standard basis $E=\{e_i\}$ that can span $\R^n$. For a complex number $c$, it can always be written as $c =a+b i, a,b\in \R^n$, then $c = Ev_a + Ev_b i = E(v_a+v_b i)$.	
\end{proof}

\begin{remark}
This lemma shows that for a complex vector space $\C^n$, it is always possible to choose a set of real-valued basis.	
\end{remark}

\begin{lemma}[convert complex-valued basis to real-valued basis]
Given a complex-valued basis $\{u_i\}$ for $\C^n$, then its conjugate $\{\conj{u_i}\}$ is also a basis. Moreover, a real-valued basis can be created by $\{u_i + \conj{u_i}\}$
\end{lemma}
\begin{proof}
Suppose $\{\conj{u_i}\}$ is not linearly independent, then there exist nonzero $a_1,a_2,...,a_n$ such that
$$a_1\conj{u_1}+a_2\conj{u_2}...+a_n\conj{u_n} = 0.$$
then this set of coefficients $\{\conj{a_1}, ...,\conj{a_n}\}$ will also make 
$$\conj{a_1}u_1+\conj{a_2}u_2...+\conj{a_n}u_n = 0.$$	
\end{proof}



\begin{mdframed}
\textbf{Caution!}
\begin{itemize}
    \item $\C^n$ over $\R$ is a vector space, but this vector space cannot have real-valued basis. (because complex-valued vectors cannot be spanned)
    \item Moreover, the dimensionality is $2n$, with basis $\{e_1,ie_1,...\}$
\end{itemize}

\end{mdframed}





\section{Linear maps \& linear operators} 
\subsection{Basic concepts of linear maps}\index{linear maps}
\begin{mdframed}
\textbf{Notations} in this section
\begin{itemize}
    \item $\F$ denotes $\R$ or $\C$
    \item $V$ and $W$ denote vector spaces over $\F$
    \item $\mathcal{L}(V,W)$ denotes all linear maps from $V$ to $W$.
\end{itemize}
\end{mdframed}

\begin{definition}[linear map]
\cite[54]{axler2015linear}A \textbf{linear map} from $V$ to $W$ is a function $T: V\to W$ with the following properties:
$$T(au+bv) = aT(u) + bT(v), \forall a,b \in \F,\forall u \in U, v\in W$$
\end{definition}

\begin{lemma}[linear map maps zero element to zero element]
Let $T\in \cL(V,W)$, then $T(0_V) = 0_W$.
\end{lemma}
\begin{proof}
	$T(0_V) = T(a+-a) = T(a) - T(a) = 0_W$.
\end{proof}

\begin{remark}
This lemma provides a necessary condition for us to judge whether a function is a linear map or not. 
\end{remark}

\begin{definition}[null space]\index{null space}
\cite[59]{axler2015linear} For $T\in \mathcal{L}(V,W)$, the null space of $T$ is
$$\cN(T) = \{x\in V: Tx  = 0\}$$
\end{definition}

\begin{lemma}[null space as a subspace]
The null space of a linear map $T\in \cL(V,W)$ is a subspace of $V$.
\end{lemma}
\begin{proof}
	directly from linearity of $T$.
\end{proof}


\begin{lemma}[zero null space is equivalent to 1-1 ]\label{ch:linearalgebra:th:linearMapsZeroNullSpaceEquivalentToOneOne}
Let $T\in \mathcal{L}(V,W)$, then $T$ is injective(1-1) if and only if $\cN(T) = \{0\}$
\end{lemma}
Proof: (1) Suppose $Tx = Ty \Rightarrow T(x-y) = 0$, if $\cN(T)=\{0\}$, then $x=y$, therefore 1-1; (2) The converse(1-1 implies nullity): let $v\in \cN(T)$, then $T(v) = 0 = T(0) \Rightarrow v = 0$ due to 1-1. (another proof, suppose $dim(\cN(T)) > 0$, then $T(x-y)$ cannot lead to $x = y$).

\begin{definition}[range]
For $T\in \cL(V,W)$, the range of $T$ is defined as
$$\cR(T) = \{Tv ,\forall v\in V\}$$
\end{definition}

\begin{definition}[surjective,onto]
A function $T:V\to W$ is called surjective/onto if its range equals $W$.
\end{definition}

\begin{lemma}\index{range}
The range of a linear map $T\in \cL(V,W)$ is a subspace of $W$.
\end{lemma}
\begin{proof}
	directly from linearity of $T$.
\end{proof}


\begin{lemma}[surjective criterion]
$T\in \cL(V,W)$ is surjective if $dim(\cR(T)) = dim(W)$
\end{lemma}
\begin{proof}
directly from theorems in subspace that equality in dimension leads to equality in subspaces\autoref{ch:linearalgebra:th:equaldimensionimpliesequalsubspace}.
\end{proof}
 

\begin{example}[Examples of linear maps/operators]
Examples are \cite{krim2015geometric}
\begin{itemize}
    \item Operator $T: L^2([a,b])\rightarrow L^2([a,b])$ defined as $T f(t)=t f(t)$
    \item The differentiation operator $D:\mathcal{C}^1(\R)\rightarrow \mathcal{C}(\R)$
     \item The integration operator $T:\mathcal{C}(\R) \rightarrow \mathcal{C}^1(\R)$
     \item The trace operator. 
     \item The convolution operator $H: L^1(\R)\rightarrow L^1(\R)$
     \item The Laplacian $\Delta: \mathcal{C}^\infty(\R^n)\rightarrow \mathcal{C}^\infty(\R^n)$
\end{itemize}

Counter-examples are:
\begin{itemize}
    \item The determinant operator.
\end{itemize}
\end{example}

\subsection{Fundamental theorem of linear maps}
\begin{theorem}[\textbf{fundamental theorem of linear maps,Rank-nullity theorem}]\index{fundamental theorem of linear maps}\index{Rank-nullity theorem}\label{ch:linearalgebra:th:ranknullitytheorem}
	\cite[62]{axler2015linear}Suppose $V$ is finite-dimensional and $T\in \cL(V,W)$, then $\cR(T)$ is finite dimensional and 
	$$dim(V) = dim(\cN(T)) + dim(\cR(T))$$
\end{theorem}
\begin{proof}
	Denote $dim(V)=m+n,dim(\cN(T))=m$. Let $u_1,u_2,..u_m$ be the basis of $\cN(T)$, let $u_1,u_2,..,u_m,v_1,v_2,...,v_n$ be the basis of $V$. Then for any $v\in V, v= \sum_{i=1}^m a_i u_i + \sum_{j=1}^n b_j v_j$, $Tv\in \cR(T), Tv = \sum_{i=1}^n b_j Tv_j$, therefore the $\cR(T) \subset span(Tv_1,Tv_2,...,Tv_n)$. Further, we need to prove $Tv_1,Tv_2,...,Tv_n$ are linearly independent set: suppose it is not, then there are nonzero coefficients $c_i$s such that  $$\sum_{i=1}^n c_i Tv_i = 0 = T(\sum_{i=1}^n c_i v_i) = 0$$
	which suggest $\sum_{i=1}^n c_i v_i \in \cN(T)$, however, by assumption $v_i$ is linearly independent of basis of $\cN(T)$, therefore $\sum_{i=1}^n c_i v_i = 0$, however contradict $v_i$ are linear independent.
\end{proof}


\begin{remark}[relation to fundamental theorem of linear algebra]
	For matrix, $dim(\cR(A)) = dim(\cR(A^T))$, therefore it is consistent with fundamental theorem of linear algebra.
\end{remark}


\begin{corollary}\cite[64]{axler2015linear}
\begin{itemize}
	\item \textbf{mapping into smaller space implies injective(non 1-1):} Suppose $V$ and $W$ are finite-dimensional vector space such that $dim(V) > dim(W)$, then no linear maps $T$ from $V$ to $W$ is injective, that is $dim(\cN(T)) > 0$.
	\item \textbf{mapping into larger space implies non-surjective(non onto):} Suppose $V$ and $W$ are finite-dimensional vector space such that $dim(V) < dim(W)$, then no linear maps $T$ from $V$ to $W$ is surjective, that is $dim(\cR(T)) < dim(W)$.
\end{itemize}	
\end{corollary}
\begin{proof}
(1)	
	\begin{align*}
	dim(\cN(T)) &=  dim(V) - dim(\cR(T)) \\
	&\geq dim(V) - dim(W) > 0
	\end{align*}
	where we use $dim(\cR(T)) \leq dim(W)$.
(2)
		\begin{align*}
	dim(\cR(T)) &=  dim(V) - dim(\cN(T)) \\
	&\leq dim(V) < dim(W) 
	\end{align*}
	where we use $dim(\cN(T)) \geq 0$.
\end{proof}


\begin{remark}[application in linear equation theory]
	A simple application is under-determined linear homogeneous system has infinitely many solutions.
\end{remark}

\begin{theorem}[existence of inverse linear map]\label{ch:linearalgebra:th:existenceOfInverseLinearMaps}\cite[80]{axler2015linear}
\begin{itemize}
	\item A linear map $T: V\to V$ has the following equivalent statement:
	\begin{itemize}
		\item $T$ has an inverse $T^{-1}: V\to V$
		\item $T$ is onto, i.e. $\cR(T) = V$(\autoref{ch:linearalgebra:th:linearMapsZeroNullSpaceEquivalentToOneOne}).
		\item $T$ is 1-1; or equivalently $\cN(T) = \{0\}$.
	\end{itemize}
	\item 	
	$T\in \cL(V,W)$ has an inverse $T^{-1}\in \cL(W,V)$ if  and only if $T$ is onto and 1-1. 
\end{itemize}	
\end{theorem}
\begin{proof}
(1) (a) implies (b)(c)	The existence of inverse requires that $T$ is 1-1 and onto. (b) implies (c)
use rank-nullity theorem(\autoref{ch:linearalgebra:th:ranknullitytheorem}) such that $dim(V) = dim(\cN(T)) + dim(\cR(T))$ to prove. If $T$ is 1-1, then $\cN(T) = \{0\}$(\autoref{ch:linearalgebra:th:linearMapsZeroNullSpaceEquivalentToOneOne}). Therefore $dim(\cR(T)) = dim(V) \Leftrightarrow \cR(T) = W$. (c) implies (a)(b) use rank-nullity again we get $\dim(\cN(T)) = 0$, implying 1-1. 1-1 and onto implies the existence of inverse.
(2) forward: if $T$ is onto and 1-1 then $T^{-1}$ exists be definition. converse: if $T^{-1}$ exists, then $T^{-1}$ is a linear map(\autoref{ch:linearalgebra:th:BasicPropertiesOfIsomorphism}), therefore $T^{-1}\in \cL(W,V)$.
\end{proof}



\subsection{Isomorphism}
\subsubsection{Homomorphism \& isomorphism}\index{isomorphism}
\begin{definition}[homomorphism]\index{homomorphism}
Suppose vector space $V, W$, a function $T:V\rightarrow W$ is called \emph{linear map}, also known as \textbf{homomorphism} if $$T(u+v) = T(u)+T(v), \forall u,v\in V.$$	
\end{definition}

\begin{remark}
	A homomorphism is a mapping that preserves selected structure between two algebraic structures, with the structure to be preserved being given by the naming of the homomorphism.\cite{wiki:homomorphism}
	For example,
	\begin{itemize}
		\item A group homomorphism is a homomorphism that preserves the group structure. It may equivalently be defined as a semigroup homomorphism between groups.
		\item A linear map is a homomorphism that preserves the vector space structure, namely the Abelian group structure and scalar multiplication.
	\end{itemize}	
\end{remark}


\begin{definition}[isomorphism]\hfill
\begin{itemize}
	\item 	An \textbf{isomorphism} between two vector spaces $V \text{and} ~W$ is a map $f:V \rightarrow W$ that
	\begin{enumerate}
		\item $f$ is one-to-one and onto (bijective)
		\item preserves structures: If $v_1,v_2 \in V$ then $$f(v_1+v_2) = f(v_1) + f(v_2)$$ and if $v \in V$ and $r \in \mathbb{F}$, then $$f(rv)=rf(v)$$
	\end{enumerate}
\item We say $V$ and $W$ are \textbf{isomorphic} to each other if there exists an isomorphism $T:V\to W$.
\end{itemize}	 
\end{definition}

\begin{remark}\hfill
	\begin{itemize}
		\item The isomorphism is further abstracted as bijective linear operators.
		\item Homomorphism differs from isomorphism in the bijection. 
	\end{itemize}
\end{remark}

\begin{lemma}[basic properties of isomorphisms]\label{ch:linearalgebra:th:BasicPropertiesOfIsomorphism}\hfill
Consider a linear transformation $T$ from $V$ to $W$. We assume $T$ is bijection such that $T^{-1}$ exists.
\begin{itemize}
	\item If $T$ is an isomorphism, then so is $T^{-1}$.
	\item A linear transformation $T$ from $V$ to $W$ is an isomorphism if and only if
	$$\cN(T) = \{0\}, \cR(T) = W.$$
	\item Consider an isomorphism $T$ from $V$ to $W$. If $f_1,f_2,...,f_n$ is a basis of $V$, then $T(f_1),T(f_2),...,T(f_n)$ is a basis of $W$.
	\item If $V$ and $W$ are isomorphic, then $dim(V) = dim(W)$. 
\end{itemize}	
\end{lemma}
\begin{proof}
(1) We first how that $T^{-1}$ is linear. Consider $f,g$ in $V$ and $k$ and $m$ in $\F$. Then
\begin{align*}
T^{-1}(kf + mg) &= T^{-1}(TT^{-1}(kf) + TT^{-1}(mg))\\
&=T^{-1}T(T^{-1}(kf) + T^{-1}(mg)) \\
&=T^{-1}(kf) + T^{-1}(mg)\\
&=kT^{-1}(f) + mT^{-1}(g)
\end{align*}	
From the definition of function map, we know that $\cR(T^{-1}) = V$. and $T^{-1}$ is 1-1.
(2) see \autoref{ch:linearalgebra:th:existenceOfInverseLinearMaps}.
(3) For any $g$ in $W$, there exists $T^{-1}(g)$ in $V$ such that
$$T^{-1}(g) = c_1f_1 + c_2f_2 + \cdots +c_nf_n,$$
because $f_i$s span $V$. Applying $T$ on both sides we get 
$$g = c_1T(f_1) + c_2T(f_2) + \cdots + c_nT(f_n),$$
that is, $T(f_1),T(f_2),...,T(f_n)$ span $W$. 
To show that $T(f_1),T(f_2),...,T(f_n)$ are linear independent, we consider a relation
$$b_1T(f_1) + b_2T(f_2) + \cdots + b_nT(f_n) = 0,$$
or
$$T(b_1f_1 + b_2f_2 + \cdots + b_nf_n) = 0.$$
Since $\cN(T) = \{0\}$, we have $$b_1f_1 + b_2f_2 + \cdots + b_nf_n = 0.$$
Further because $f_1,f_2,...f_n$ are linear independent, we have
$b_1=...=b_n = 0$. Therefore, $T(f_1),T(f_2),...,T(f_n)$ are linear independent.
(4) directly from (3). Note that dimension is the cardinality of the basis.
\end{proof}



\subsection{Coordinate}

\begin{definition}[coordinate vector, coordinate map]\index{coordinate vector}\index{coordinate map}\hfill
\begin{itemize}
	\item Let $B = \{b_1,b_2,...,b_n\} $ be a basis of a vector space $V$ such that any element $x\in V$ can be represented by
	$$x = x_1b_1 + x_2b_2+\cdots + x_nb_n.$$
	
	Then the \textbf{coordinate vector} of $x$ with respect to basis $B$ is defined be a vector in $\R^n$, denoted by $[x]_B$, such that $$[x]_B = \begin{bmatrix}
	x_1\\
	x_2\\
	\cdots \\
	x_n
	\end{bmatrix}.$$
	Note that we have
	$$x = B[x]_B,$$
	where here we use $B$ to denote the matrix with columns of $b_1,b_2,...,b_n$.
	\item The \textbf{coordinate map} associated with basis $B$ of vector space $V$ is a linear map $\phi_B: V\to \R^n$.
	\item The \textbf{inverse coordinate map} is such that
	$$\phi^{-1}([x]_B) = x_1b_1 + x_2b_2 + ... +x_nb_n: \R^n\to V.$$	
\end{itemize}	
\end{definition}

\begin{lemma}[]
\begin{itemize}
	\item Let $V$ be a finite dimensional vector space with basis $B$. Then the coordinate map $\phi_B$ is an isomorphism.
	\item Any finite dimensional vector space $V$ is isomorphic to the Euclidean space $\R^{dim(V)}$. 
\end{itemize}	
	
\end{lemma}


\begin{example}\hfill
\begin{itemize}
	\item The coordinate map associated with the basis $\{1,t,t^2,...,t^n\}$ of the polynomial vector space is the isomorphism
	$$a_0+a_1t+a_2t^2 + \cdots + a_nt^n \to (a_0,a_1,...,a_n)^T.$$	
	\item The coordinate map associated with the basis
	$$\{\begin{bmatrix}
	1&0\\
	0&0
	\end{bmatrix},\begin{bmatrix}
	0&1\\
	0&0
	\end{bmatrix},\begin{bmatrix}
	0&0\\
	1&0
	\end{bmatrix},\begin{bmatrix}
	0&0\\
	0&1
	\end{bmatrix}\},$$
	is the isomorphism
	$$\begin{bmatrix}
	a&b\\
	c&d
	\end{bmatrix} \to (a,b,c,d)^T$$
	
\end{itemize}	

\end{example}


\subsection{Linear maps and matrices}

\begin{lemma}[matrices for linear opeartors]
\cite[146]{axler2015linear} Suppose $T\in \cL(V)$ and $v_1,v_2,...,v_n$ is a basis of $V$. The matrix $M(T)$ of $T$ with respect to this basis is required to satisfied
$$T(v_j) = \sum_{i=1}^n m_{ij}v_i $$
\end{lemma}


\subsection{Change of basis and similarity}\index{change of basis}
\subsubsection{Change of basis for coordinate vector}
\begin{lemma}[change of basis for vector representation]
Given a basis $B_1$ and a basis $B_2$, a coefficient vector $v_1$ with respect to $B_1$ has the coefficient vector $v_2$ given as 
$$B_1v_1 = B_2v_2 \Leftrightarrow v_2 = B_2^{-1}B_1v_1$$
specifically, if $B_1 = E$,i.e., $B_1$ is the standard basis $I$, then
$$v_2 = B_2^{-1}v_1$$
\end{lemma}

\begin{mdframed}
\textbf{Note:}\\
When we write $v\in V$ as a tuple $(v_1,v_2,...,v_n)$, we are \textbf{implicitly use the standard basis as the basis}. 
\end{mdframed}

\subsubsection{Change of basis for linear maps}

\begin{theorem}[change of basis for linear operator, similarity transform]\label{ch:linearalgebra:th:changeofbasis}\index{similarity transform}
Suppose $T\in \cL(V)$ has a matrix representation $M_1$ with respect to $B_1$, then the matrix representation $M_2$ with respect to $B_2$ is given as
$$M_2 = (B_2^{-1}B_1)M_1(B_2^{-1}B_1)^{-1} = B_2^{-1}B_1M_1(B_1^{-1}B_2)$$
specifically, if $B_1 = E$,i.e., $B_1$ is the standard basis, then
$$M_2 = B_2^{-1}M_1B_2$$
and 
$$M_1 = B_2M_2B_2^{-1}$$
where $M_1$ is the matrix representation in standard basis.
\end{theorem}

\begin{proof}
Because $M_1$ will map a vector with respect to $B_1$ to a vector with respect to $B_1$, we need to first transform the input vector with respect to $B_2$ to be with respect to $B_1$ and transform the input vector with respect to $B_1$ to be with respect to $B_2$. 
\end{proof}




\begin{remark}[interpret matrix diagonalizing]
Consider a square matrix $A$ can be written as (via eigendecomposition) 
$$A = P\Lambda P^{-1} \Leftrightarrow P^{-1}\Lambda P$$
then we interpret $P$ as the new basis, and in this new basis representation, the linear operator has diagonal representation.
\end{remark}




\begin{remark}
Change of basis will not affect linear mapping, which is in nature an associative relationship between input space and output space. 
\end{remark}

\begin{lemma}[change of basis for subspace representation]\label{ch:linearalgebra:th:changeBasisSubspaceRepresentation}
Let $A$ and $B$ be two $n\times p$ matrices, both with full rank and $\cR(A) = \cR(B)$. Then there exists $A = BC$, with $C$ being the $p\times p$ nonsingular matrix.	
\end{lemma}
\begin{proof}
Because $\cR(A) = \cR(B)$, then for each column $b_i$ of $B$, it should be able to write as
$$b_i = Ac_i,c_i\in \R^p, i = 1,2,...,p.$$
Therefore, $B = AC$. To show $C \in \R^{p\times p}$ is nonsingular, we use the matrix product inequality
$$p = rank(A) = rank(AC)\leq \min(rank(A),rank(C)) \implies rank(C) = p.$$ 	
\end{proof}


\subsubsection{Similarity}

\begin{definition}[similarity of matrices]
Two square matrices $A,B\in \R^{n\times n}$ are said to be \textbf{similar}, denote by $A\sim B$, if there exists an invertible matrix $P\in \R^{n\times n}$ such that
$$A = PBP^{-1}.$$	
\end{definition}

\begin{lemma}[similarity is an equivalence relation]
Matrices similarity is an equivalence relation; that is, 	
\begin{itemize}
	\item (reflexivity) $A\sim A$.
	\item (symmetric) If $A\sim B$, then $B\sim A$.
	\item (transitivity) If $A\sim B, B\sim C$, then $A\sim C$.
\end{itemize}	
\end{lemma}
\begin{proof}
(1) $A = I^{-1}AI$.
(2) $A \sim B$ implies $A = PBP^{-1}$, which further implies $B = P^{-1}AP$. Since $P^{-1}$ is invertible, we have
$B\sim A$.
(3) Suppose
$A = PBP^{-1}, B = QCQ^{-1}, $
then $A = PQCQ^{-1}P^{-1} = GQG^{-1}$ where $G = PQ$. Therefore, $A\sim C$.
\end{proof}

\subsection{Eigenvalue,eigenvector and invariant subspaces}

\begin{definition}[invariant subspace]\index{invariant subspace}\cite[132]{axler2015linear}
Let $V$ be a vector space, let $T\in \cL(V)$and let $U$ be a subspace of $V$. $U$ is called an invariant subspace under $T$ if
$$Tu\in U,\forall u\in U$$
\end{definition}

\begin{definition}[eigenvector of linear operator]\index{eigenvector}\cite[134]{axler2015linear}
Suppose $T\in \cL(V)$ and $\lambda \in \F$ is an eigenvalue of $T$. A vector $v\in V$ is called an eigenvector of $T$ corresponding to $\lambda$ if $v\neq 0$ and $Tv = \lambda v$.
\end{definition}

\begin{remark}
eigenspace is 1-dimensional invariant subspace.
\end{remark}


\begin{theorem}[conditions for upper-triangular matrix]\cite[148]{axler2015linear}
Suppose $T\in \cL(V)$ and $v_1,v_2,...,v_n$ is a basis of $V$. Then the following are equivalent:
\begin{itemize}
    \item the matrix $T$ with respect to the basis $v_1,v_2,...,v_n$ is upper triangular
    \item $Tv_j \in span(v_1,...,v_j),\forall j$
    \item $span(v_1,...,v_j)$ is invariant under $T$ for each $j=1,...,n$
\end{itemize}
\end{theorem}

\begin{proof}
(1) equivalent to (2) Based on the definition of matrix of linear operator, we have $Tv_j = \sum_{i=1}^j m_{ij}v_j$; (2) to (3) $Tv_1 \subseteq span(v_1) \subseteq span(v_1,...,v_j), ..., Tv_j \subseteq span(v_1,...,v_j)$; (3) to (2) is directly from definition of invariant space. 
\end{proof}



\begin{theorem}[existence of upper triangle matrix]\label{ch:linearalgebra:th:existenceofupertrianglematrix}
\cite[149]{axler2015linear} Suppose $V$ is a finite dimensional \textbf{complex} vector space and $T\in \cL(V)$. Then $T$ has an upper-triangle matrix with respect to some basis of $V$. 
\end{theorem}



\begin{theorem}
Let $A\in \C^{n\times n}$. There exists a non-singular $S$ such that $T = S^{-1}AS$ is upper-triangular. $T$ has the same eigenvalues as $A$ with the same multiplicity, and the eigenvalues of $T$ will appear on the diagonal.
\end{theorem}







\begin{definition}
\cite[137]{axler2015linear} Suppose $T\in \cL(V)$ and $U$ is a subspace of $V$ invariant under $T$, \begin{itemize}
    \item The restriction operator $T|_U \in \cL(U)$ is defined by 
    $$T|_U(u) = Tu,\forall u \in U$$
    \item The quotient operator $T/U \in \cL(V/U)$ is defined by
    $$(T/U)(v+U) = Tv + U$$
\end{itemize}
\end{definition}

\section{Dual space}
\subsection{Dual space and its basis}
\begin{definition}[dual space]
Given any vector space $V$ over a field $F$, the \textbf{(algebraic) dual space} $V^*$ is defined as \textbf{the set of all linear maps} $\phi: V \to  \F$, where $\phi$ satisfies
$$\phi(\alpha v_1+\beta v_2) = \alpha \phi(v_1) + \beta \phi(v_2),$$ 
for all $\alpha,\beta\in \F$ and all $v_1,v_2\in V$.
\end{definition}

\begin{example}For a linear functional $f:\PP_3\to \R$, we have
	$$f(c_0+c_1t+c_2t^2+c_3t^3) = c_0f(1)+c_1f(t)+c_2f(t^2)+c_3f(t^3) = c_0a_0+c_1a_1+c_2a_2+c_3a_3,$$
	where $a_0 = f(1), a_1=f(t),a_2=f(t^2), a_3=f(t^3).$ Note that for different linear functional $f$, we will have different $a_0,a_1,a_2,a_3$ coefficients.
	
	Therefore, the dual space of $\PP_3$ is given by
	$$\PP_3^* = \{f(c_0+c_1t+c_2t^2+c_3t^3)=c_0a_0+c_1a_1+c_2a_2+c_3a_3,a_0,a_1,a_2,a_3\in \R\}.$$
\end{example}


\begin{lemma}[dual space is vector space]
The dual space $V^*$ itself becomes a vector space over $F$ 
\begin{itemize}
	\item the zero element in $V^*$ is a zero function 0 such that
	$$0(v) = 0 \forall v\in V.$$
	\item the associated addition and scalar multiplication 
	satisfying:
	\begin{align*}
	(\psi + \phi)(x) &= \psi(x) + \phi(x)\\
	(a\phi)(x) &= a\phi(x)
	\end{align*}
	for all $\psi,\phi \in V^*, x\in V, a\in F.$ 
\end{itemize}	
\end{lemma}


\begin{lemma}
Suppose that $V$ is finite-dimensional and let $(v_1,v_2,...,v_n)$ be a basis of $V$. For each $i,i=1,2,...,n$, define a linear functional $f_i: V\to \F$	by setting
$$f_i(v_j) = \begin{cases*}
1, ~if~i=j\\
0,~if~i\neq j
\end{cases*}.$$
Then 
\begin{itemize}
	\item $(f_1,f_2,...,f_n)$ is a basis of $V^*$, called the \textbf{dual basis} of $(v_1,v_2,...,v_n)$.
	\item $dim(V) = dim(V^*).$
\end{itemize}
\end{lemma}
\begin{proof}
(1)	
(a)First we check that $(f_1,...,f_n)$ is linearly independent. Suppose that $a_1,a_2,...,a_n\in \F$ such that
$$a_1f_1 + ... +a_nf_n = 0$$.
Note that the 0 on the right denotes the zero functional which sends
all $v\in V$ to  $0 \in F$; the left the summation of functionals. The above equality above is an equality of function maps, which should hold for all $v\in V$.

In particular, evaluating both sides on $v_i$ gives
$$(a_1f_1 +...+ a_nf_n)(v_i) = a_1f_1(v_i) +...+a_nf_n(v_i) = a_i.$$
on the left and on the right we have $$0(v_i) = 0.$$
Therefore, 
$$a_i = 0\forall i=1,2,...,n.$$

Therefore, $f_1,f_2,...,f_n$ are linearly independent.
(b) Now we show that $(f_1,...,fn)$ spans $V^*$. 
Let $f\in V^*$. We claim that there exists $b_1,...,b_n\in \F$ such that
$$f = b_1f_1+b_2f_2+...+b_nf_n.$$
Note that this the equality for maps; therefore both sides should give the same result for all $v\in V$.
By linearity, it suffices to check that this is true on the basis $v_1,v_2,...,v_n$. Indeed, for each $i$,
we have
$$(b_1f_1+...+b_nf_n)(v_i) = b_1f_1(v_i)+...+b_nf_n(v_i) = b_i, \forall i=1,2,...,n.$$
Therefore, if we select $b_i = f(v_i)$, then
for any $f\in V^*, $
$$f(v_i) = (b_1f_1+b_2f_2+...+b_nf_n)(v_i),\forall i=1,2,...,n.$$
Thus, $f$ and $b_1f_1 +...+b_nf_n$ agree on the basis, so
we conclude that they will agree on all elements in  $V$. Hence $(f_1,f_2,...,f_n)$ spans $V^*$.
(2) Note that the dimension of $V^*$ is the number of basis functions.
\end{proof}

\begin{example}
If $V$ is $\R^2$ with basis $e_1=(1,0)$ and $e_2=(0,1)$, then the associated dual basis $e^1$ and $e^2$ are one-forms such that $$e^1(e_1) = 1, e^1(e_2)=0,e^2(e_1)=0, e^2(e_2)=1.$$	
\end{example}

\begin{example}
If $V$ is $\R^3$ with standard basis $e_1,e_2,e_3$, then the linear functions
\begin{align*}
f_1(x_1e_1+x_2e_2+x_3e_3) &= x_1\\
f_2(x_1e_1+x_2e_2+x_3e_3) &= x_2\\
f_3(x_1e_1+x_2e_2+x_3e_3) &= x_3
\end{align*}	
is the basis of $(\R^3)^*.$
\end{example}

\begin{example}
	If $V$ is $\PP_3$ with standard basis $\{1,t,t^2,t^3\}$, then the linear functions
	\begin{align*}
	f_1(a + bt + ct^2 + dt^3) &= a\\
	f_2(a + bt + ct^2 + dt^3) &= b\\
	f_3(a + bt + ct^2 + dt^3) &= c\\
	f_3(a + bt + ct^2 + dt^3) &= d
	\end{align*}	
	is the basis of $(\PP_3)^*.$
\end{example}


\begin{definition}[transpose of a linear map]
If $f:V\to W$ is a linear map, then the transpose $f^*\to W^*\to V^*$ is defined by
$$f^*(\phi) = \phi\cdot f,$$
for every $\phi\in W^*.$

The resulting functional $f^*(\phi)$ in $V^*$ is called the pullback of $\psi$ along $f$.	
\end{definition}

\subsection{Dual transformation}

\begin{definition}[dual transformation]
A linear\textbf{ transformation} $T:V\to W$ induces a transformation
$$T^*(f)= f\circ T: W^*\to V^*, f\in V^*$$
between the dual spaces, called the \textbf{dual transformation}. 
Specifically, the formula means that $T^*(f)$ is a linear function on $V$, and the value of the function at $v\in V$ is
$$T^*(f)(v) = f\circ T(v).$$
\end{definition}

\begin{remark}
A linear transformation is mapping between vector spaces; its associated dual transformation is mapping between dual spaces(that is, it transforms a linear functional to another linear functional).	
\end{remark}

\begin{lemma}[dual transformation is linear]
Let $T:V\to W$ be a linear transformation. Let $T^*$ be its associated dual transformation. Then $T^*:V^*\to W^*$ is linear. 		
\end{lemma}
\begin{proof}
Let $f,g\in V^*$. Then we want to show $$T^*(f+g) = T^*(g) + T^*(f), T^*(\alpha f) = \alpha T^*(f),\forall \alpha\in \F.$$
We have
\begin{align*}
T^*(f+g)(v) &= (f+g)\circ T(v) \\
&= (f+g)(Tv) \\
&= f(Tv) + g(Tv) \\
&= (T^*f)(v) + (T^*g)(v) \\
&=(T^*f + T^*g)(v), \forall v\in V.
\end{align*}
and
\begin{align*}
T^*(\alpha f)(v) &= (\alpha f)\circ T(v) \\
&= (\alpha f)(Tv) \\
&= \alpha f(Tv) \\
&= (T^*f)(v) + (T^*g)(v) \\
&=\alpha(T^*f)(v), \forall v\in V.
\end{align*}
\end{proof}


\begin{definition}[matrix of dual transformation]
Consider a linear transformation $T:\R^n\to \R^m$ is represented by $T(x) = Ax, A\in \R^{m\times n}$. Then its dual transformation $T^*: R^{m}\to \R^n$.	
\end{definition}


\section{Fundamental theorem of linear algebra}
\subsection{Basics of ranks}
\begin{definition}[rank and nullity]\index{rank}\index{nullity}\index{null space}\cite[129]{banerjee2014linear}\hfill
	\begin{itemize}
		\item The \textbf{rank of a matrix} is the dimensionality of its column space/range, i.e., the number of linearly independent columns or the number of linearly independent rows(as we will see in \autoref{ch:linearalgebra:th:fundamentalTheoremOfRanks}).
		\item The \textbf{nullity of a matrix} is the dimensionality of its null space. 
	\end{itemize}	
\end{definition}

\begin{lemma}[rank of matrix products]\label{ch:linearalgebra:th:rankOfMatrixProducts}\hfill
\begin{itemize}
	\item Let $A$ and $B$ be square matrices of the same size, then 
	$$rank(AB) = dim(\cR(AB)) \leq dim(\cR(A)) = rank(A)$$
	$$dim(\cN(AB)) \geq dim(\cN(A))$$
	\item For any compatible matrix $A,B$, $$rank(AB)\leq \min(rank(A),rank(B)).$$
	\item Let $A\in \R^{m\times n}$ and $B\in \R^{n\times k}$. If $rank(B) = n$, then
	$$rank(AB) = rank(A).$$
	\item Let $A\in \R^{m\times n}$ and $C\in \R^{l\times m}$. If $rank(C) = m$, then
	$$rank(CA) = rank(A).$$
\end{itemize}	
\end{lemma}
\begin{proof}
	(1)Let $y\in \cR(AB)$, then there exists a $x$, such that $y = ABx = Az,z=Bx$. Then $y$ is also in $\cR(A)$, and therefore $\cR(AB)\subseteq \cR(A)$ and thus
	$dim(\cR(AB)) \leq dim(\cR(A))$. The inequality for null space dimensionality can be proved via \autoref{ch:linearalgebra:th:rankNullitytheorem}.
	(2) Note that similar to (1), we have $rank(AB) \leq rank(A)$. Take the transpose, we have
	$rank(AB) = rank(B^TA^T)\leq rank(B^T) = rank(A)$, where we use the fundamental theorem of ranks(\autoref{ch:linearalgebra:th:fundamentalTheoremOfRanks}) such that $rank(A) = rank(A^T)$.
	(3) If $y\in \cR(A)$, then there exists a $b\in \R$ such that $y = Ab$. Because $B$ is of full row rank, then there exists a $z \in \R$ such that $b = Bz$. Therefore, $y = ABz$. As a result, we have proved $\cR(A) \subseteq \cR(AB)$. In (1), we prove $\cR(AB)\subseteq \cR(A)$. Eventually, we have $rank(AB) = A$.
	(4)
	$$rank(CA) = rank(A^TC^T) = rank(A^T) = rank(A).$$
\end{proof}


\begin{theorem}[rank sum inequality]\cite[206]{meyer2000matrix}\label{ch:linearalgebra:th:ranksuminequality}
	Let $A, B \in \F^{n\times n}$, then 
	\begin{itemize}
		\item $\cR(A + B) \subset \cR(A) + \cR(B)$
		\item $rank(A + B) \leq rank(A) + rank(B)$
		\item $rank(A + B) \geq \abs{rank(A) - rank(B)}$
	\end{itemize}
\end{theorem}
\begin{proof}
	(1)	Let $x = (A + B)y$, for some $y\in \R^n$, then $x = Ay + By$, indicating that $x\in \cR(A) + \cR(B)$.
	(2) $dim(\cR(A+B)) = rank(A+B) = dim(\cR(A)) + dim(\cR(B)) - dim(\cR(A)\cap \cR(B)) \leq rank(A) + rank(B)$ from \autoref{ch:linearalgebra:th:dimensionofsum}.
	(3)	use the fact the $rank(B) = rank(-B)$ and (2).
\end{proof}

\subsection{Fundamental theorem of ranks}
\begin{theorem}[The rank-nullity theorem]\index{rank-nullity theorem}\label{ch:linearalgebra:th:rankNullitytheorem}
	Let $A \in \F^{m \times n}$, then
	$$dim(\cN(A)) + dim(\cR(A)) = n.$$
\end{theorem}
\begin{proof}
	see linear map theory part \autoref{ch:linearalgebra:th:ranknullitytheorem}.
\end{proof}


\begin{lemma}[orthogonality between row space and null space]
	\cite[102]{banerjee2014linear} 
	For any matrix $A$, the subspace $\cR(A^T)$ is orthogonal to $\cN(A)$ and $\cR(A^T)\cap \cN(A) = \{0\}.$.
\end{lemma}
\begin{proof}
	Let $x \in \cN(A)$ and  $y\in \cR(A^T)$, then 
	$$x^Ty = x^TA^Tz = (Ax)^Tz = 0;$$
	therefore the subspace $\cR(A^T)$ is orthogonal to $\cN(A)$. Let $x \in \cN(A)$, let $x\in \cR(A^T)$, then $x^Tx = 0 \Rightarrow x = 0$. 
\end{proof}


\begin{lemma}\label{ch:linearalgebra:rankPropertyofXTXmatrix}
	\cite[102]{banerjee2014linear} \hfill
\begin{itemize}
	\item For any matrix $A$, $\cN(A) = \cN(A^TA)$.
	\item $A^TA$ is of full rank if and only if $A$ of full column rank. 
	\item  $dim(\cR(A)) = dim(\cR(A^TA))$; or equivalently, $rank(A)=rank(A^TA)$.
\end{itemize}	
\end{lemma}
\begin{proof}
	(1)(a) $\cN(A) \subseteq \cN(A^TA)$ is easy; (b) Let $x \in \cN(A^TA)$, that is $$A^TAx = 0 \implies x^TA^TAx = 0 \implies (Ax)^T(Ax) = 0 \implies Ax = 0.$$
	Therefore $\cN(A^TA) \subseteq \cN(A)$
	(b) Note that $A^TA$ is a square matrix. If $\cN(A) = 0$ then $\cN(A^TA) = \cN(A) = 0$.
	(3) 	First, we have $dim(\cR(A)) = n - dim(\cN(A)) = n - dim(\cN(A^TA)) =\dim(\cR(A^TA)). $
\end{proof}


\begin{theorem}[\textbf{fundamental theorem of ranks}]\index{fundamental theorem of ranks}\label{ch:linearalgebra:th:fundamentalTheoremOfRanks}
	\cite[132]{banerjee2014linear} For any matrix $A$
	$$dim(\cR(A)) = dim(\cR(A^T));$$
	or equivalently, the column rank equals the row rank,
	$$rank(A) = rank(A^T).$$
\end{theorem}
\begin{proof}
	Note that for any matrix $A$, we have $\cR(AA^T) \subseteq \cR(A)$, which implies $rank(A^TA) \leq rank(A^T)$. From above theorem, we know that $rank(A) = rank(A^TA) \leq rank(A^T)$, which says the rank of any matrix is less or equal than its transpose. Then $dim(\cR(A^T)) \leq dim(\cR((A^T)^T)) = dim(\cR((A))$, contradiction. Therefore, we have to have $dim(\cR(A)) = dim(\cR(A^T))$.
\end{proof}



\begin{remark}
	Note that when we decompose a matrix, its sum of rank of the decomposed matrix will increase, i.e., 
	$$rank(A + B) \leq rank(A) + rank(B)$$
	(for example $rank(A+A) < rank(A) +rank(A) = 2rank(A)$)
	and the equality only holds when $\cR(A)\cap \cR(B) = \emptyset$. 
\end{remark}


\subsection{Fundamental theorem of linear algebra}
\begin{theorem}[\textbf{fundamental theorem of linear algebra}]\index{fundamental theorem of linear algebra}
	\cite{calafiore2014optimization}\cite[178]{meyer2000matrix}\label{ch:linearalgebra:th:fundamentalTheoremOfAlgebra}For any given matrix $A \in \R^{m\times n}$, it holds that $\mathcal{N}(A)\perp \mathcal{R}(A^T)$ and $\mathcal{N}(A^T)\perp \mathcal{R}(A)$, hence
	$$\mathcal{N}(A) \oplus \mathcal{R}(A^T) =\R^n, \mathcal{N}(A^T)\oplus \mathcal{R}(A) = \R^m$$
	and
	$$rank(A) = rank(A^T),$$
	$$dim(\mathcal{N}(A)) + rank(A) = n,$$
	$$dim(\mathcal{N}(A^T)) + rank(A^T) = m.$$
\end{theorem}

\begin{proof}
(1)	we can always decompose $R^n = \cN(A) \oplus \cN(A)^{\perp}$ due to orthogonal complement theorem(\autoref{ch:linearalgebra:th:OrthogonalDecompositionInFiniteLinearSpace}). Therefore, we want to show $\cN(A)^{\perp} = \cR(A^T)$.
(a) First $\cR(A^\perp) \perp \cN(A)$, therefore, $\cR(A^\perp) \subseteq \cN(A)^\perp$. Let $z \in \cR(A^\perp)$. Then there exists a $y\in \R^m$ such that $z = A^Ty$. Let $m\in \cN(A)$ such that $Am = 0$. We have $m^Tz = m^TA^Ty = 0$.
(b) Because of $dim(\cR(A^T)) =  r = n - dim(\cN(A))$ due to rank-nullity theorem(\autoref{ch:linearalgebra:th:rankNullitytheorem}), then 
	$\cR(A^T) = \cN(A)^{\perp}$(see theorem for subspaces that equal dimensionality implies equality \autoref{ch:linearalgebra:th:equaldimensionimpliesequalsubspace}). 

(2)	Others can be proved similarly.
\end{proof}


\begin{remark}[interpretation]\hfill
	\begin{itemize}
		\item we can always decompose $R^n = \cN(A) \oplus \cN(A)^{\perp}$ and $R^m = \cR(A) \oplus \cR(A)^{\perp}$ due to orthogonal complement theorem in Hilbert space(\autoref{ch:linearalgebra:th:OrthogonalDecompositionInFiniteLinearSpace}). 
		\item For $x=Ay$, if we transpose the equation, we have $x^T = y^TA^T$, where $x^T,y^T$ are still the same vector in the output/input space. By informal symmetric argument, we need to have $\cN(A)^{\perp} = \cR(A^T),\cR(A)^{\perp} = \cN(A^T)$.
	\end{itemize}
\end{remark}


\begin{remark}[How to calculate different subspace]\hfill
	\begin{itemize}
		\item $\cR(A)$ can be directly obtained from linear independent columns in $A$.
		\item $\cR(A^T) = \cN(A)^\perp$ can be directly obtained from linearly independent rows in $A$.
		\item $\cN(A)$ can be calculated from solution space $Ax = 0$.
		\item $\cN(A^T)$ can be calculated from solution space $A^Tx = 0$.
	\end{itemize}
\end{remark}


\begin{corollary}[range-null decomposition]\index{range-null decomposition}\label{ch:linearalgebra:th:RangeNullVectorDecomposition}
	Given matrix $A\in \R^{m\times n}$, we decompose \textbf{uniquely} any $p \in \R^m $ as
	$$p = p_N + p_{N^\perp}$$
	where $p_N\in \cN(A), p_{N^\perp} \in \cR(A^T)$ and $p_{N^\perp} = A^T y$ for some $y\in \R^m$
\end{corollary}
\begin{proof}
Note that $\cN(A)$ and $\cR(A^T)$ are orthogonal complementary.
\end{proof}


\section{Complementary subspaces and projections}
\subsection{General complementary subspaces}
\begin{definition}[complementary subspaces]\index{complementary subspace}
	\cite[392]{meyer2000matrix}Subspaces $X,Y$ of a vector space $V$ are said to be complementary if
	$$V = X + Y, X\cap Y = 0$$
	we can also denoted as
	$$V = X \oplus Y$$
\end{definition}

\begin{definition}[angle between complementary subspaces]\cite[389]{meyer2000matrix}
	The angle between two complementary subspaces $X$ and $Y$ such that $$\R^n = X\oplus Y$$ can be defined as
	$$\cos(\theta) = \max_{u\in X,v\in Y} \frac{v^Tu}{\norm{v}\norm{u}} = \max_{u\in X,v\in Y,\norm{v}=1,\norm{u}=1} v^Tu$$
\end{definition}

\begin{remark}
	It is easy to see 
	\begin{enumerate}
		\item as two complementary subspace between orthogonal complementary, the angle is 90, since $v^Tu = 0$.
		\item In 3D, for an origin-passing line and a hyperplane, the angle is given by the angle between the line director and a vector in the plane that maximize the dot product.
	\end{enumerate}
\end{remark}



\begin{theorem}\cite[383]{meyer2000matrix}
	For a vector space $V$ with subspaces $X$ and $Y$ having respective basis $B_X$ and $B_Y$, the following statement are equivalent:
	\begin{itemize}
		\item $V = X\oplus Y$
		\item $B_X\cap B_Y = \emptyset$ and $B_X\cup B_Y$ is a basis for $V$.
	\end{itemize}
\end{theorem}
\begin{proof}
	Straight forward from the property of direct sum, see \autoref{ch:linearalgebra:sec:vectorspace:subsec:sumdirectsum}.
\end{proof}


\begin{definition}[projection along subspace]\index{projection along subspace}
Suppose $V=X\oplus Y$ such that for every $v\in V$, $v$ can be decomposed uniquely as $v=x+y,x\in X,y\in Y$.
	\begin{itemize}
		\item The vector $x$ is called the projection of $v$ onto $X$ along $Y$.
		\item The vector $y$ is called the projection of $v$ onto $Y$ along $X$.
	\end{itemize}
\end{definition}

\begin{remark}
	Only for complementary subspaces we can define projection.
\end{remark}

\begin{definition}[projector]\index{projector}\cite[386]{meyer2000matrix}
	Given two complementary subspaces $X,Y$ of vector space $V$ such that for every $v\in V$, we have unique decomposition of $v=x + y$. Then a linear operator $P(v) = x$ is called the projector onto $X$ along $Y$.
\end{definition}


\begin{theorem}[basic properties of projector]\cite[386]{meyer2000matrix}\label{ch:linearalgebra:th:projectorproperty}
	Given a projector $P$ onto subspace $\cX$ along subspace $\cY$, then
	\begin{itemize}
		\item $P^2 = P$
		\item The range of $P$ si the fixed point set of $P$, that is $P(x) = x,\forall x\in \{x = P(v),v\in V\}$		
		\item $I-P$ is the complementary projector onto $Y$ along $X$
		
		\item The matrix representation for projectors in $V=\F^n$ is given as
		$$P = [X|0][X|Y]^{-1}  = [X|Y]\begin{bmatrix}
		I & 0 \\
		0 & 0 \\
		\end{bmatrix} [X|Y]^{-1}$$
		where $X,Y$ are basis of subspace $\cX$ and $\cY$.		
	\end{itemize}
\end{theorem}
\begin{proof}
	(1) $P(P(v)) = P(x) = x = P(v)$; (2) $P(P(v)) = P(x) = x = P(v), \forall v\in V$, that is, the range $P(v)$ is the fixed point set.(3) $(I-P)(v) = v-x=y$; \\
	(4)  In a vector space spanned by basis $X\cup Y$, the matrix representation of $P$ is
	$$\begin{bmatrix}
	I & 0 \\
	0 & 0 \\
	\end{bmatrix}$$
	then we can use change of basis theorem to prove it(see \autoref{ch:linearalgebra:th:symmetricdecomposition}).
\end{proof}

\begin{theorem}[projector and idempotent]\index{idemopotent}\cite[387]{meyer2000matrix}
	Let $P$ be a linear operator on $V$ such that $P^2 = P$, then we have
	\begin{itemize}
		\item $\cR(P)$ and $\cN(P)$ are complementary subspaces, that is
		$$V = \cR(P) \oplus \cN(P)$$
		\item $P$ is a projector onto $\cR(P)$ along $\cN(P)$.
	\end{itemize}
\end{theorem}
\begin{proof}
	(1) At first $V = \cR(P) +  \cN(P)$
	Let any $x\in V$, we have
	$$x = Px  + (I-P)x$$
	where $Px \in \cR(P), (I-P)\in \cN(P)$
	Then we can show that $\cR(P)\cap \cN(P) = 0$: let $x\in \cR(P)\cap \cN(P) = 0$, then $x = Pv, Px = 0$, which implies
	$$x = Pv = P^2 v = Px = 0$$
	Therefore 
	$$V = \cR(P) \oplus \cN(P)$$
	(2) $Pv = x$ where $x\in \cR(P)$ and $v=x+y,x\in \cR(P),y\in \cN(P)$. Therefore by definition $P$ is a projector onto $\cR(P)$ along $\cN(P)$.
\end{proof}


\begin{corollary}[The range and null space of a projection matrix]
	For a projection matrix $P$, the range space is the column space of $P$, and the null space is the column space of $I-P$.
\end{corollary}
\begin{proof}
$P(I-P)x = (P-P^2)x = 0,\forall x$.	
\end{proof}


\subsection{Orthogonal complementary spaces and projections}\label{ch:linearalgebra:sec:OrthogonalComplementarySubspacesProjections}
\begin{definition}[orthogonal complement]\index{orthogonal complement}\cite[404]{meyer2000matrix}
For a subset $M$ in an inner product space $V$, we define $M^\perp$ to be the set 
$$M^\perp = \{x\in V: \ip{x,m}=0,\forall m\in M\}$$	
$M^\perp$ is known as the orthogonal complement.
\end{definition}

\begin{lemma}[orthogonal complement forms a subspace]\cite[404]{meyer2000matrix}
For a \textbf{subset} $M$ in an inner product space $V$, $M^\perp$ is a subspace of $V$, \textbf{no matter $M$ is a subspace or not.}
\end{lemma}
\begin{proof}
	By the definition of $M^\perp$, it is easy to show that $M^\perp$ contains 0, and it is closed under addition and multiplication.
\end{proof}

\begin{theorem}[orthogonal decomposition in finite linear space]\cite[404]{meyer2000matrix}\label{ch:linearalgebra:th:OrthogonalDecompositionInFiniteLinearSpace}
For a \textbf{subspace} $M$ in an inner product space $V$, then we have
\begin{itemize}
	\item $V = M\oplus M^\perp$, that is, for any vector $v\in V$, we have a unique decomposition $v=m+n,m\in M,n\in M^\perp$
	\item $dim(M^\perp) = dim(V) - dim(M)$
	\item $M^{\perp^\perp} = M$
\end{itemize}
\end{theorem}
\begin{proof}
(1) First $M\cap M^\perp = 0$, let $x\in M\cap M^\perp$, then $\ip{x,x}=0\Rightarrow x = 0$. Second, $S=M\oplus M^\perp \subseteq V$. Suppose $S$ is a proper subset, and $V = span(B_M,B_{M^\perp},q)$. When we use Gram-Smith procedure to  $B_M\cup B_{M^\perp}\cup q$, we will yield $q' = 0$ because $q'$ has to be orthogonal to $M$, then $q\subset M^\perp$. Therefore $$V = M\oplus M^\perp.$$
(2) Note that $$dim(V) = dim(M) + dim(M^\perp) - dim(M\cap M^\perp),$$ then use $M\cap M^\perp = 0$ in (1). (3) Let $m\in M$, then $m\perp M^\perp$, that is $M \subset M^{\perp^\perp}$. Because $dim(M^{\perp^\perp}) = dim(M)$, we have $M^{\perp^\perp} = M$ via \autoref{ch:linearalgebra:th:equaldimensionimpliesequalsubspace}.  
\end{proof}


\begin{definition}[orthogonal projection]\cite[429]{meyer2000matrix}
For any inner product space $V$ and a subspace $M$, we have $V = M\oplus M^\perp$, therefore $v = m + n, m\in M,n\in M^\perp, \forall v\in V$.Therefore, we can define an linear operator $P$ such that $P(v)= m$, then $P$ is called the orthogonal projector onto $M$ (along $M^\perp$.)
\end{definition}

\begin{theorem}[orthogonal projector representation]\cite[430]{meyer2000matrix}
Let $\cM$ be an $r$ dimensional subspace of $\R^n$, let $M$ and $N$ be the basis of $\cM$ and $\cN = M^\perp$. Then we have
\begin{itemize}
	\item $P_M = M(M^TM)^{-1}M^T$
	\item If $M$ is \textbf{orthonormal basis} such that $M^TM = I$, then $P_M = MM^T$
\end{itemize}
\end{theorem}
\begin{proof}
(1) From \autoref{ch:linearalgebra:th:projectorproperty}, we know that $$P_M = [M | 0][M | N]^{-1}$$
we can verify that
$$\begin{bmatrix}
(M^TM)^{-1}M^T\\
N^T
\end{bmatrix}[M | N] = I$$
because $M^TN = 0,N^TM = 0$. Then we have
$$[M | N]^{-1} = \begin{bmatrix}
(M^TM)^{-1}M^T\\
N^T
\end{bmatrix}$$
then we have
$$P_M = [M | 0][M | N]^{-1} = M(M^TM)^{-1}M^T.$$
(2) use $M^TM = I$. 
\end{proof}




\begin{theorem}[characterization of orthogonal projector]\cite[433]{meyer2000matrix}\label{ch:linearalgebra:th:characterizationoforthogonalprojector}
Suppose $P\in \R^{n\times n}$ satisfying $P^2 = P$, that is, $P$ is a projector;  then $P$ is a orthogonal projector if
\begin{itemize}
	\item $P$ is symmetric matrix; moreover, if $P$ is an orthogonal projector, then $P$ is symmetric.	
	\item $\cR(P)\perp \cN(P)$
	\item $\norm{P}_2 = 1 $
\end{itemize}
\end{theorem}
\begin{proof}
(1) If $P$ is orthogonal projector, then $P$ has matrix representation $P_M = M(M^TM)^{-1}M^T$ with respect to some basis $M$, it is easy to show that $P_M$ is symmetric.
(2) First, let $x\in \cR(P)$, then $x = Py$. Let $z\in \cN(P)$, then $x^Tz = y^TPz = 0$. Therefore, $\cR(P)\perp \cN(P)$. 
(3) If $P$ satisfies $P^2=P, P^T = P$, we can use spectral decomposition of $P$ \autoref{ch:linearalgebra:th:spectralpropertyorthogonalprojector} to prove. For example, $\norm{P}_2 = \lambda_{max} = 1$(\autoref{ch:linearalgebra:th:matrix2normeigenvalue}). 
\end{proof}



\begin{theorem}[spectral properties of orthogonal projector]\index{orthogonal projector}\label{ch:linearalgebra:th:spectralpropertyorthogonalprojector}
Let real matrix $P$ be an orthogonal projector (that is, $P^2=P, P^T = P$), then we have
\begin{itemize}
	\item The only possible eigenvalues are 1 and 0. 
	\item $\cR(P)$ are the eigenspace associated with eigenvalue 1; that is, Columns of $P$ are and only are the eigenvectors associated with eigenvalue 1.(Note that $P$ is not necessarily full rank, and therefore some columns are the linear combination of the other columns.) 
	\item $\cR(I-P)$ are the eigenspace assciated with eigenvalue 0
	 
	\item The algebraic multiplicity of 1 equals $rank(P)$, the algebraic multiplicity of 0 is $rank(I-P) = dim(\cN(P))$.
	\item $Tr(P) = rank(P)$.
	\item The diagonal entries of $P$ are all between 0 and 1 inclusively.
\end{itemize}
\end{theorem}
\begin{proof}
(1)Let $\lambda$ be a eigenvalue of $P$ for the eigenvector $v$, then $Pv=\lambda v \Rightarrow P^2 v = \lambda^2 v = Pv = \lambda v$. Therefore, $\lambda$ satisfy $\lambda^2 = \lambda$, which yields $\lambda = 1$ or $\lambda =0$. 
(2)$P^2 = P$ suggests columns of $P$ are the eigenvectors of eigenvalue 1. Let $v$ be the eigenvector associated with eigenvalue 1, then $Pv = v$, suggesting $v\in \cR(P)$. Therefore $\cR(P) = \cN(I-P)$(the eigenspace associated with eigenvalue 1). Similarly we can prove (3).
(4) Since $P$ is real matrix, from \autoref{ch:linearalgebra:th:symmetricmatrixspectraldecomposition}, we know that the algebraic multiplicity equals the geometric multiplicity. Therefore, $\cN(P)$(the null space corresponds to eigenvalue 0) has dimensionality equaling the number of independent columns of $I-P$, i.e., $rank(P)$. Similarly, we can show algebraic multiplicity of 1 equals $rank(P)$.\\
(5) Directly from (2) since $Tr(P) = \sum_{i}\lambda_i$.
(6) First, all diagonal entries will be non-negative(\autoref{ch:linearalgebra:th:characterizeNonnegativeDefiniteMatrixbyEigenvalueAndEntries}). Second, 
\begin{align*}
P &= P^2\\
\implies P_{ii} &= \sum_{j=1}^{n} P_{ij}^2 \\
&= P_{ii}^2 + \sum_{j\neq i} P_{ij}^2 \\
&\geq P_{ii}^2
\end{align*}
which implies that $0\leq P_{ii} \leq 1, i=1,2,...,n.$
\end{proof}


\begin{remark}[orthogonal projector and positive semidefinite matrix]
Orthogonal projectors is a subset of positive semidefinite matrix; particularly, a positive semidefinite matrix with only 0 and 1 eigenvalue is orthogonal projector. 		
\end{remark}


\begin{example}[elementary orthogonal projector] Let $u\in \R^n$. Then the matrix $P_u = \frac{uu^T}{u^Tu}$ and $I-P_u$ are called \textbf{elementary orthogonal projectors} associated with $u$.
	
$P_u$ is an $n\times n$ matrix of rank one that is symmetric and idempotent, i.e., $P_u^T = P_u, P_u^2 = P_u$.	
\end{example}

\begin{lemma}[uniqueness of orthogonal projectors with the same column basis] Let $A$ and $B$ be two $n\times p$ matrices, both with the full column rank and such that $\cR(A)$ and $\cR(B)$. Then 
	$$P_A = A(A^TA)^{-1}A^T = B(B^TB)^{-1}B^T=P_B.$$
\end{lemma}
\begin{proof}
Since $\cR(A) = \cR(B)$, there exists a $p\times p$ nonsingular matrix $C$ such that $A = BC$(\autoref{ch:linearalgebra:th:changeBasisSubspaceRepresentation}). 
We have
\begin{align*}
P_A &= A(A^TA)^{-1}A^T \\
&=BC(C^TB^TBC)^{-1}C^TB^T \\
&=BCC^{-1}(B^TB)^{-1}C^{-T}C^TB^T \\
&=BCC^{-1}(B^TB)^{-1}C^{-T}C^TB^T \\
&=B (B^TB)^{-1} B^T \\
&=P_B
\end{align*}
\end{proof}



\subsection{Decomposition of orthogonal projectors}

\begin{lemma}[decomposition of orthogonal projector]\label{ch:linearalgebra:th:decompositionOfOrthogonalProjectorOrthogonalcolumns}\cite[222]{banerjee2014linear}
	Let $A\in \R^{m\times n}$ with full column rank. Let $X$ be partitioned as $A = [A_1~~ A_2]$. Let $P_A,P_{A_1},P_{A_2}$ be the orthogonal projectors associated with $A,A_1,A_2$.
It follows that the following statements are equivalent
	\begin{itemize}
		\item $A_1A_2^T = 0,$
		\item $P_A = P_{A_1} + P_{A_2},$ that is
		$$A(A^TA)^{-1}A^T =  A_1(A^T_1A_1)^{-1}A^T_1 + A_2(A^T_2A_2)^{-1}A^T_2.$$
		\item $$P_{A_1}P_{A_2} = P_{A_2}P_{A_1} = 0.$$
	\end{itemize}
\end{lemma}
\begin{proof}
	(1) to (2) 
	\begin{align*}
	P_A &= A(A^TA)^{-1}A^T \\
	&=
	[A_1 ~ A_2] \begin{bmatrix}
	A_1^TA_1 & A_1^TA_2 \\
	A_2^TA_1 & A_2^TA_2 \\
	\end{bmatrix} [A_1 ~ A_2]^T \\
	&=
	[A_1 ~ A_2] \begin{bmatrix}
	A_1^TA_1 & 0 \\
	0 & A_2^TA_2 \\
	\end{bmatrix} [A_1 ~ A_2]^T \\ 
	&= A_1(A^T_1A_1)^{-1}A^T_1 + A_2(A^T_2A_2)^{-1}A^T_2
	\end{align*}
	(2) to (3)
	Note that 
	\begin{align*}
	(P_{A_1} + P_{A_2})^2 &= P_{A_1}^2+ P_{A_2}^2+ P_{A_2}P_{A_1}+ P_{A_1}P_{A_2} \\
	&= P_{A_1} + P_{A_2}+ P_{A_2}P_{A_1}+ P_{A_1}P_{A_2}\\
	&= P_{A_1} + P_{A_2}\\
	\implies & P_{A_2}P_{A_1}+ P_{A_1}P_{A_2} = 0.
	\end{align*}
	Left multiply $P_{A_2}P_{A_1}+ P_{A_1}P_{A_2} = 0$ we get
	$P_{A_1}P_{A_2}P_{A_1}+ P_{A_1}P_{A_2} = 0$; right multiply 
	$P_{A_2}P_{A_1}+ P_{A_1}P_{A_2} = 0$ we get
	$P_{A_2}P_{A_1}+ P_{A_1}P_{A_2}P_{A_1} = 0$; from the two equations, we get
	$$P_{A_1}P_{A_2} = P_{A_2}P_{A_1}.$$
	Plus  $P_{A_2}P_{A_1}+ P_{A_1}P_{A_2} = 0$, we get
	$$P_{A_1}P_{A_2} = P_{A_2}P_{A_1} = 0.$$
	(3) to (1):
	$$A_1P_{A_1}P_{A_2}A_2 = A_1A_2 = 0.$$
\end{proof}

\begin{theorem}[decomposition of orthogonal projector]\label{ch:linearalgebra:th:decompositionOfOrthogonalProjector}\cite[224]{banerjee2014linear}
Let $X\in \R^{m\times n}$ with full column rank. Let $X$ be partitioned as $X = [X_1 ~ X_2]$. Let $Z = (I - P_{X_1})X_2$.
It follows that the following statements are equivalent
\begin{itemize}
	\item $X_1^TZ = 0,$
	\item $\cR(X) = \cR([X_1~Z])$
	\item $P_X = P_{X_1} + P_Z$; that is,
	$$  X(X^TX)^{-1}X^T  =  X_1(X^T_1X_1)^{-1}X_1 + (Z)(ZZ^T)^{-1}(Z)^T. $$ 
\end{itemize}
\end{theorem}
\begin{proof}
(1) $$X_1^T(I - P_{X_1})X_2 = (X_1^T-X_1^T)X_2 = 0.$$(2) (a) Let $u\in \cR(X)$, then exists vectors $\alpha, \beta $ such that
\begin{align*}
u &= X_1\alpha + X_2\beta \\
  &= X_1\alpha + (I - P_{X_1} + P_{X_1})X_2\beta \\
  &= X_1\alpha +P_{X_1}X_2\beta+ Z\beta \\
  &= X_1\alpha + X_1(X^T_1X_1)^{-1}X_1^TX_2\beta + Z\beta\\
  &= X_1(\alpha + (X^T_1X_1)^{-1}X_1^TX_2\beta) + Z\beta
\end{align*}
therefore, $u \in \cR([X_1~Z])$.
(b) Let $u \in \cR([X_1~Z])$. then exists vectors $\alpha, \beta $ such that 
\begin{align*}
u &= X_1\alpha + Z\beta \\
&= X_1\alpha + (I - P_{X_1})X_2\beta \\
&= X_1\alpha -P_{X_1}X_2\beta+ X_2\beta \\
&= X_1\alpha - X_1(X^T_1X_1)^{-1}X_1^TX_2\beta + X_2\beta\\
&= X_1(\alpha - (X^T_1X_1)^{-1}X_1^TX_2\beta) + X_2\beta
\end{align*}
(3) use \autoref{ch:linearalgebra:th:decompositionOfOrthogonalProjectorOrthogonalcolumns}.
\end{proof}




\begin{corollary}[low rank update of orthogonal projector]\label{ch:linearalgebra:th:lowRankUpdateOfOrthogonalProjector}\cite[173]{theil1971principles}
Let $X\in \R^{m\times n}$ with full column rank and $X=[X_1,X_2,...,X_n]$. Let $W = [X_2,X_3,...,X_n]$.
	
Define $$H = X(X^TX)^{-1}X^T, M = I - H,  G = W(W^TW)^{-1}W^T, N = I - G.$$
It follows that
\begin{itemize}
	\item $$H = G + \frac{(NX_1)(NX_1)^T}{X_1NX_1}$$
	\item $$M = N - \frac{(NX_1)(NX_1)^T}{X_1NX_1}$$
\end{itemize}
\end{corollary}
\begin{proof}
(1)(informal) Use the property $G^2 = G, G^T = G, N^2 = N, N^T = N, GN = 0$, we can show that
$$(G + \frac{(NX_1)(NX_1)^T}{X_1NX_1})^2 = G + \frac{(NX_1)(NX_1)^T}{X_1NX_1},$$ 
and it is also symmetric.
(formal proof using \autoref{ch:linearalgebra:th:decompositionOfOrthogonalProjector})
(2) $$I - H = I - G -  \frac{(NX_1)(NX_1)^T}{X_1NX_1}$$
\end{proof}

\begin{remark}[interpretation]
We are augmenting $G$ with a basis $X_1$ projected in the space of complementing $W$ via $NX_1$. 
The additional projector associated with $NX_1$ is given by 
$$\frac{(NX_1)(NX_1)^T}{(NX_1)^T(NX_1)} = \frac{(NX_1)(NX_1)^T}{X^T_1N^TNX_1} = \frac{(NX_1)(NX_1)^T}{X^T_1NX_1}$$	
\end{remark}


\section{Orthonormal basis and projections}
\begin{definition}[orthonormal basis]
For inner product space, a basis is orthonormal if each vector has unit length, and orthogonal to other vectors. 
\end{definition}
\begin{remark}
Only in inner product space, we define orthogonality by inner product; in ordinary vector space, we do not have the concept of orthogonality.
\end{remark}


\begin{lemma}[representing vectors using orthonormal basis]
Let $\{e_i\}$ be a orthonormal basis for $V$, then for all $v \in V$, it can be represented as
$$v = \ip{v,e_1}e_1 + ... +\ip{v,e_n}e_n$$
\end{lemma}
\begin{proof}
	Let $v=\sum_i a_i e_i$ and use inner product to determine $a_i$.
\end{proof}




\subsection{Gram-Schmidt Procedure}

\subsection{Orthogonal-triangular decomposition}
\cite{calafiore2014optimization} Any square $A \in \R^{n\times n}$ can be decomposed as
$$A = QR$$
where $Q$ is an orthogonal matrix, and $R$ is an upper triangle matrix. If $A$ is nonsingular, then the factors 

\begin{theorem}[QR decomposition]\index{QR decomposition}
	Suppose $A\in \R^{m\times n},m\geq n$. Then we have
	\begin{itemize}
		\item there exists a orthonormal matrix $Q\in \R^{m\times m}$ and upper triangular matrix $R\in \R^{m\times n}$ such that $$A = QR$$
		\item If $\hat{Q}\in \R^{n\times n}$ and $\hat{R}\in \R^{n\times n}$, then
		$$A = QR=[\hat{Q},N]\begin{bmatrix}
		\hat{R}\\
		0
		\end{bmatrix} = \hat{Q}\hat{R}$$
		where $\hat{Q}\in \R^{m\times n}$ consists of the basis of $\cR(A)$, $N$ consists of the basis of $\cN(A^T)$ and $\hat{R}\in \R^{n\times n}$.  
		\item We can choose $R$ to have nonnegative diagonal entries
		\item If $A$ is of full rank, we can choose $R$ with positive diagonal entries, in which case the economical form $\hat{Q}$ and $\hat{R}$ will be unique. 
		\item If $A$ is square nonsingular, then $A = QR$ is unique.
	\end{itemize}
\end{theorem}
\begin{proof}
	(1)(2) Consider the Gram-Smith process for the columns of matrix $A$ given as
	\begin{align*}
	q_1 &= a_1, p_1=q_1/\norm{1_1}\\
	q_i & =a_i - \sum_{j=1}^{i-1} \ip{a_i,p_j}p_j,p_i = p_i/\norm{q_i},i=2,...,n
	\end{align*}
	or
	\begin{align*}
	a_1 &= r_{11}p_1\\
	a_j &= \sum_{i=1}^j r_{ij}p_i,j=2,...,n\\
	r_{ii} &= \norm{q_i}, r_{ij} = \ip{a_j,p_i}
	\end{align*}
	in which orthonormal basis $p_1,...,p_n$ for the column space $span(a_1,...,a_n)$ will be produced. 
	We can see that $a_i \in span(p_1,...,p_i)$, and therefore in matrix form we have
	$$A = \hat{Q}\hat{R}$$
	where $\hat{Q} \in \R^{m\times n}$ will consist of $p_1,...,p_n$ as columns and $R\in \R^{n\times n}$ will an upper triangular matrix. 
	The complete form $Q$ can be augmented with basis of $\cN(A^T) = \cR(A)^\perp$, such that $Q\in \R^{m\times m}$ consist of the complete orthonormal basis of $\R^m$.
	(3)(4)(5) If $a_1,...,a_n$ are linearly independent, from GS process, the matrix $\hat{Q},\hat{R}$ are uniquely determined, and the diagonal entries of $R$ is always positive. If $a_1,...,a_n$ are linearly dependent, there will exist scenario that 
	$$a_k \in span(p_1,...,p_{k-1})$$ and we can set $r_{kk} = 0$. 
\end{proof}


\subsection{Orthonormal basis for linear operators}

\begin{lemma}[existence of orthonormal basis]
\cite[185]{axler2015linear}Every finite-dimensional inner product space has an orthonormal basis.
\end{lemma}
\begin{proof}
Because $V$ has a basis, then we can use Gram-Schmidt procedure to make it orthonormal.
\end{proof}



\begin{lemma}[existence of upper triangular matrix with respect to orthonormal basis]
\cite[186]{axler2015linear} Suppose $T\in \cL(V)$. If $T$ has an upper triangular matrix with respect to some basis, then $T$ has an upper-triangular matrix with respect to some orthonormal basis of $V$.
\end{lemma}

\begin{proof}
note that the Gram-Schmidt matrix is upper triangular.
\end{proof}
 

\begin{theorem}[Schur's theorem]
Suppose $V$ is a finite-dimensional complex vector space and $T\in \cL(V)$. Then $T$ has an upper-triangular matrix with respect to some orthonormal basis of $V$.
\end{theorem}

\begin{proof}
directly from above theorem and the existence of upper triangular matrix theorem\autoref{ch:linearalgebra:th:existenceofupertrianglematrix}.
\end{proof}



\subsection{Riesz representation theorem}
\begin{theorem}
\cite[188]{axler2015linear}\cite[345]{johnsonbaugh2010foundations} Suppose $V$ is \textbf{finite} dimensional and $\phi$ is a linear functional on $V$. Then there is a \textbf{unique} vector $u\in V$ such that 
$$\phi(v) = \ip{u,v},\forall v\in V$$
Moreover, $$\norm{\phi} = \norm{u}$$
\end{theorem}

\begin{proof}
(1)Let $e_1,e_2,...,e_n$ be the orthonormal basis of $V$. Then
\begin{align*}
    \phi(v) &= \phi(\ip{v,e_1}e_1 + \ip{v,e_2}e_2,...) \\
    & = \ip{v,e_1}\phi(e_1) + \ip{v,e_2}\phi(e_2) + ...\\
    & = \ip{v,\conj{\phi(e_1)}e_1 + \conj{\phi(e_2)}e_2+ ...} 
\end{align*}
Therefore, we can let $u = \conj{\phi(e_1)}e_1 + \conj{\phi(e_2)e_2} + ...$
Uniqueness is easy.
(2) $$\norm{\phi} = \sup_{\norm{v}=1}\ip{u,v} =\norm{u}\norm{v}$$
where we use the fact that $\ip{u,v}^2 \leq \norm{u}^2\norm{v}^2$, and the equality can be achieved.
\end{proof}






\section{Eigenvectors and eigenvalues of Matrices: general theory}

\subsection{Existence and properties of eigenvalues}
\begin{definition}[characteristic equation]
	For a square matrix $A$, the equation
	$$det[A-\lambda I] = 0$$
	is called the characteristic equation of $A$. The resulting polynomial in $\lambda$ is called characteristic polynomial.
\end{definition}

\begin{theorem}[existence of roots, Fundamental theorem of algebra, recap] Every non-constant single-variable polynomials with complex coefficients has at least one complex root, or equivalently, every non-zero single variable, degree $n$ polynomial with complex coefficients has, counted with multiplicity, exactly $n$ roots. 
\end{theorem}
\begin{proof}
See \autoref{ch:topics-in-abstract-algebra:th:fundamentalthalgebra}.
\end{proof}


\begin{theorem}[existence of solution to characteristic polynomial]\label{ch:linearalgebra:th:existenceOfEigenvalueAndEigenvectors}\hfill
\begin{itemize}
	\item Any square matrix $A\in \C^{n\times n}$ has $n$ eigenvalues in $\C$, counted with multiplicity.
	\item Every square matrix $A$ has at least one eigenvalue and a corresponding (nonzero) eigenvector.
\end{itemize}	
\end{theorem}
\begin{proof}
From the fundamental theorem of algebra(\autoref{ch:topics-in-abstract-algebra:th:fundamentalthalgebra}), we know there exist a $\lambda$ such that $det[A-\lambda I] = 0$. Then from linear equation solution theory, the linear equation $(A-\lambda I)x=0$ must have $dim(\mathcal{N(A-\lambda I)}) \geq 1$.	
\end{proof}

\begin{remark}\hfill
\begin{itemize}
    \item Each distinct eigenvalues $\lambda_i,i=1,2,...,k \leq n$, has an associated \emph{algebraic multiplicity} $\mu_i \geq 1$, and $\sum_{i=1}^k\mu_i = n$.
    \item To each distinct eigenvalues $\lambda_i,i=1,2,...,k$, there corresponds a whole subspace $\phi_i = \mathcal{N}(\lambda_i I - A)$ of eigenvalues associated with this eigenvalue, called eigenspace.
\end{itemize}
\end{remark}

\begin{mdframed}
\textbf{Caution! Possible nonexistence of eigenvalues on $\R^2$}\\
consider the linear map $T:\R^2\to \R^2$, 
$$T(x,y) = (-y,x)$$
with the matrix representation of
$$
\begin{vmatrix}
    0 &-1\\1 & 0
\end{vmatrix}
$$
which simply rotates a vector. \textbf{We cannot find a scalar in $\F$ such that rotating a vector in $\R^2$ equals its scalar multiplication}. However, if the linear map is from $\C^2$ to $\C^2$, we can find eigenvalue and its corresponding eigenvector. \cite[135]{axler2015linear}
\end{mdframed}

\begin{theorem}[properties of eigenvalues]\label{ch:linearalgebra:th:eigenvalueproperty}\hfill
\begin{itemize}
	\item (invariance under similar transformation)Let $A$ be a squared matrix, let $B$ be any invertible matrix. Then the eigenvalues
	of $A$ and $T = BAB^{-1}$ are the same. 
	\item (transformation under scalar multiplication) Let $\lambda$ be the eigenvalue of $A$, then $\alpha \lambda$ will be the eigenvalues of $\alpha A$, for $\alpha\in \R$.
	\item (transformation under matrix power)Let $\lambda$ be the eigenvalue of $A$, then $\lambda^k$ will be the eigenvalues of $A^k$, for $k\in \Z_+$.
	\item (transformation under matrix polynomial)Let $\lambda$ be the eigenvalue of $A$, then $P(\lambda)$ will be the eigenvalues of $P(A)$, where $P(A) = a_0 A^0 + a_1A + a_2 A^2 + ... + a_k A^k$.
	\item (eigenvalues of an inverse)If $A$ is invertible, then for an eigenvector $v$ associated with eigenvalue $\lambda$, $A^{-1}$ has a corresponding eigenvalue $1/\lambda$, with the same eigenvector.  
\end{itemize} 
\end{theorem}
\begin{proof}
	(1)
$0=det(BAB^{-1} - I) = det(B)det(A - \lambda I) det(B)^{-1} = det(A - \lambda I)$
(2) Let $v$ be the eigenvector, then $$Av = \lambda v \implies \alpha Av = \alpha \lambda v$$
therefore $\alpha\lambda$ is the eigenvalue of $\alpha A$.
(3)Let $v$ be the eigenvector, then $$Av = \lambda v \implies A^2v = \lambda^2 v \implies A^kv = \lambda^k v$$
therefore $\lambda^k$ is the eigenvalue of $A^k$.
(4) same as (3).
(5) Let $v$ be an eigenvector of $A$ associated with eigenvalue $\lambda$, then
\begin{align*}
A^{-1}v &= A^{-1}(\lambda \frac{1}{\lambda} v)\\
&=\frac{1}{\lambda} A^{-1}(\lambda v)\\
&=\frac{1}{\lambda} A^{-1}Av\\
&=\frac{1}{\lambda} v
\end{align*}
\end{proof}



\subsection{Properties of eigenvectors}
\begin{theorem}[existence of eigenvector in complex field]
In $\C^N$, any square matrix $A\in \C^{N\times N}$ must have \textbf{at least} one eigenvector $\C^N$ associated with each distinct eigenvalue in $\C$.
\end{theorem}
\begin{proof}
Because for any matrix $A$, we can always have at least one eigenvalues in $\C$, therefore we can always have at least one eigenvector $\C^N$.
For each distinct eigenvalue, $\cN(A - \lambda I)$ has dimensionality equal or greater than 1(From linear equation solution theory,$A - \lambda I$ is singular, then the linear equation $(A-\lambda I)x=0$ must have $dim(\mathcal{N(A-\lambda I)}) \geq 1$.). Therefore, $\cN(A - \lambda I)$ must have one eigenvector as its basis. 	Also see \autoref{ch:linearalgebra:th:existenceOfEigenvalueAndEigenvectors}.
\end{proof}


\begin{lemma}[linear independence of eigenvectors]\label{ch:linearalgebra:th:linearIndependenceOfEigenVectors}
Let $\lambda_1,\lambda_2,...,\lambda_k$ be distinct eigenvalues of $A \in \R^{n\times n}$ and $k\leq n$, then 
\begin{itemize}
	\item the corresponding eigenvectors $e_1,e_2,...,e_k$ are linearly independent. \footnote{in \autoref{ch:linearalgebra:th:existenceOfEigenvalueAndEigenvectors}, every distinct eigenvalue has at least one eigenvector associated with it}.
	\item let $\mu_i$ denote the corresponding algebraic multiplicities, and let $\phi_i = \mathcal{N}(\lambda_i I - A)$, and let $u^i$ be any nonzero vectors such that $u^i \in \phi_i,i=1,2,...k$. Then $u^1,u^2,...,u^k$s are linearly independent.  
\end{itemize}
\end{lemma}
\begin{proof}
(1)Assume they are linear dependent, without loss of generality, we have 
$$e_1+\sum_{i=2}^k a_i e_i = 0$$
where for some $a_i \neq 0$. Multiply $A$, we have
$$\lambda_1 e_1 + \sum_{i=2}^k a_i\lambda_i e_i = 0$$ From the two equations, we have 
$$\sum_{i=2}^k a_i (\lambda_i - \lambda_1)e_i =0$$
indicating that $e_2,e_3,...,e_k$ are linearly dependent. Continue the same argument will lead to the conclusion that $e_{k-1}$ and $e_k$ are linearly dependent(that is, there exists some $\alpha \in \R$ such that $e_{k-1} = \alpha e_k$), which is obvious not true.
(2) Directly from (1) because $Au^i = \lambda_i u^i.$	
\end{proof}



\subsection{Right and left eigenvectors}
\begin{definition}[Right and left eigenvectors]\index{right eigenvector}\index{left eigenvector}
\cite[82]{luenberger1979introduction}Given a square matrix $A$, an eigenvector $e_i$ is right eigenvector if there exists $\lambda_i \in \F$ such that
$$Ae_i = \lambda_i e_i$$
An eigenvector $f_i$ is left eigenvector if there exists $\lambda_i \in \F$ such that
$$f_i^TA = \lambda_i f_i$$
\end{definition}

\begin{lemma}[left/right eigenvectors and symmetry]\hfill
\begin{itemize}
	\item The left eigenvector of $A$ is the right eigenvector of $A^T$.
	\item For symmetric matrix $A$, the left eigenvector is the same as right eigenvector.
\end{itemize}	
\end{lemma}
\begin{proof}
(1)$$(f_i^TA)^T = (\lambda_i f_i^T)^T \implies A^Tf_i = \lambda_i f_i$$
(2) from (1).
\end{proof}


\begin{lemma}[left and right eigenvalues]\label{ch:linearalgebra:th:leftrighteigenvalue}
The left and right eigenvalues are identical.
\end{lemma}

\begin{proof}
$det(A-\lambda I) = det(A^T-\lambda I^T) = det(A^T-\lambda I)$.
\end{proof}


\begin{theorem}[orthogonality of left and right eigenvectors]\label{ch:linearalgebra:th:leftrighteigenvectororthogonality}
\cite[83]{luenberger1979introduction}\label{ch:linearalgebra:th:leftrightorthogonality} For any two distinct eigenvalues of a matrix, the left eigenvector of one eigenvalue is orthogonal to the right eigenvalues of the other.
\end{theorem}
\begin{proof}
Let $e_i,f_i$ be the left and eight eigenvectors of eigenvalue $\lambda_i$.
 Let $e_j,f_j$ be the left and eight eigenvectors of eigenvalue $\lambda_j$. 
 Then
 $$f_i^T (Ae_j) = \lambda_j f_i^Te_j $$
 $$(f_i^T A)e_j = (\lambda_i f_i^T)e_j $$
 then $(\lambda_i - \lambda_j)f_i^Te_j = 0 \Rightarrow f_i^Te_j = 0$
\end{proof}

 
 
\begin{theorem}[orthogonality of eigenvectors for symmetric matrix]\label{ch:linearalgebra:th:EigenvectororthogonalityRealSymmetricMatrix}
 For a real-valued symmetric matrix $A$, eigenvectors of distinct eigenvalues are orthogonal.
\end{theorem}
 
 
 
\subsection{Diagonalizable matrices}
\begin{definition}[algebraic multiplicity,geometric multiplicity]\index{algebraic multiplicity}\index{geometric multiplicity}\hfill
\begin{itemize}
	\item The algebraic multiplicity $\mu_i$ of eigenvalue  $\lambda_i$ is its multiplicity as a root of the characteristic polynomial. 
	\item The geometric multiplicity $\gamma_i$ of eigenvalue $\lambda_i$ is the dimensionality of the null space $\cN(A-\lambda_i I)$.
\end{itemize}	
\end{definition}

\begin{theorem}[boundedness of geometric multiplicity]\cite[323]{banerjee2014linear}\label{ch:linearalgebra:boundednessGeometricmultiplicity}
For a square matrix $A$ with eigenvalues $\lambda$, we have
$$1\leq \mu \leq \gamma \leq n$$
that is, the geometric multiplicity $\mu$ is bounded by the algebraic multiplicity $\gamma$.
\end{theorem}
\begin{proof}
The algebraic part can be obtained from fundamental theorem of algebra. For geometric multiplicity, it will always be greater than 1 because the null space $A - \lambda I$ with $det(A - \lambda I) = 0$ has non-zero dimensionality. 
Let $[x_1,...,x_\mu]$ be the basis of the eigenspace. We can extend this basis to $[x_1,...,x_\mu,...,x_n] = P$, then
\begin{align*}
P^{-1}AP &= P^{-1}[\lambda x_1,...,\lambda x_\mu, Ax_{\mu + 1},..., Ax_n]\\
&=[\lambda e_1,..,\lambda e_{\mu}, PAx_{\mu + 1},...,PAx_{n}]\\
&=\begin{Bmatrix}
\lambda I_\mu & B\\
0 & D
\end{Bmatrix}
\end{align*}
Then $Det[A - x I] = Det[P^{-1}AP - x I] = (x-\lambda)^\mu det[D-xI]$
which has least $\mu$ roots counting multiplicity.
\end{proof}
 


\begin{theorem}[diagonalizable matrices]\index{diagonalizable matrix}Let $\lambda_i,i=1,2,...,k \leq n$ be the distinct eigenvalues of $A\in \R^{n\times n}$, let $\mu_i$ denote the corresponding algebraic multiplicities, and let $\phi_i = \mathcal{N}(\lambda_i I - A)$. Let further $U^i $ be a matrix containing the basis of $\phi_i$.
\begin{itemize}
	\item  If $dim(U^i)=\nu_i=\mu_i, \forall i$, the matrix $A$ is said to be \textbf{diagonalizable}.
	\item Assume $A$ is diagonalizable. then
	$$U=[U^1 ~U^2~ ... ~U^k]$$
	is invertible, and $$A = U\Lambda U^{-1}$$
	where 
	\[\begin{bmatrix}
	\lambda_1 I_{\nu_{1}} &0&0&\dots\\
	0&\lambda_2 I_{\nu_{2}}&0&\dots\\
	0&0&\lambda_3 I_{\nu_{3}}&\dots\\
	&\dots&\dots&\dots\\
	\end{bmatrix}\]
	\item The space $\R^n$ can be decomposed as the direct sum of all eigenspaces. 
\end{itemize}
\end{theorem}
\begin{proof}
(2) From the linear independence of the eigenvectors associated with distinct eigenvalues(\autoref{ch:linearalgebra:th:linearIndependenceOfEigenVectors}), $U$ will contain $n$ linear independent columns, therefore invertible. For every column $u$ in $U$, we have
$Au = \lambda u$; therefore $AU = U\Lambda \Leftrightarrow A = U\Lambda U^{-1}$.	
(3) Use the criterion for direct sum(\autoref{ch:linearalgebra:th:SubspaceDirectSumCriterion})
\end{proof}

\begin{lemma}[enough distinct eigenvalues implies diagonalizability]
	A square matrix $A\in \F^{n\times n}$ can be diagonalized if it has $n$ \textbf{distinct} eigenvalues.
\end{lemma}
\begin{proof}
	use algebraic and geometric multiplicity inequality(\autoref{ch:linearalgebra:boundednessGeometricmultiplicity}).
\end{proof}



\begin{lemma}[zero eigenvalue and singularity]
A square matrix $A$ is invertible if and only if it has no zero eigenvalues.
\end{lemma}
\begin{proof}
use the fact of $Det(A) = \prod \lambda_i$ or $dim(\cN(A - 0I)) = dim(\cN(A)) 
\geq 1$.
\end{proof}
\begin{remark}
	If a square matrix has zero eigenvalue, that means the null space in non-trivial. Then the eigenvector corresponding to the zero-eigenvalue span the null space. Since $(A-\lambda I) x = 0 \Rightarrow Ax = 0$.
\end{remark}


\begin{remark}
	\textbf{A matrix is diagonalizable does not imply it is invertible}, since it might contain eigenvalue of 0.
\end{remark}

\section{Eigenvalue and eigenvectors of matrices: case studies}
\subsection{Real diagonalizable matrix}
\begin{theorem}
Let $A\in \R^{n\times n}$ has $n$ distinct eigenvalues, then the complex eigenvalues come as conjugate pairs, and corresponding eigenvectors are conjugate to each other.
\end{theorem}
\begin{proof}
Because the characteristic polynomial coefficients are real-valued, then its complex roots come as conjugate pairs(see polynomial theory section). Let $V_1$ be the eigenvector of eigenvalue $\lambda_1$, then
$$AV_1 = \lambda_1 V_1 \Rightarrow \conj{AV_1} = \conj{\lambda_1 V_1} \Rightarrow A\conj{V_1} = \conj{\lambda_1}\conj{V_1}$$
Therefore $V_2 = \conj{V_1}$ is the eigenvector associated with $\lambda_2 = \conj{\lambda_1}$. 
\end{proof}

\begin{remark}
Note that $V_1$ must have non-zero imaginary part, otherwise we cannot have $AV_1 = \lambda_1V_1$, where $\lambda_1$ has non-zero imaginary part.
\end{remark}

\begin{theorem}[convert complex eigenvector to real eigenvector]\index{complex eigenvector}
Let $A\in \R^{n\times n}$ has $n$ distinct eigenvalues. Suppose $A$ has a pair of complex conjugated eigenvalue $\lambda_1 = a+bi,\lambda_2=a-bi,a,b\in \R$, with a pair of corresponding complex conjugated eigenvectors $V_1=C+Di,V_2=C-Di,C,D \in \R^n$
then we can create the 2-D real-valued subspace as
$$A[C, D] = [C, D]\begin{pmatrix}
a & b\\
-b & a\\
\end{pmatrix} $$
\end{theorem}
\begin{proof}
We have 
$$A(C + Di) = (a+bi)(C+Di) = (aC-bD) + i(aD + bc)$$
$$A(C - Di) = (a-bi)(C-Di) = (aC-bD) - i(aD + bc)$$
Sum each other, and we get
$$AC = aC-bD$$
Subtract each other, and we get
$$AD = bC+aD$$
\end{proof}


\begin{remark}
\textbf{This conversion is only appealing when we want to make everything real in order to interpret its physical meaning.} It is not appealing in its mathematical structure since it makes two 1D subspace become one 2D subspace.
\end{remark}


\begin{corollary}
Let $A\in \R^{n\times n}$ has $n$ distinct eigenvalues. Then there exists an invertible matrix $T$ such that
$$T^{-1}AT = \begin{pmatrix}
\lambda_1 &  &  &  &  & \\ 
 & \ddots &  &  &  & \\ 
 &  &  \lambda_k&  &  & \\ 
 &  &  &  D_1 & & \\ 
 &  &  &  & \ddots & \\ 
 &  &  &  &  & D_l
\end{pmatrix}$$
where $D_j$ has the form of $$D_j = \begin{pmatrix}
a_j & b_j\\
-b_j & a_j\\
\end{pmatrix} $$
Moreover, \textbf{every item in this decomposition is real-valued}.
\end{corollary}


\subsection{Real symmetric matrix}\index{real symmetric matrix}
\subsubsection{Spectral properties}
\begin{theorem}
[Eigen-decomposition of a real symmetric matrix]\cite{calafiore2014optimization}\label{ch:linearalgebra:th:symmetricmatrixspectraldecomposition} Let $A\in \R^{n\times n}$ be symmetric, let $\lambda_i,i=1,...,k \leq n$, be the distinct eigenvalues of $A$,let further $\mu_i$ denote the algebraic multiplicity of $\lambda_i$, and let $\phi_i = \mathcal{N}(\lambda_i I - A)$, we have:
\begin{itemize}
    \item $\lambda_i \in \R$
    \item $\phi_i \perp \phi_j$
    \item The eigenvectors can be chosen to lie in $\R^n$
    \item $dim\phi_i = \mu_i$
\end{itemize}
\end{theorem}
\begin{proof}
(1) $$(Ae_i)^H e_i = \conj{\lambda_i} e_i^H e_i = e_i^H(Ae_i) = \lambda_i e_i^H e_i \Rightarrow \lambda_i = \conj{\lambda_i}$$ 
(2) See self-adjoint linear operator theory and left right eigenvector orthogonality theory(\autoref{ch:linearalgebra:th:EigenvectororthogonalityRealSymmetricMatrix}).\\
(3) Let $V$ be an eigenvector with $\lambda$, then $\conj{V}$ will be an eigenvector associated with with $\lambda$ since $$AV=\lambda V\Rightarrow \conj{AV} =  \conj{\lambda V} \Rightarrow A\conj{V} =  \lambda\conj{V}$$
then we can remove the imaginary part by $V+\conj{V}$,which is also an eigenvector.
(4) Consider $\lambda$ is a eigenvalue with algebraic multiplicity greater than 1. Let $P = [x_1,...,x_n] = [x_1 X_2]$ be the orthonormal basis, with $x_1$ being the normalized eigenvector associated with $\lambda$.
Then 
\begin{align*}
	P^TAP = \begin{Bmatrix}
	\lambda & \lambda x_1^TX_2\\
	\lambda X_2^Tx_1 & X_2^TAX_2
	\end{Bmatrix} = \begin{Bmatrix}
	\lambda & 0\\
	0 & X_2^TAX_2
	\end{Bmatrix}
\end{align*}
Let $B=X_2^TAX_2$. Note that $det(A-tI) = det(P^TAP-tI) = (t-\lambda)det(B-tI)$. Since $A$ has $\lambda$ with multiplicity of $\gamma$, $B$ will has $\lambda$ with multiplicity of $\gamma - 1$. 
We can continue the same operation on $B$, and when we reduce one algebraic multiplicity, we get out of one eigenvector. \textbf{The key is after the operation, $B$ is still real symmetric, and we can continue the process}.
\end{proof}




\begin{remark}
The important implications are:
\begin{enumerate}
\item symmetric matrix can always be \textbf{diagonalized}. More deeply, any quadratic function represented by symmetric matrix can always be completed to square forms.
\item The eigenspaces are orthogonal to each other, and there eigenvectors will be orthogonal to each other. 
\item \textbf{The eigenvalues are real.}
\end{enumerate}
\end{remark}





\begin{theorem}[Spectral theorem for symmetric matrix]
\cite{calafiore2014optimization}Let $A\in \R^{n\times n}$ be symmetric, let $\lambda_i,i=1,2,..,$ be the eigenvalues of $A$ (counting multiplicities). Then, there exists a set of orthonormal vectors $u_i,i=1,2...,n$ such that $Au_i = \lambda_i u_i$. Equivalently, there exists an orthogonal matrix $U=[u_1,...,u_n], U^TU = UU^T = I$, such that 
$$A = U\Lambda U^T$$
\end{theorem}
\begin{remark}
The implication is that any symmetric matrix can be decomposed as a weighted sum of simple rank-one matrix. 
\end{remark}


\begin{remark}[\textbf{zero eigenvalue issue}]
A symmetric matrix might contain zero eigenvalues, in this case, there are diagonal entries in $\Lambda$ that are zeros. Then the corresponding eigenvectors in $U$ will be the basis span the null space. 
\end{remark}




\subsubsection{Rayleigh quotients}

\begin{theorem}[Rayleigh quotients]\cite[126]{calafiore2014optimization}\index{Rayleigh quotient}\cite[235]{horn2012matrix}\label{ch:linearalgebra:th:Raylleighquotient}
	Given a symmetric matrix $A\in \R^{n\times n}$, it holds that 
	$$\lambda_{min}(A)\leq \frac{x^TAx}{x^Tx} \leq \lambda_{max}(A),\forall x\neq 0, x\in \R^n.$$
	Moreover, 
	$$\lambda_{max}(A) = \max_{\norm{x}_2 = 1} x^TAx$$
	$$\lambda_{min}(A) = \min_{\norm{x}_2 = 1} x^TAx$$
	and the maximum and minimum value are attained for $x$ is the unit eigenvector of $A$ associated with its largest/smallest eigenvalues of $A$.
\end{theorem}

\begin{proof}
	(1) Let $x\neq 0$, let $A = U\Lambda U^T$, then
	$$x^TAx = x^TU \Lambda U^Tx = y^T\Lambda y = \sum_{i=1}^n \lambda_i y_i^2 \leq \lambda_{max} \norm{y}_2^2 = \lambda_{max} \norm{U^Tx}_2^2 = \lambda_{max} \norm{x}_2^2.$$
	Similarly, we can prove another inequality.
	(2) (second method) Use maximization, we have
	$$\max_{x\in \R^n } x^T A x, ~st~ x^Tx = 1.$$
	The first order KKT condition gives
	$$Ax = \lambda x;$$
	that is, optimal $x$ should have the same direction of eigenvectors. It is easy to see optimal $x$ should be the eigenvector with the maximum eigenvalue. 
\end{proof}

\begin{corollary}[generalized Rayleigh quotients]\label{ch:linearalgebra:th:GeneralizedRaylleighquotient}
	Given a symmetric matrix $A\in \R^{n\times n}$ and positive symmetric matrix $\Sigma\in \R^{n\times n}$, it holds that 
	$$\lambda_{min}(\Sigma^{-1/2} A\Sigma^{-1/2})\leq \frac{x^TAx}{x^T\Sigma x} \leq \lambda_{max}(\Sigma^{-1/2} A\Sigma^{-1/2}),\forall x\neq 0, x\in \R^n,$$
	where $\Sigma = \Sigma^{1/2}\Sigma^{1/2}$, and $\Sigma^{1/2}$ is a positive semi-definite symmetric matrix and the matrix square root of $\Sigma$ (\autoref{ch:linearalgebra:th:matrixSquareRoot}). 
	
	The maximium/minimum value is achieved at $x^* = \Sigma^{-1/2}u^*$, where $u^*$ is the unit eigenvector associated with the maximum/minimum eigenvalue of matrix $\Sigma^{-1/2} A\Sigma^{-1/2}$.
	
	Moreover, $x^*$ is also the eigenvectors of $\Sigma^{-1}A$ associated with the maximum/minimum eigenvalue. In other words, the matrix $\Sigma^{-1}A$ and $\Sigma^{-1/2} A\Sigma^{-1/2}$ have the same eigenvalues, and their eigenvectors are connected by $x^* = \Sigma^{-1/2}u^*$.
\end{corollary}
\begin{proof}
	(1) Note that	
	\begin{align*}
	\frac{x^TAx}{x^TBx} &= \frac{x^TAx}{x^T\Sigma^{1/2}\Sigma^{1/2}x} \\
	&= \frac{x^TAx}{x^T\Sigma^{1/2}\Sigma^{1/2}x} \\
	&= \frac{u^T\Sigma^{-1/2}A\Sigma^{-1/2}u}{u^Tu} \quad(use~u = \Sigma^{1/2}x) 
	\end{align*}
	Then we use \autoref{ch:linearalgebra:th:Raylleighquotient}.
	(2) To show the connection of eigenvalue problem of $B^{-1}A$, we have
	\begin{align*}
	\Sigma^{-1/2}A\Sigma^{-1/2}u^* &= \lambda u^* \\
	\Sigma^{-1/2}A\Sigma^{-1/2}\Sigma^{1/2}x &= \lambda \Sigma^{1/2}x \\
	\Sigma^{-1/2}Ax &= \lambda \Sigma^{1/2}x \\
	\Sigma^{-1/2}\Sigma^{-1/2}Ax &= \lambda x \\
	B^{-1}Ax = \lambda x
	\end{align*}
\end{proof}


\begin{corollary}
Let $A$ be an $m\times m$ symmetric matrix with eigenvalues $\lambda_1\geq \lambda \geq \cdots \geq \lambda_m$, and denote the corresponding normalized eigenvectors as $P_1,P_2, ...,P_m$. Then the supremum of $$\sum_{i=1}^r x_i^TAx_i = Tr(X^TAX),$$
with $X= [x_1,...,x_r],$ over all sets of $r\leq m$ mutually orthonormal vectors $x_1,...,x_r$, is equal to $\sum_{i=1}^r \lambda_i$ and is attained when $x_i = P_i, i=1,2,..., r$. 	
\end{corollary}
\begin{proof}
Use the inequality technique similar in \autoref{ch:linearalgebra:th:Raylleighquotient}.
\end{proof}

\begin{corollary}[maximization lemma]
	Given symmetric positive definite matrix $A\in \R^{n\times n}$ and a vector $d\in \R^p$. It follows that
	$$\max_{x\in\R^p} \frac{(d^T x)^2 }{x^TAx}, ~st~ x^TAx = 1$$
	has maximum value of $d^TA^{-1}d$, which is attained at $x = \frac{A^{-1}d}{\norm{A^{-1}d}^2}.$	
\end{corollary}
\begin{proof}
	The Lagrange is given by
	$$L(x) = x^Tdd^Tx - \lambda (x^TAx - 1).$$
	Then first order KKT condition gives
	$$dd^T x = \lambda Ax \implies A^{-1}d (d^Tx) = \lambda x,$$
	that is, optimal $x$ should have the same direction of $A^{-1}d$. The rest is straight forward. 
\end{proof}

\begin{corollary}[\textbf{matrix 2-norm}]\index{matrix 2-norm}\label{ch:linearalgebra:th:matrix2normeigenvalue}
	For a matrix $A$, if we define its norm as
	$$\norm{A}_2 = \max_{x\neq 0} \frac{\norm{Ax}_2}{\norm{x}_2}$$
	then
	$$\norm{A}_2 = \sqrt{\lambda_{max}(A^TA)}$$
	Moreover, if $A$ is square, then $\norm{A}_2 = \lambda_{max}$
\end{corollary}

\begin{proof}
	$\norm{Ax} = \sqrt{x^TA^TAx}$ and $A^TA$  is a symmetric matrix. Then we can use \autoref{ch:linearalgebra:th:Raylleighquotient}.
\end{proof}


\begin{theorem}[connections of spectral properties of $XX^T$ and $X^TX$]\label{ch:linearalgebra:th:spectralpropertiesofXXTandXTX}
	Let $X$ be a real-valued matrix, then the eigen decomposition of $XX^T$ and $X^TX$ are related. If
	$$XX^T = U\Lambda U^T$$
	then
	$$X^TX = V\Lambda V^T$$
	That is they have the same \textbf{non-zero} eigenvalue.Moreover,
	$u_i = Xv_i/\sqrt{\lambda_i}, v_i = X^Tu_i/\sqrt{\lambda_i}$
\end{theorem}

\begin{proof}
	$XX^T$ is symmetric and therefore can have a eigen-decomposition. Let $u_i$ be an eigenvector $XX^T$, then $$XX^T u_i = \lambda u_i \Rightarrow X^TXX^Tu_i = \lambda_i X^Tu_i$$
	therefore $X^Tu_i$ is an eigenvector of $X^TX$ with length $$\norm{X^Tu} = \sqrt{u_i^TXX^Tu_i} = \sqrt{\lambda_i u_i^Tu_i}=\sqrt{\lambda_i}$$
	The rest is straight forward.
\end{proof}


\begin{remark}
	This theorem is important in proving SVD theorem.
\end{remark}

\subsubsection{Pointcare inequality}

\begin{theorem}[Pointcare inequality]\index{Pointcare inequality}\label{ch:linearalgebra:th:pointcareinequality}
	\cite[126]{calafiore2014optimization} Let $A\in \F^{n\times n}$ be a symmetric matrix, and let $V$ be any $k,1\leq k\leq n$ dimensional subspace of $\R^n$. Then there exist vectors $x,y\in V$, with $\norm{x}_2 = \norm{y}_2 = 1$, such that
	$$x^TAx \leq \lambda_k(A),$$
	$$ y^TAy \geq \lambda_{n-k+1}(A)$$
	where $\lambda_k$ is the $k$th largest eigenvalue(with $\lambda_n$ being the largest eigenvalue).( $\lambda_1,\lambda_2,..\lambda_n$ are sorted in increasing order.)
\end{theorem}
Proof: (1)Let $A = U\Lambda U^T$, let $Q = span(u_k,u_{k+1},...,u_n)$ and $dim(Q) = n-k+1$, then $V\cap Q$ is not nonempty (since $dim(Q) + dim(V) > n$). Let $x\in V\cap Q$, and $x$ must have representation of $x = \sum_{i=k}^n \eta_i u_i$, let $U_k = [u_k,u_{k+1},...,u_n]$then
$$x^TAx = (U_k \eta)^T U\Lambda U^T (U_k \eta) = \sum_{i=k}^n \eta_i^2 \lambda_i \leq \lambda_k \sum_{i=k}^n \eta_i^2 = \lambda_k$$
(2) Consider the matrix $-A$, where $\lambda_k(-A) = -\lambda_{n-k+1}(A)$


\begin{remark}[when the equality hold]
	When we take $V = span(u_1,...,u_k)$,the equality will hold.
\end{remark}

\begin{remark}[restatement of Rayleigh quotient]
	When $k=n$, we have 
	$$x^TAx \leq \lambda_{max}(A)\norm{x}^2$$
	and
	$$x^TAx \geq \lambda_{min}(A)\norm{x}^2$$
\end{remark}


\begin{corollary}[minimax principle,Courant-Fisher theorem]\index{minimax principle}\index{Courant-Fisher theorem}
	\cite{wiki:minmax}\cite[127]{calafiore2014optimization}\cite[237]{horn2012matrix} Let $A\in \F^{n\times n}$ be a symmetric matrix, and let $V$ be any subspace of $\R^n$. Then for $k\in \{1,2,...,n\}$ it holds that
	$$\lambda_k(A) = \min_{dim V = k} ~~ \max_{x\in V,\norm{x}_2 = 1} x^TAx $$
	and
	$$\lambda_k(A) = \max_{dim V = n-k+1} ~~ \min_{x\in V,\norm{x}_2 = 1} x^TAx$$
	where $\lambda_1,\lambda_2,..\lambda_n$ are sorted in increasing order.
\end{corollary}
\begin{proof}
	directly from Pointcare inequality.
\end{proof}


\begin{remark}[reduction to Reylergh quotient]\hfill
	\begin{itemize}
		\item If we let $dim V = n = k$ in the first equation, we have
		$$\lambda_n(A) = \min_{dim V = n} ~~ \max_{x\in V,\norm{x}_2 = 1} x^TAx = \max_{x\in V,\norm{x}_2 = 1} x^TAx $$
		the minimization operator can be drop because it is the whole ambient space.
		\item If we let $dim V = n, k = n$ in the second equation, we have
		$$\lambda_1(A) = \max_{dim V = n} ~~ \min_{x\in V,\norm{x}_2 = 1} x^TAx = \max_{x\in V,\norm{x}_2 = 1} x^TAx $$
		the maximization operator can be drop because it is the whole ambient space.
	\end{itemize}
\end{remark}


\subsection{Hermitian matrix}\index{Hermitian matrix}
\begin{definition}[conjugate transpose]\index{conjugate transpose}
Given a matrix $A\in \F^{m\times n}$, the conjugate transpose of $A$ is denoted as $A^H$ such that
$$(A^H)_{ij} = \conj{A_{ji}}$$
\end{definition}


\begin{lemma}[elementary property of conjugate transpose]\hfill
\begin{itemize}
    \item $(A+B)^H = A^H + B^H$
    \item $(A^H)^H = A$
    \item $(AB)^H=B^HA^H$
    \item $(rA)^H = \conj{r}A^H$
    \item $(A^{-1})^H = (A^H)^{-1}$ if $A$ is invertiable
\end{itemize}
\end{lemma}
\begin{proof}
We only prove (3) and (5). (3): $[(AB)^H]_{ij} = \sum_k \conj{A_{jk}B_{ki}} = \sum_{k} B^H_{ik}A^H_{kj}$; (5) $A^H (A^{-1})^H = (A^{-1}A)^H = I^H = I$, where we have used (3).
\end{proof}


\begin{lemma}[elementary property of transpose]\index{transpose}\hfill
\begin{itemize}
    \item $(A+B)^T = A^T + B^T$
    \item $(A^H)^T = A$
    \item $(AB)^T=B^TA^T$
    \item $(rA)^T = rA^T$
    \item $(A^{-1})^T = (A^T)^{-1}$ if $A$ is invertible
\end{itemize}
\end{lemma}
\begin{proof}
Similar to above lemma.
\end{proof}

\begin{definition}
A matrix $A$ is Hermitian if $A^H = A$.
\end{definition}


\begin{theorem}
If $A$ is Hermitian, then all eigenvalues of $A$ are real.
\end{theorem}
\begin{proof}
same as the real symmetric case.
\end{proof}


\begin{theorem}[spectral theorem for Hermitian matrix]
Let $A$ be a Hermittian matrix, then there exists a unitary matrix $U$ and real diagonal matrix $D$ such that
$$A = UDU^H$$
\end{theorem}
\begin{proof}
	same as the real symmetric case.
\end{proof}

\subsection{Matrix congruence}
\begin{definition}[congruence]\index{congruence}\cite[281]{horn2012matrix}
Let $A,B\in \F^{n\times n}$. If there exists a nonsingular matrix $S$ such that 
\begin{itemize}
    \item $B=SAS^T$, the $B$ is said to be \textbf{congruent} to $A$.
    \item $B=SAS^H$, the $B$ is said to be \textbf{*congruent} to $A$.
\end{itemize}
\end{definition}

\begin{lemma}
Both \textbf{congruence} and \textbf{*congruence} are equivalence relationship.
\end{lemma}
\begin{proof}
This can be easily showed that transitivity, reflectivity and symmetric are satisfied using the property of $(S^T)^{-1} = (S^{-1})^T$ and nonsingular matrix form a group.
\end{proof}



\begin{definition}[inertia]\index{inertia}\cite[280]{horn2012matrix}
Let $A,B\in \F^{n\times n}$ be \textbf{Hermitian}. The \textbf{inertial} of $A$ is the ordered triple
$$i(A) = (i_+(A),i_-(A),i_0(A))\in \mathbb{N}^3$$
\end{definition}

\begin{definition}[inertia matrix]\index{inertia matrix}\cite[282]{horn2012matrix}
The inertial matrix for a Hermitian matrix $A$ is defined as
$$I(A) = I_{i_+}\oplus I_{i_-} \oplus 0_{i_0}$$
\end{definition}


\begin{lemma}
Each Hermitian matrix $A$ is \textbf{*congruent} to its inertia matrix.
\end{lemma}
\begin{proof}
Since $A$ is Hermitian, based on spectral decomposed theorem, we have
$A = U\Lambda U^H$. 
We can rewrite $\Lambda$ as:
$$\Lambda = DI(A)D$$
where $$D=diag(\lambda_1^{1/2},...,\lambda_{i_+}^{1/2},\lambda_{i_+ + 1}^{1/2},...,-\lambda_{i_+ + i_-}^{1/2},1,...,1)$$
and 
$A = U\Lambda U^H = UDI(A)DU^H = SI(A)S^H$ where $S$ is non-singular. Therefore $A$ is *congruence to its inertia matrix.
\end{proof}


\begin{theorem}[Sylvester inertia theorem]\cite[282]{horn2012matrix}\index{Sylvester inertia theorem}\label{ch:linearalgebra:th:Sylvesterinertiatheorem}
Hermitian matrices $A,B$ are *congruence if and only if they have the same inertia.That is
$$i(A) = i(SAS^H)$$
\end{theorem}
\begin{proof}
(1) forward: If $A$ and $B$ have the same inertial, then both of them will *congruence to the same inertia matrix, and then $A$ and $B$ will be *congruence to each other since *congruence is equilvalence relation.(2) converse: 
\end{proof}

















\subsection{Complex symmetric matrix}
\begin{mdframed}
\textbf{Caution!}\\
\begin{itemize}
    \item Complex symmetric matrices are not Hermitian, and therefore do not admit spectral decomposition.
    \item Complex symmetric matrices are \textbf{fundamentally different from} real symmetric matrices, and therefore do not admit spectral decomposition.
\end{itemize}


\end{mdframed}

\subsection{Unitary, orthonormal \& rotation matrix}
\begin{definition}[unitary matrix]\index{unitary matrix}
A complex square matrix $U$ is unitary if
$$U^HU=UU^H = I$$
A real orthonormal matrix is also unitary. 
\end{definition}

\begin{theorem}
Any eigenvalue of an unitary matrix has absolute value 1 for real eigenvalue and modulus 1 for complex eigenvalue.
\end{theorem}
\begin{proof}
$$Rx = \lambda x \Rightarrow \norm{Rx} = \norm{\lambda x} = \abs{\lambda}\norm{x} = \norm{x} \Rightarrow \abs{\lambda} = 1$$
where we have used the preservation of length.
\end{proof}



\begin{definition}[orthonormal matrix]\index{orthonormal matrix}
A real square matrix $A$  is orthonormal matrix if
$$A^TA = AA^T = I$$
\end{definition}


\begin{lemma}
An orthonormal matrix has determinant value of 1 or -1.
\end{lemma}
\begin{proof}
$Det(AA^T) = 1 = det(A)det(A^T)=det(A)^2$, where we have use $det(A)=det(A^T)$.	
\end{proof}


\begin{definition}[rotation matrix]\index{rotation matrix}
An orthonormal matrix $A$ is called a rotation matrix if $det(A)=1$.
\end{definition}

\begin{lemma}[preservation of length]
Let $v\in \R^n$, and $A \in \R^{n\times n}$ is orthonormal matrix, then
$$\norm{v}_2 = \norm{Av}_2$$
\end{lemma}
\begin{proof}
$$\norm{Av}_2^2 = v^TA^TAv = v^Tv = \norm{v}^2_2$$
\end{proof}


\begin{theorem}
Any eigenvalue of an orthonormal has absolute value 1 for real eigenvalue and modulus 1 for complex eigenvalue.
\end{theorem}

\begin{proof}
$$Rx = \lambda x \Rightarrow \norm{Rx} = \norm{\lambda x} = \abs{\lambda}\norm{x} = \norm{x} \Rightarrow \abs{\lambda} = 1$$
where we have used the preservation of length.
\end{proof}


\begin{theorem}
Any rotation matrix $A$ in $\R^3$ has a real eigenvalue of 1. The eigenvector of this eigenvalue is called axis of rotation.
\end{theorem}

\begin{proof}
The characteristic polynomial of $A$ is a 3 degree polynomial with real coefficients, therefore it must have one real eigenvalue. If it has three real eigenvalues, then each of them is 1 or -1. Because the determinant is 1, which is the product the eigenvalues, therefore one eigenvalue must be 1. If it has one pair complex conjugated eigenvalue $a+bi,a-bi$, the real eigenvalue cannot be -1: $(a-bi)(a-bi)(-1) = -a^2-b^2 <0$.
\end{proof}





\section{Singular Value Decomposition theory}\index{SVD}\index{singular value decomposition}
\subsection{SVD fundamentals}
\begin{theorem}[complete form SVD]\label{ch:linearalgebra:th:SVD}
\cite[142]{calafiore2014optimization} Any matrix $A \in \R^{m\times n}$ can be factored as
$$A = U\Sigma V^T$$
where $U\in \R^{m\times m}, V\in \R^{n\times n}, \Sigma \in \R^{m\times n}$ and $\Sigma$ is rectangle diagonal matrix, and the diagonal entries  have first $r = rank(A)$ \textbf{non-zero/positive decreasing entries}$(\sigma_1,...,\sigma_r)$, all others are zeros. Moreover, $\sigma_i^2 = \lambda_i(AA^T) = \lambda_i(A^TA)$ and $u_i$ and $v_i$ are eigenvectors of $A^TA$ and $AA^T$.
\end{theorem}

\begin{proof}
(1) consider the matrix $AA^T$, which is symmetric and therefore diagonalizable. Let $AA^T = \sum_{i=1}^r \lambda_i u_iu_i^T$, where $u_i$s are orthonormal set. (2) consider the matrix $A^TA$, which is symmetric and therefore diagonalizable. Let $A^TA = \sum_{i=1}^r \lambda_i v_iv_i^T$, where $v_i$s are orthonormal set. (3) We claim that $$u_i = \frac{Av_i}{\sqrt{\lambda_i}},v_i = \frac{A^Tu_i}{\sqrt{\lambda_i}}$$
Because $A^TAv_i = \lambda_i v_i \Rightarrow AA^TAv_i = \lambda_i Av_i$, therefore $u_i$ and $Av_i$ only differ by a scalar multiplication $c$. Similarly, $A^Tu_i$ and $v_i$ only differ by a scalar multiplication. Then $u_i^T u_i = c^2 v_i^TA^TAv_i = c^2 \lambda_i = 1 \Rightarrow c = 1/\sqrt{\lambda_i}$
(3) $u_i^TAv_j = \delta_{ij} = \sqrt{\lambda_i} = \sigma_i \Rightarrow U^TAV = \Sigma$
\end{proof}


\begin{note}[interpretation of blocks]\hfill
\begin{itemize}
    \item The first $r$ columns of $U$ span the range space of $\cR(A)$, the last $n-r$ columns span $\cR(A)^\perp$, and by fundamental theorem of linear algebra, $\cR(A)^\perp = \cN(A^T)$.
    \item The first $r$ columns of $V$ span a subspace that will contribute to the final result (we denote as $\cN(A)^{\perp}$, and by fundamental theorem of linear algebra $\cN(A)^{\perp} = \cR(A^T)$), while the last $n-r$ columns span the null space $\cN(A)$, which will not contribute to the final result. 
    \item The working mechanism of SVD is: first map/decompose the vector into two subspace(one space will contribute, and one null space), then only scale the components in the contributing space, finally project to the range space span by the first $r$ columns in $U$. 
\end{itemize}
\end{note}

\begin{remark}[redundant information in full form SVD]\hfill
\begin{itemize}
    \item If we change the entries in the last $n-r$ columns of $V$, the resulting matrix from product $U\Sigma V^T$ will not change. 
    \item If we change the entries in the last $m-r$ columns of $U$, the resulting matrix from product $U\Sigma V^T$ will not change. 
    \item Therefore, the only equivalent information in $A$ is $\cR(A),\cN(A)^\perp$, and the relationship between the two subspace. 
\end{itemize}
\end{remark}

\begin{remark}[relationship between $U$ and $V$]
It is common mistake to think that $U$ and $V$ are orthogonal to each other, i.e. $U^TV = I$. Actually, $U$ and $V$ only have relations when $A$ is symmetric:
\begin{itemize}
	\item $U$ is the eigenvectors of $A^TA$, $V$ is the eigenvector of $AA^T$.  
	\item If $A$ is not square, $U$ and $V$ cannot even multiply together. 
	\item If $A$ is symmetric, $U$ and $V$ are both eigenvectors of $A^2$ and $A$.
\end{itemize}
\end{remark}



\begin{corollary}[compact form SVD]
\cite[146]{calafiore2014optimization}Any matrix $A \in \R^{m\times n}$ can be factored as
$$A = \sum_{i=1}^r \sigma_i u_i v_i^T = U_r\Sigma_r V^T_r$$
where $U_r\in \R^{m\times r}, V_r\in \R^{r\times n}, \Sigma_r \in \R^{r\times r}$ and $\Sigma = diag(\sigma_1,...,\sigma_r)$ is diagonal matrix, and the diagonal entries being non-zero/positive decreasing entries.Moreover, $\sigma_i^2 = \lambda_i(AA^T) = \lambda_i(A^TA)$ and $u_i$ and $v_i$ are eigenvectors of $A^TA$ and $AA^T$.
\end{corollary}


\begin{lemma}[SVD of inverse]
Let $A$ be a invertible matrix with SVD as
$$A = U\Sigma V^T$$
then
$$A^{-1} = V\Sigma^{-1}U^T$$
\end{lemma}
\begin{proof}
$A^{-1} = (U\Sigma V^T)^{-1} = V^{-T}\Sigma^{-1}U^{-1} = V\Sigma^{-1}U^T$.
\end{proof}






\subsection{SVD and matrix norm}
\begin{theorem}[Frobnius norm]\index{Frobnius norm}\label{ch:linearalgebra:th:matrixFrobniusnormsingularvalueeigenvalue}
For any matrix $A\in \R^{m\times n}$, then
$$\norm{A}_F^2 = \sum_{i=1}^r \sigma^2_i$$
where $\sigma_i$ are singular values of $A$.
\end{theorem}
\begin{proof}
$\norm{A}_F^2 = Tr(AA^T) = Tr(U\Sigma^2 U^T) = Tr(U^TU \Sigma^2) = Tr(\Sigma^2)$
\end{proof}


\begin{theorem}[matrix 2-norm]\index{matrix 2-norm}\label{ch:linearalgebra:th:matrix2normsingularvalueeigenvalue}
For any matrix $A\in \R^{m\times n}$, then
$$\norm{A}_2^2 =  \sigma^2_1$$
or $$\norm{A}_2 =  \sigma_1$$

where $\sigma_1$ is the largest singular value of $A$.

Particularly, if $A$ is symmetric, then 
$$\norm{A}_2 = \max_{i} \abs{\lambda_i}.$$
\end{theorem}

\begin{proof}
Because $\norm{Ax}_2^2 = x^TA^TAx$, from Rayleigh quotient theorem(\autoref{ch:linearalgebra:th:Raylleighquotient}), we know that for $\norm{x}=1$, the maximum value of $x^TA^TAx = \lambda_{max}(A^TA) = \sigma_1^2$.
\end{proof}



\begin{lemma}[condition number from SVD]\index{condition number}
Let $A$ be a square matrix, then the condition number $cond$ of $A$:
$$cond = \frac{\norm{A}_2}{\norm{A^{-1}}_2} = \frac{\sigma_{max}}{\sigma_{min}}$$
\end{lemma}



\subsection{SVD  vs. eigendecomposition}



\begin{lemma}
	If $A \in \R^{n\times n}$ and is symmetric positive semi-definite, then $\lambda_i = \sigma_i$ in sorted order. Moreover, if $A = U\Sigma V^T$ via SVD and $A = W\Lambda W^T$ via eigen-decomposition, then $U=V=W$. (If $A$ is real-symmetric, then $U=V=W$ up to the $\pm$ sign.)
\end{lemma}
\begin{proof}
	From SVD, then $U,V$ are both the eigenvectors of $AA$; From eigen-decomposition, we have $A = W\Lambda W^T$, and therefore $AA = W\Lambda^2W^T$, and therefore $W = U = V$.
\end{proof}


\begin{itemize}
	\item From the derivation of singular value, we know that $\sigma_i^2 = \lambda_i(A^TA) = \lambda_i (A^2) = \lambda_i^2(A)$.
\end{itemize}



\begin{corollary}
	If $A \in \R^{n\times n}$ and is symmetric, then $\abs{\lambda_i} = \sigma_i$ in sorted order.
\end{corollary}


\begin{mdframed}
	\textbf{Caution!}\\
	For general square matrix $A$, the eigenvalue of $A$ might not have simple relations to singular values. 
\end{mdframed}


\subsection{SVD low rank approximation}\index{low rank approximation}


\subsubsection{Frobenius norm low rank approximation}
\begin{lemma}[Unitary invariance of Frobenius norm]\index{Frobenius norm}
For all $A\in \R^{m\times n}$, $\norm{A}_F = \norm{QAR}_F$ for $Q,R$ are orthonormal matrices. 
\end{lemma}
\begin{proof}
Use the fact that $\norm{A}_F^2 = Tr(AA^T) = Tr(A^TA)$, then $\norm{QAR}_F^2 = \norm{QARR^TA^TQ^T} = \norm{Q^TQAA^T} = \norm{A^TA} = \norm{A}$.
\end{proof}

\begin{theorem}[Frobenius norm low rank approximation]\label{ch:linearalgebra:th:SVDFrobeniusnormlowrankapproximation}
\cite[153]{calafiore2014optimization}Let $A \in \R^{m\times n}$, with $rank(A)=r$, then minimization problem
$$\min_{A_k \in \R^{m\times n},rank(A_k)=k} \norm{A-A_k}^2_F$$
with $1\leq k\leq r$ has the solution
$$A_k^* = \sum_{i=1}^k \sigma_i u_i v_i^T$$
with the optimal value of $\sum_{i=k+1}^r \sigma^2_i$
\end{theorem}
\begin{proof}
Use orthonormal invariance of Frobnius norm, we have $$\norm{\Sigma - U^TA_kV}_F^2$$
Let $Z = U^TA_kV$, $Z$ is better to be diagonal(since off-diagonal terms only make things worst). Then best diagonal matrix $Z$ of rank $k$ can be is first $k$ entries equal $\sigma_i$.
\end{proof}

\begin{corollary}[rank approximation alternative formulation]
Let $S$ be a matrix of size $m\times n$. Let $S$ have SVD given by
$$S = U\Sigma V^T.$$

It follows that
\begin{itemize}
	\item the value of 
	$$\norm{S-P}_F^2 = Tr((S-P)(S-P)^T)$$
	is minimum among matrices $P$ of the same size but of rank $r \leq rank(S)$, when $P = U_rU^T_rS$, where $U_r$ is $m\times r$ and the columns of $U_r$ are the $r$ normalized eigenvectors of $SS^T$ with the $r$ largest eigenvalues (or the first $r$ columns of $U$).  
	\item Alternatively,  $P = SV_rV_r^T$, where $V_r$ is $r\times n$ and the columns of $V_r$ are the $r$ normalized eigenvectors of $S^TS$ with the $r$ largest eigenvalues (or the first $r$ columns of $V$).
\end{itemize}
\end{corollary}
\begin{proof}
Note that 	$$P = \sum_{i=1}^r \sigma_i u_i v_i^T$$
\end{proof}


\begin{lemma}
For any matrix $A\in \R^{n\times d}$, let $A_k = \sum_{i=1}^k \sigma_i u_iv_i^T, k\leq rank(A)=r$, then
$$\norm{A-A_k}_2 = \sigma_{k+1}$$
\end{lemma}
\begin{proof}
Let $A = \sum_{i=1}^r \sigma_i u_iv_i^T$ be the SVD of $A$, then we have
$$A-A_k = \sum_{i=1+k}^r \sigma_i u_iv_i^T$$
Based on the definition of 2-norm, we have
$$\norm{A-A_k}^2_2 = \max_{\norm{x}=1} \norm{(A-A_k)x}^2_2$$
In order to maximize the above, $x$ should lie in the subspace spanned by $v_{k+1},v_{k+2},...,v_r$, and we write $x=\sum_{i=k+1}^r a_i v_i$
then we have
$$\norm{(A-A_k)x}^2_2 = \sum_{i=1+k}^r a_i^2 \sigma_i^2 \leq \sigma^2_{k+1}\sum_{i=k+1}^r a_i^2 =\sigma^2_{k+1} $$
and the maximum is attained at $x=v_{k+1}$
\end{proof}




\subsubsection{Two-norm low rank approximation}
\begin{lemma}[Unitary invariance of 2-norm]
	For all $A\in \R^{m\times n}$, $\norm{A}_2 = \norm{QAR}_2$ for $Q,R$ are orthonormal matrices. 
\end{lemma}
\begin{proof}
	Use the fact that $\norm{Ax}_2^2 = xAA^Tx$ , then $\norm{QARx}_2^2 = x^tR^TA^TQ^TQARx = x^tR^TA^TARx $, and $\norm{x} = \norm{Rx}$, therefore
	$$\frac{\norm{Ax}}{\norm{x}} = \frac{\norm{ARx}}{\norm{Rx}} = \frac{\norm{Ay}}{\norm{y}}$$
\end{proof}


\begin{theorem}[matrix 2-norm low rank approximation]\index{matrix 2-norm}\label{ch:linearalgebra:th:SVD2normlowrankapproximation}
Let $A \in \R^{m\times n}$, with $rank(A)=r$, then minimization problem
$$\min_{A_k \in \R^{m\times n},rank(A_k)\leq k} \norm{A-A_k}^2_2$$
with $1\leq k\leq r$ has the solution
$$A_k^* = \sum_{i=1}^k \sigma_k u_i v_i^T$$
and 
$$\norm{A-A_k^*}_2 = \sigma^2_{k+1}$$
\end{theorem}
\begin{proof}
Since $A_k$ has at most rank $k$, then its null space has at least dimensionality of $n-k$ based on then rank-nullity theorem. Consider the subspace $S$ spanned by $ v_{1},v_{2},...,v_{k+1}$, then $S\cap \cN(A_k)\neq \emptyset$, based on dimensionality argument ($dim(S)+dim(\cN{A_k})>n$). Let $x\in S\cap \cN(A_k),\norm{x}=1$, then $x=\sum_{i=1}^{k+1}a_i v_i$. We have
$$\norm{(A-A_k)x} = \norm{Ax} \geq \sigma_{k+1}$$
where the minimum is attained when $x = v_{k+1}, B=A_k^*$. And therefore $$\norm{A-A_k}_2 \geq \norm{(A-A_k)x} \geq \sigma_{k+1}$$

\textbf{Note:}(1) one might wander why we do not let $S$ spanned by $ v_{1},v_{2},...,v_{k+2}$ and take $x=v_{k+2}$, we can do so and will obtain a looser bound of $$\norm{A-A_k}_2 \geq \sigma_{k+2}$$

(2) On the other hand, if $S$ spanned by $k$ singular vector, then $\norm{A-B} \geq \norm{(A-B)y} \geq \norm{(A-B)}x$, where $x$ is in the subspace spanned by $k$ singular vector, and $y$ is in the subspace spanned by $k+1$ singular vectors. Therefore, we can get tighter bound when use subspace spanned by $k+1$ singular vectors. 
(3) Therefore, we can get the tightest bound when use subspace spanned by $k+1$ singular vectors. Then it is easy to  prove that among all the subspaces span by the $k+1$ singular vectors, the maximum of the minimum in the $k+1$ singular values is $\sigma_{k+1}$. 

\end{proof}




\section{Generalized eigenvectors and Jordan normal forms}
\subsection{Generalized eigenvectors}
\begin{definition}[defective eigenvalue]
	An eigenvalue $\lambda_i$ of matrix $A$ is called \textbf{defective} if its geometric multiplicity ,$dim(\cN(A-\lambda_i I))$, is strictly less than its algebraic multiplicity.  
\end{definition}

\begin{example}
Consider the matrix 
$$A = \begin{bmatrix}
1 & 1\\
0 & 1
\end{bmatrix}.$$
This matrix has eigenvalue $\lambda = 1$ with multiplicity of 2. 
Note that
$$A - I = \begin{bmatrix}
0 & 1\\
0 & 0
\end{bmatrix},$$
which is clear that the range space dimension is 1 and therefore the null space dimension is 1. 

So the eigenvalue 1 is defective.	
\end{example}








\begin{definition}[generalized eigenvector of rank $r$]\index{generalized eigenvector}
For a given eigenvalue $\lambda$, the vector $x$ is a \textbf{generalized eigenvector of rank $r$} if
\begin{align*}
(A - \lambda I)^rx &= 0\\
(A - \lambda I)^{r-1}x &\neq 0.
\end{align*}
Particularly, the eigenvector $v$ is a generalized eigenvector of rank 1 since 
$$(A-\lambda I)v = 0, (A-\lambda I)^0 v = v \neq 0.$$
\end{definition}

\begin{remark}
If a vector $u$ is the generalized eigenvector of rank $s$, then it cannot be the generalized eigenvector of rank $m > s$, because $$(A - \lambda I)^{m}u = 0, (A - \lambda I)^{m-1}u = 0.$$
\end{remark}

\begin{definition}[a chain of generalized eigenvectors of length $r$]
Given an eigenvalue $\lambda$, we say that vectors $v_1,v_2,...,v_r$ form a \textbf{chain of generated eigenvectors of length r} if $v_1\neq 0$ and 
	\begin{align*}
v_{r-1} &= (A - \lambda I)v_r\\
v_{r-2} &= (A - \lambda I)v_{r-1}\\
&\vdots\\
v_{1} &= (A - \lambda I)v_{2}\\
0 &= (A - \lambda I)v_{1}.
\end{align*}	

We can write down the matrix form as
$$A\begin{bmatrix}
v_r\\
v_{r-1}\\
\vdots\\
v_1
\end{bmatrix} = \begin{bmatrix}
\lambda & 1 &  & \\ 
& \ddots & \ddots & \\ 
&  & \ddots & 1\\ 
&  &  & \lambda
\end{bmatrix}\begin{bmatrix}
v_r\\
v_{r-1}\\
\vdots\\
v_1
\end{bmatrix}.$$
\end{definition}




\begin{remark}[generate a chain of generalized eigenvectors of length $r$]
	Given an eigenvalue $\lambda$ and its generalized eigenvector $u$ of rank $r$, that is
	\begin{align*}
	(A - \lambda I)^ru &= 0\\
	(A - \lambda I)^{r-1}u &\neq 0.
	\end{align*}
	We can define vectors $v_1,v_2,...,v_r$ as follows
	\begin{align*}
	v_r &= (A - \lambda I)^0 u = u\\
	v_{r-1} &= (A - \lambda I)^1 u \\
	\vdots
	v_{1} &= (A - \lambda I)^{r-1} u 
	\end{align*}	
\end{remark}

\begin{theorem}[linear independence among a chain of generalized eigenvectors] \label{linearIndependenceChainofGeneralizedEigenvectors}
The vectors in a chain of generalized eigenvectors, $v_1,v_2,...,v_r$ given by, 
	\begin{align*}
v_{r-1} &= (A - \lambda I)v_r\\
v_{r-2} &= (A - \lambda I)v_{r-1}\\
&\vdots\\
v_{1} &= (A - \lambda I)v_{2}\\
0 &= (A - \lambda I)v_{1},
\end{align*}
 are linearly independent.	
\end{theorem}
\begin{proof}
We consider the linear combination
$$\sum_{i=1}^{r} a_iv_i = 0.(*)$$
Using the chain definition, we have
$$v_i = (A - \lambda I)^{r-i}v_r;$$
Equation (*) becomes 
$$\sum_{i=1}^{r} a_i(A - \lambda I)^{r-i}v_r = 0.(**)$$

We multiply $(A-\lambda I)^{r-1}$ to (**), we get
$$ a_r(A - \lambda I)^{r-1}v_r = a_rv_1 = 0 \implies a_r = 0.$$
Similarly, we multiply $(A-\lambda I)^{r-2}$ to (**), we get
$$ a_{r-1}(A - \lambda I)^{r-2}v_{r-1} = a_{r-1}v_1 = 0 \implies a_{r-1} = 0.$$
We can continue to prove $$a_1=a_2=...=a_r = 0.$$ 
Therefore, $$v_1,v_2,...,v_r$$ are linearly independent.
\end{proof}



\begin{definition}[generalized eigenvector]\index{generalized eigenvector}
	A generalized eigenvector $x$ for eigenvalue $\lambda$ is a solution to $(A-\lambda I)^k x = 0$.
\end{definition}

\begin{definition}[generalized eigenspace]\index{generalized eigenspace}
	The generalized eigenspace $G(\lambda,A)$ is the set of all generalized eigenvectors associated with the eigenvalue $\lambda$ of matrix $A$.
\end{definition}

\begin{lemma}
	If $x \in \cN(A-\lambda I)$ with k be any positive integer, then $$x \in \cN((A-\lambda I)^k)$$
	that is a eigenvector of $\lambda$ is also a generalized eigenvector of $\lambda$.
\end{lemma}
\begin{proof}
	$$(A-\lambda I )^k v = (A-\lambda I )^{k-1} (A-\lambda I )v = 0$$
\end{proof}


\begin{mdframed}
	\textbf{Caution! Possible linear dependence between different generalized eigenvectors.} Let $\lambda_i$ be a eigenvalue with multiplicity of $k>1$, then the generalized eigenvector solved from $(A-\lambda_i)^m v_1 = 0$ and $(A-\lambda_i)^n v_2 = 0$, where $m \neq n$, then $v_1$ might linearly depend on $v_2$. \\
	Suppose $m < n$, then $v_1$ is also the solution of $(A-\lambda_i)^n v_2 = 0$.
\end{mdframed}

\begin{lemma}[The dimensionality of generalized eigenspace]
	Let $\lambda_i$ be a eigenvalue with algebraic multiplicity of $k>1$, then the generalized eigenspace associated with $\lambda_i$ has dimensionality $k$.
\end{lemma}
\begin{proof}
	TODO
\end{proof}


\subsection{Upper triangle matrix and nilpotent matrix}
\begin{theorem}
	\cite[149]{axler2015linear}
	Let $A\in \C^{n\times n}$. There exists a nonsingular $S$ such that $$T = S^{-1}AS$$ is upper-triangular. Moreover, $T$ has the same eigenvalues as $A$ with the same multiplicity, showing \textbf{on the diagonal}. 
\end{theorem}
\begin{proof}
	for the existence proof, see ref.
\end{proof}

\begin{lemma}
	The finite powers of a upper-triangular matrix (all diagonal entries $a_{11},a_{22},...a_{nn}$ are nonzeros) $A\in \F^{n\times n}$, i.e., $A^k$, will still be upper triangular((all diagonal entries are nonzero). Moreover, the diagonal terms of $A^k$ is $a_{11}^k,a_{22}^k,...,a_{nn}^k$ 
\end{lemma}
\begin{proof}
	can be directly proved via matrix multiplication.
\end{proof}


\begin{definition}[nilpotent matrix]
	An nilpotent matrix $A$ is an $n\times n$ matrix such that there exists a finite power $k \leq n$ for which $A^k = 0$.
\end{definition}

\begin{lemma}
	If $A\in \F^{n\times n}$ is nilpotent, then $A^n = 0$. 
\end{lemma}
\begin{proof}
	by definition,  there exists $k \leq n$ for which $A^k = 0$.
\end{proof}


\begin{definition}[strictly upper-triangular matrix]
	A matrix $A\in \F^{n\times n}$ is strictly upper-tiangular if all entries on and below the diagonal are 0.
\end{definition}


\begin{lemma}
	For a strictly upper-triangular matrix $A\in \F^{n\times n}$, $A$ is nilpotent and $A^n = 0$.
\end{lemma}
Proof: consider how $A^k$ acts on standard basis $e_1,e_2,...,e_n$. 
\begin{align*}
Ae_1 &= 0\\
Ae_2 &\in span(e_1)\\
A^2 e_2 &= 0\\
Ae_3 &\in span(e_1,e_2)\\
A^2e_3 &\in span(e_1)\\
A^3e_3 & =0\\
\dots
\end{align*}


\begin{lemma}
	\cite[242]{axler2015linear}
	Let $A \in \F^{n\times n}$, then
	\begin{itemize}
		\item $$\{0\} = \cN(A^0) \subseteq \cN(A^1) \subseteq \cN(A^2) \subseteq \cN(A^3) \subseteq \dots $$
		\item If there is an nonnegative integer $m$ such that $\cN(A^m) = \cN(A^{m+1})$,
		then $\cN(A^m) = \cN(A^{m+1}) = \cN(A^{m+2}) = \cN(A^{m+3}) ...$
		\item If there is an nonnegative integer $m$ such that $\cN(A^m) \subsetneq \cN(A^{m+1})$, then for all $k \leq m$
		$$\cN(A^k) \subsetneq \cN(A^{k+1})$$
	\end{itemize}
\end{lemma}


\begin{proof}
	(1) $A^k x = 0 \Rightarrow A A^k x = A^{k+1}x = 0$;
	(2) Suppose there exist an integer $k > 0$ such that 
	$$\cN(A^m+k) \neq \cN(A^{m+1+k})$$
	or equivalently
	$$\cN(A^m+k) \subsetneq \cN(A^{m+1+k})$$
	then there exists a $v$ such that $A^{m+1+k}x = 0$ but $A^{m+k}x \neq 0$, then for
	$$A^{m+1}A^{k}x = 0, A^{m}A^k x \neq 0$$
	therefore, $\cN(A^{m+1}) \subsetneq \cN(A^m)$ because $A^k x$ is in the former but not the latter. 
	(3) directly from (2) by contradiction.
\end{proof}


\begin{remark}
	For similar inequality of range, see \cite[251]{axler2015linear}.
\end{remark}

\begin{lemma}[null space saturation]
	Let $A \in \R^{n\times n}$, then
	$$\cN(A^n) = \cN(A^{n+1}) = \cN(A^{n+2})...$$
\end{lemma}

\begin{proof}
	Suppose $\cN(A^n) \subsetneq \cN(A^{n+1})$, then from above theorem, we have
	for $k \leq n$, $\cN(A^k) \subsetneq \cN(A^{k+1})$, as a result, the dimensionality of $\cN{A^n}$ will be $n+1$, which is impossible for a n by n matrix.
\end{proof}


\subsection{Jordan normal forms}

\begin{definition}[Jordan basis, Jordan canonical form]\index{Jordan basis}
\cite[273]{axler2015linear}
Suppose $T\in \cL(V)$. A basis of $V$ is called a \textbf{Jordan basis} for $T$ if with respect to this basis $T$ has a block diagonal matrix
$$\begin{pmatrix}
A_1 & ~ & 0\\
~ & \ddots & ~ \\
0 & ~ & A_p
\end{pmatrix}$$
where each $A_j$ is an upper triangular matrix of the form
$$
\begin{pmatrix}
\lambda_j & 1 & ~ & 0\\
~ & \ddots & \ddots & 0 \\
~ & ~ & \ddots & 1 \\
0 & ~ & ~ & \lambda_j
\end{pmatrix}$$
which is known as \textbf{Jordan block}.
\end{definition}

\begin{example}
	The matrix $B$ is a Jordan basis(Jordan canonical form)
	$$B=\begin{pmatrix}
	2 & 1 & 0 & 0 & 0 & 0\\ 
	0 & 2 & 0 & 0 & 0 & 0\\ 
	0 & 0 & 3 & 0 & 0 & 0\\ 
	0 & 0 & 0 & 3 & 1 & 0\\ 
	0 & 0 & 0 & 0 & 3 & 1\\ 
	0 & 0 & 0 & 0 & 0 & 3
	\end{pmatrix}$$
	
\end{example}

\begin{lemma}[Jordan decomposition]\index{Jordan form}\index{Jordan form decomposition}
\cite[273]{axler2015linear}Suppose $V$ is a complex vector space. If $T\in \cL(V)$, then there is a basis of $V$ that is a Jordan basis for $T$. In matrix form, we have
$$M = PJP^{-1}$$
where $J$ is the Jordan basis and $P$ is the invertible matrix. 
\end{lemma}


\begin{remark}
If matrix $A$ is diagonalizable, then Jordan decomposition reduce to eigendecomposition.
\end{remark}

 

\begin{lemma}[matrix function of Jordan block]
\cite{wiki:matrixfunction}\cite[600]{meyer2000matrix} Let $f(z)$ be an analytical function of a complex argument. Applying the function on a Jordan block $A\in \R^{n\times n}$ given as
$$
\begin{pmatrix}
\lambda & 1 & ~ & 0\\
~ & \ddots & \ddots & 0 \\
~ & ~ & \ddots & 1 \\
0 & ~ & ~ & \lambda
\end{pmatrix}$$
then $f(z)$ is given as
$$
\begin{pmatrix}
f(\lambda) & f'(\lambda) & ~ & \frac{f^{n-1}(\lambda)}{(n-1)!}\\
~ & \ddots & \ddots & 0 \\
~ & ~ & \ddots & \frac{f^{n-1}(\lambda)}{(n-1)!} \\
0 & ~ & ~ & f(\lambda)
\end{pmatrix}$$
\end{lemma}
\begin{proof}
use Taylor expansion on $f(z)$, which is analytical and Taylor series exists. 
\end{proof}
 



\begin{lemma}[The power of Jordan block]
Let $A$ be a Jordan block given as
$$
\begin{pmatrix}
\lambda_j & 1 & ~ & 0\\
~ & \ddots & \ddots & 0 \\
~ & ~ & \ddots & 1 \\
0 & ~ & ~ & \lambda_j
\end{pmatrix}$$
then $A^m$ is given as
$$
\begin{pmatrix}
\lambda^m & \binom{m}{1}\lambda^{m-1} & \dots & \binom{m}{n-1}\lambda^{m-n+1}\\
~ & \ddots & \ddots & 0 \\
~ & ~ & \ddots & \binom{m}{1}\lambda^{m-1} \\
0 & ~ & ~ & \lambda^m
\end{pmatrix}$$
\end{lemma}
\begin{proof}
this can be directly verified.
\end{proof}


\begin{lemma}[The exponential of Jordan block]
Let $A\in \R^{n\times n}$ be a Jordan block given as
$$
\begin{pmatrix}
\lambda & 1 & ~ & 0\\
~ & \ddots & \ddots & 0 \\
~ & ~ & \ddots & 1 \\
0 & ~ & ~ & \lambda
\end{pmatrix}$$
then $\exp(At)$ is given as 
$$
\begin{pmatrix}
\exp(\lambda t) & \binom{m}{1 }\lambda t \exp(\lambda t) & \dots & \binom{m}{n-1}(\lambda t)^{m-n-1}\exp(\lambda t) \\
~ & \ddots & \ddots & 0 \\
~ & ~ & \ddots & \binom{m}{1 }\lambda t \exp(\lambda t) \\
0 & ~ & ~ & \exp(\lambda t)
\end{pmatrix}$$
\end{lemma}

\begin{lemma}[conditional boundedness of matrix exponential]\index{matrix exponential}
Given an arbitrary square matrix $A$, if all its eigenvalue has a negative real part, then $\exp(At) \to 0$ as $t \to \infty$.
\end{lemma}
\begin{proof}
directly from above.
\end{proof}



\begin{lemma}[conditional boundedness of matrix power]\index{matrix power}\label{ch:linearalgebra:th:boundedmatrixpower}
Given an arbitrary square matrix $A$, if all its eigenvalue $\abs{\lambda_i} < 1$(absolute sign is interpreted as modulus for complex number), then $\norm{A^m}_2$ is bounded for any positive number $m$.
Moreover, $A^m \to 0$ as $m \to \infty$.
\end{lemma}
\begin{proof}
From Jordan decomposition, $A = PJP^{-1}$, then
$A^m = P J^m P^{-1}$. Note that $J^m$ has diagonal entries of $\lambda^m_i$. Therefore, $J^m$ is a matrix with every entry finite(including the corner terms $\binom{m}{n} \lambda^m \to 0$), therefore $A^m$ is bounded for every $m$ and will go to 0.
\end{proof}




\section{Matrix factorization}
\subsection{Orthogonal-triangular decomposition}
\begin{theorem}[QR decomposition properties]\index{QR decomposition}\label{ch:linearalgebra:th:QRDecompositionProperties}
Suppose $A\in \R^{m\times n},m\geq n$. Then we have
\begin{itemize}
	\item there exists a orthonormal matrix $Q\in \R^{m\times m}$ and upper triangular matrix $R\in \R^{m\times n}$ such that $$A = QR$$
	\item If $\hat{Q}\in \R^{n\times n}$ and $\hat{R}\in \R^{n\times n}$, then
	$$A = QR=[\hat{Q},N]\begin{bmatrix}
	\hat{R}\\
	0
	\end{bmatrix} = \hat{Q}\hat{R}$$
	where $\hat{Q}\in \R^{m\times n}$ consists of the basis of $\cR(A)$, $N$ consists of the basis of $\cN(A^T)$ and $\hat{R}\in \R^{n\times n}$.  
	\item We can choose $R$ to have nonnegative diagonal entries
	\item If $A$ is of full rank, we can choose $R$ with positive diagonal entries, in which case the economical form $\hat{Q}$ and $\hat{R}$ will be unique. 
	\item If $A$ is square nonsingular, then $A = QR$ is unique.
\end{itemize}
\end{theorem}
\begin{proof}
(1)(2) Consider the Gram-Smith process for the columns of matrix $A$ given as
\begin{align*}
	q_1 &= a_1, p_1=q_1/\norm{1_1}\\
	q_i & =a_i - \sum_{j=1}^{i-1} \ip{a_i,p_j}p_j,p_i = p_i/\norm{q_i},i=2,...,n
\end{align*}
or
\begin{align*}
a_1 &= r_{11}p_1\\
a_j &= \sum_{i=1}^j r_{ij}p_i,j=2,...,n\\
r_{ii} &= \norm{q_i}, r_{ij} = \ip{a_j,p_i}
\end{align*}
in which orthonormal basis $p_1,...,p_n$ for the column space $span(a_1,...,a_n)$ will be produced. 
We can see that $a_i \in span(p_1,...,p_i)$, and therefore in matrix form we have
$$A = \hat{Q}\hat{R}$$
where $\hat{Q} \in \R^{m\times n}$ will consist of $p_1,...,p_n$ as columns and $R\in \R^{n\times n}$ will an upper triangular matrix. 
The complete form $Q$ can be augmented with basis of $\cN(A^T) = \cR(A)^\perp$, such that $Q\in \R^{m\times m}$ consist of the complete orthonormal basis of $\R^m$.
(3)(4)(5) If $a_1,...,a_n$ are linearly independent, from GS process, the matrix $\hat{Q},\hat{R}$ are uniquely determined, and the diagonal entries of $R$ is always positive. If $a_1,...,a_n$ are linearly dependent, there will exist scenario that 
$$a_k \in span(p_1,...,p_{k-1})$$ and we can set $r_{kk} = 0$. 
\end{proof}

\begin{remark}
\textbf{QR decomposition is the matrix form of Gram-Smith procedures.} 
\end{remark}


\subsection{LU decomposition}
\begin{definition}[LU decomposition with partial pivoting, LUP]\index{LU decomposition}
The LU decomposition with partial pivoting for a square matrix $A$ is given as
$$PA = LU$$
where $P$ is the permutation matrix (to reorder rows in $A$), $L$ and $U$ are the lower and upper triangle matrix. 
\end{definition}

\begin{remark}[existence and uniqueness]\hfill
	\begin{itemize}
		\item If $P$ is not used to reorder rows, then $LU$ decomposition might not exist.
		\item Any square matrix $A$ admits an $LUP$ decomposition.
		\item The $LUP$ decomposition is not unique(for example, set $L' = -L, U' = -U$).
	\end{itemize}
\end{remark}

\subsection{Cholesky decomposition}
\begin{definition}[Cholesky decomposition]\index{Cholesky decomposition}
The Cholesky decomposition of a Hermitian positive definite matrix $A$ is a decompsition of the form
$$A = LL^H$$
where $L$ is a lower triangular matrix with real and positive diagonal entries, and $L^H$ denotes the Hermitian.
\end{definition}

\begin{remark}[extension to positive semidefinite matrix]
If we allow the diagonal entries to be zero, then positive semi-definite matrix also has Cholesky decomposition(might not be unique).
\end{remark}

\begin{remark}[QR decomposition vs. Cholesky decomposition,existence and uniqueness]\hfill
	\begin{itemize}
		\item Note that any for real symmetric positive definite matrix $A$, we know that $A = BB^T$(\autoref{ch:linearalgebra:th:symmetricdecomposition}). If we do a unique QR decomposition on $B$ as $B^T = QL^T$, then $BB^T = LL^T$.
		\item The Cholesky decomposition for positive semidefinite matrices always exists. It is unique only for positive definite matrices. 
	\end{itemize}
\end{remark}


\section{Positive and non-negative matrix theory}\label{ch:linearalgebra:sec:positive-and-non-negative-matrix-theory}\index{positive matrix}\index{non-negative matrix}
\begin{definition}
If $A=[a_{ij}]$ is matrix, we write:
\begin{itemize}
    \item $A>0$ if all $a_{ij} > 0$, $A$ is called strictly positive.
    \item $A\geq 0$ if all $a_{ij} \geq 0$ and for at least one $a_{ij} > 0$, $A$ is called positive or strictly non-negative.
    \item $A \geqq 0$ if all $a_{ij} \geq 0$, $A$ is called non-negative.
\end{itemize}
\end{definition}

\begin{theorem}[Frobenius-Perron] \index{Frobenius-Perron}\label{ch:linearalgebra:th:Frobeniusperroneigenvalue}
\cite[191]{luenberger1979introduction}If $A > 0$, then there exists $\lambda_0 >0$ and $x_0 > 0$ such that \begin{itemize}
    \item $Ax_0 = \lambda_0 x_0$, $\lambda_0$ is known as the Frobenius-Perron eigenvalue.
    \item if $\lambda_i \neq \lambda_0$ is any eigenvalue of $A$, then $\abs{\lambda} < \lambda_0$;
    \item $\lambda_0$ is an eigenvalue with geometry and algebra multiplicity 1.
\end{itemize}
\end{theorem}

\begin{mdframed}
\textbf{Caution!}\\
Non-negative matrices are not necessarily positive semidefinite; that is, they might have negative eigenvalues.
For example,
$$A = \begin{pmatrix}
1 & 100\\
100 & 1
\end{pmatrix}$$
then $x^TAx <0, x=[1,-100]^T$.
\end{mdframed}



\begin{theorem}[extension to strictly non-negative]
\cite[193]{luenberger1979introduction}Let $A \geq 0$, and suppose there is an integer $m$ such that $A^m > 0$. Then there exists $\lambda_0 \geq 0$ and $x_0 \geq 0$ such that \begin{itemize}
    \item $Ax_0 = \lambda_0 x_0$;
    \item if $\lambda_i \neq \lambda_0$ is any eigenvalue of $A$, then $\abs{\lambda} < \lambda_0$;
    \item $\lambda_0$ is an eigenvalue with geometry and algebra multiplicity 1.
\end{itemize} 
\end{theorem}

\begin{remark}
In the situation that there does not exist a $m$, then the above will not hold. For example, the stochastic matrix for some reducible chains.
\end{remark}

\begin{theorem}[extension to non-negative]
\cite[193]{luenberger1979introduction}Let $A \geqq 0$. Then there exists $\lambda_0 \geq 0$ and $x_0 \geq 0$ such that \begin{itemize}
    \item $Ax_0 = \lambda_0 x_0$;
    \item if $\lambda_i \neq \lambda_0$ is any eigenvalue of $A$, then $\abs{\lambda} < \lambda_0$;
\end{itemize} 
\end{theorem}

\begin{remark}
In the non-negative case, the multiplicity property of the eigenvalue is lost. 
\end{remark}

\begin{theorem}[bounds on Frobenius-Perron eigenvalue]\label{ch:linearalgebra:th:boundednessFrobeniusperroneigenvalue}
Let $A \geqq 0$ with the largest eigenvalue $\lambda_0$, we have
$$\min_i \Delta_i \leq \lambda_0 \leq \max_i \Delta_i$$
and
$$\min_i \delta_i \leq \lambda_0 \leq \max_i \delta_i$$
where $\Delta_i$ is the sum for column $i$, $\delta_i$ is the sum of the row $i$. 
\end{theorem}
\begin{proof}
Let $x$ be the normalized eigenvector corresponds to $\lambda_0$, then $Ax = \lambda_0 x$
Sum up rows, we have $$\sum_{i=1}^n\Delta_1 x_1 = \lambda_0$$ i.e., $\lambda_0$ is the convex combination of $\Delta_i$s. 
Similarly, we have $A^Tx'=\lambda_0 x'$, where $x'$ is the eigenvector for $A^T$.	
\end{proof}



\begin{corollary}[eigenvalues of stochastic matrix]
The stochastic matrix $P$ of an irreducible and aperiodic Markov chain has Frobenius-Perron eigenvalue 1.		
\end{corollary}
\begin{proof}
Directly from \autoref{ch:linearalgebra:th:Frobeniusperroneigenvalue} and \autoref{ch:linearalgebra:th:boundednessFrobeniusperroneigenvalue}.
\end{proof}


\section{Positive definite matrices and quadratic forms}
\subsection{Quadratic forms}
\begin{definition}[quadratic forms]\index{quadratic forms}
For an $n\times n$ matrix $A$, the quadratic form associated with $A$ is defined as
$$x^TAx = \sum_{i,j}a_{ij}x_ix_j$$
\end{definition}

\begin{mdframed}
\textbf{caution!}\\
Note that given a quadratic form  $\sum_{i,j}a_{ij}x_ix_j$, the matrix $A$ satisfying
$$x^TAx = \sum_{i,j}a_{ij}x_ix_j$$
is not unique, unless we require $A$ to be symmetric.
\end{mdframed}


\begin{lemma}[every quadratic form is associated with an unique symmetric matrix]
Given a square matrix $A$ with its associated quadratic form $x^TAx$, there exist an unique symmetric matrix $B$ such that
$$x^TAx = x^TBx$$
where $$B = \frac{1}{2}(A+A^T)$$
That is, every quadratic form is associated with an unique symmetric matrix.
\end{lemma}
\begin{proof}
We have
$$x^TAx = (x^TAx)^T = x^TA^Tx$$
then $$\frac{1}{2}x^T(A+A^T)x = x^TAx$$ is proved.
To show uniqueness, note that by equaling the coefficients, we have $a_{ij} = B_{ij} + B_{ji}$, the symmetry requirement impose $B_{ij} = B_{ji}$ and then therefore given a quadratic form, its associated symmetric matrix is unique.
\end{proof}

\begin{lemma}
Let $A$ be symmetric square matrix. Then $x^TAx = 0$ for every $x\in\R^n$ if and only if $A = 0$.
\end{lemma}
\begin{proof}
(1) forward part is straight forword; (2) The converse part: (a)set $x=e_i$, and we get $e_iAe_i = a_{ii} = 0$; (b) set $x=e_j+e_k$, and we get $x^Ax = 2a_{jk} = 0$
\end{proof}





\subsection{Real symmetric non-negative definite matrix}
\subsubsection{Characterization}
\begin{definition}[non-negative definite, positive definite]\hfill
\begin{itemize}
	\item A square matrix $A \in \R^{n\times n}$ is \textbf{non-negative definite} if
	$$x^T A x \geq 0, \forall x\in \R^n.$$
	\item A square matrix $A \in \R^{n\times n}$ is \textbf{positive definite} if
	$$x^T A x > 0, \forall x\in \R^n, x\neq 0.$$
\end{itemize}	
	
\end{definition}


\begin{lemma}[characterization by eigenvalues and diagonal entries]\label{ch:linearalgebra:th:characterizeNonnegativeDefiniteMatrixbyEigenvalueAndEntries}\hfill
\begin{itemize}
	\item 	Let $A$ be a real symmetric matrix. If  $A$ is non-negative definite, then
	\begin{itemize}
		\item \textbf{non-negative real eigenvalues.}(necessary and sufficient)
		\item \textbf{non-negative diagonal entries}(necessary conditions)
	\end{itemize} 
\item 	Let $A$ be a real symmetric matrix. If  $A$ is positive definite, then
\begin{itemize}
	\item \textbf{positive real eigenvalues.}(necessary and sufficient)
	\item \textbf{positive diagonal entries}(necessary conditions)
\end{itemize} 
\end{itemize}	

\end{lemma}
\begin{proof}
	(1)(necessary)
	The proof of eigenvalues are real directly from the results in symmetric matrix. For the non-negativity, let $u_i$ be an unit eigenvector corresponding to eigenvalue $\lambda_i$, we have
	$$Au_i = \lambda_i u_i \Rightarrow u_i^T A u_i = \lambda_i \geq 0.$$
	(sufficient)
	Let $A$ have eigendecomposition of $A = P\Lambda P^T$. Then for any $x\in \R^n$
	\begin{align*}
	x^TAx &= xP\lambda Px\\
	&= y\lambda y \\
	&= \sum_{i=1}^n y_i^2 \lambda \geq 0
	\end{align*}
	(2) Let $e_i$ be the standard basis, then
	$$e_i^TAe_i = a_ii \geq 0$$
\end{proof}

\begin{lemma}[characterization by submatrix]\label{ch:linearalgebra:th:PositiveDefinitenessCharacterizedbySubmatrix}\hfill
Let $A$ be a $n\times n$ symmetric matrix and $Q = x^TAx,x\in \R^n$. Let $A_k$ be the $k\times k$ submatrix of  $A$ such that $A_k = (A)_{1\leq i\leq k,1\leq j\leq k}$.Then the following statements are equivalent:
	\begin{itemize}
		\item $Q > 0$ for all $x\in \R^n, x\neq 0$.
		\item All eigenvalues of $A$ are positive.
		\item For each $1\leq k\leq n$, $A_k$ is positive definite.
		\item $det(A_k) > 0,$ for $1\leq k\leq n$.
	\end{itemize}	
\end{lemma}
\begin{proof}
(1) is equivalent (2) is from \autoref{ch:linearalgebra:th:characterizeNonnegativeDefiniteMatrixbyEigenvalueAndEntries}. (1) implies (3): Assume $Q > 0$ for all $x\neq 0$. Then for any $1\leq k\leq n$, 
\begin{align*}
0 &< (x_1,...,x_k,0,...,0)A(x_1,...,x_k,0,...,0)^T \\
  &= (x_1,...,x_k)A_k (x_1,...,x_k)^T\\
  &= Q_k
\end{align*}
for all $(x_1,...,x_k)\neq 0$. Therefore, $A_k$ is positive definite.
(3) implies (4): $A_k$ has all positive eigenvalues. The determinant is the product of all eigenvalues. (4) implies (1)(2).  The determinant is the product of all eigenvalues. We can get that every eigenvalue is positive if $det(A_k) > 0,$ for $1\leq k\leq n$. 
\end{proof}

\subsubsection{Decomposition and transformation}

\begin{theorem}[preserving positive definiteness]
Let $A\in \R^{n\times n}$ be a symmetric positive definite matrix,let $P\in \R^{n\times k}$, if $P$ has full column rank, then
$$P^TAP$$ is still symmetric positive definite.
\end{theorem}
\begin{proof}
Since $dim(\cN(P)) = 0$, $Px \neq 0,\forall x\neq 0$, and therefore $y^TAy > 0,$ if $y=Px \neq 0$  
\end{proof}

\begin{corollary}
Let $A\in \R^{n\times n}$ be a symmetric positive definite matrix,and let $P\in \R^{n\times n}$. if $P$ is nonsingular, then
$$P^TAP$$ is still symmetric positive definite.	
\end{corollary}


\begin{theorem}[decomposition]\label{ch:linearalgebra:th:symmetricdecomposition}
	Let $A \in \R^{n\times n}$ be a symmetric matrix.
	\begin{itemize}
		\item $A$ is non-negative definite if and only if there exists $B\in \R^{n\times k}$ such that  $A = BB^T$
		\item $A$ is positive definite if and only if there exists nonsingular $B\in \R^{n\times n}$ such that  $A = BB^T$
	\end{itemize} 
Note that $B$ is usually not unique.
\end{theorem}
\begin{proof}
(1)(a)forward: If $A = BB^T$, then for any $x\in \R^n$, $x^TAx = (xB)^T(Bx) = \norm{Bx}^2 \geq 0$, and thus $A$ is non-negative definite. (b)converse: Because $A$ is symmetric, we know that it can be diagonalized as
	$$A = V\Lambda V^T$$
	because $A$ have non-negative eigenvalues, let $B = V
	\Lambda^{1/2}$ and complete the proof.\\
	(2) similar to (1).
\end{proof}


\begin{remark}[Compare with Cholesky decomposition]
	Cholesky decomposition is usually decompose a positive symmetric matrix into the product of a \textbf{lower triangular matrix} and its conjugate transpose. 
\end{remark}


\begin{corollary}\index{orthogonal projection}
Orthogonal projectors $P$ are nonnegative/semi-positive definite.	
\end{corollary}
\begin{proof}
	$P$ is orthogonal projector and therefore is symmetric and idempotent. That is
	$P^2 = P$ and $P^T=P$, therefore $P=P^TP$ and thus $P$ is nonnegative definite.
\end{proof}
\begin{lemma}
	Let $A$ be a matrix, then $AA^T$ and $A^TA$ has the same non-zero eigenvalues. 
\end{lemma}

\begin{proof}
	Let $\lambda \neq 0$ be an eigenvalue of $A^TA$, i.e.
	$$A^TAx = \lambda x$$
	for some x. then
	$$(AA^T)Ax = A\lambda x = \lambda Ax$$
	that is $Ax$ is the eigenvector of $AA^T$ associated with eigenvalue $\lambda$. Therefore, $\lambda$ is also the eigenvalue of $AA^T$.
\end{proof}



\begin{remark}
	Both $A^TA$ and $AA^T$ are symmetric, but might have different dimensions. 
\end{remark}


\begin{lemma}
	Given a semi-positive definite symmetric matrix $H$, $H+aI$ is positive definite for $a > 0$.
\end{lemma}
\begin{proof}
	for $H$ we can decompose as $H = RR^T$, therefore for any nonzero $x$, $x^T(H+aI)x = xRR^Tx + x^Tx > 0$.
\end{proof}

\subsubsection{Matrix square root}
\begin{theorem}[matrix square root]\index{square root of matrices}\index{matrix square root}\label{ch:linearalgebra:th:matrixSquareRoot}
	Let $A \in \R^{n\times n}$ be a symmetric matrix. If $A$ is positive definite or non-negative definite, then there exists a\textbf{ positive definite or non-negative definite symmetric matrix} $B$ such that $A = B^2$. Moreover, $B$ is uniquely (up to order of eigenvectors) given by
	$$B=U\Lambda^{1/2}U^T$$
	where $U$ and $\Lambda$ are matrices associated with the eigen-decomposition of $A=U\Lambda U^T$.
\end{theorem}
\begin{proof}
	It can be verified that $B^2 = A$. To prove the uniqueness, we have (1) $B$ has to be positive definite, because $rank(A) = rank(BB) = rank(B)$
	$$B_1 = WD_1W^T,B_2=VD_2V^T$$, $B_1^2=B^2$ implies $WD_1^2W^T = VD_2^2V^T,D_1=D_2$
	
	TODO
\end{proof}






\begin{remark}[different versions of square root]
	In engineering applications, there are many definitions of a square root for a matrix. For example, in Cholesky decomposition $A=LL^T$, the triangular matrix $L$ (which is not a symmetric matrix) is usually referred as square root of $A$. See \cite{rhudy2012evaluation} for summary and discussion.
\end{remark}


\begin{corollary}[inverse of matrix square root]
	$$(A^{-1})^{1/2} = (A^{1/2})^{-1}$$
\end{corollary}
\begin{proof}
	We have
	$B = A^{1/2}$, $BB=A$,$A^{-1} = B^{-1}B^{-1}$, and therefore $B^{-1} = (A^{-1})^{1/2}$.
\end{proof}




\subsubsection{Maximization of Quadratic forms}

\begin{theorem}[maximization of Quadratic forms on the unit sphere]\label{ch:linearalgebra:th:MaximizingQuadraticFormsOnUnitSpheres}
Let $B\in \R^{n\times n}$ be a positive semi-definite matrix with eigenvalues $\lambda_1\geq\lambda_2\geq \cdots\geq \lambda_n \geq 0$ and associated unit eigenvectors $e_1,e_2,...,e_n$. Then
\begin{itemize}
	\item $$\max_{x\neq 0} \frac{x^TBx}{x^Tx} = \lambda_1,$$
	where $x^*=e_1$.
	\item $$\min_{x\neq 0, x\perp e_1} \frac{x^TBx}{x^TAx} = \lambda_2,$$
	where $x^*=e_2$.
	\item Moreover, $$\max_{x\neq 0, x\perp e_1,...,e_k} \frac{x^TBx}{x^Tx} = \lambda_{k+1},$$
	where $x^*=e_{k+1}$.
\end{itemize}
\end{theorem}
\begin{proof}
(1) and (2) are results in Rayleigh quotients theorem (\autoref{ch:linearalgebra:th:Raylleighquotient}). (3)	Because $x\perp x_1,...,x_k$; therefore, $x\in span(e_{k+1}, e_{k+2},..., e_n)$. Let
$$x = y_{k+1}e_{k+1} + y_{k+2} e_{k+2} + ... + y_n e_n,$$
we have
$$\frac{x^TBx}{x^Tx} = \frac{\sum_{i=k+1}^n \lambda_i y_i^2}{\sum_{i=k+1}^n y_i^2}.$$
Taking $y_{k+1} = 1, y_{k+2} = ... = y_n = 0$ will give the maximum value of the ratio. 
Then $x = e_{k+1}$.
\end{proof}

\begin{corollary}[maximization of general Quadratic forms on the unit sphere]\label{ch:linearalgebra:th:MaximizingGeneralizedQuadraticFormsOnUnitSpheres}
	Let $B\in \R^{n\times n}$ be a positive semi-definite matrix. Let $A \in \R^{n\times n}$ be a positive definite matrix with decompostion $A = \Sigma^{1/2}\Sigma^{1/2}$, where $\Sigma^{1/2}$ is a positive semi-definite symmetric matrix and the matrix square root of $A$ (\autoref{ch:linearalgebra:th:matrixSquareRoot}).
	
	Let $\Sigma^{-1/2}B\Sigma^{1/2}$ have eigenvalues $\lambda_1\geq\lambda_2\geq \cdots\geq \lambda_n \geq 0$ and associated unit eigenvectors $e_1,e_2,...,e_n$. Then
	\begin{itemize}
		\item $$\max_{x\neq 0} \frac{x^TBx}{x^TAx} = \lambda_1,$$
		where $x^*= \Sigma^{-1/2}u_1, u_1 = e_1$.
		\item $$\min_{x\neq 0, x\perp e_1} \frac{x^TBx}{x^TAx} = \lambda_2,$$
		where $x^*= \Sigma^{-1/2}u_2, u_2 = e_2$.
		\item Moreover, $$\max_{x\neq 0, x\perp e_1,...,e_k} \frac{x^TBx}{x^TAx} = \lambda_{k+1},$$
		where $x^*= \Sigma^{-1/2} u_{k+1}, u_{k+1} = e_{k+1}$.
	\end{itemize}
\end{corollary}
\begin{proof}
 Note that	
\begin{align*}
\frac{x^TBx}{x^TAx} &= \frac{x^TBx}{x^T\Sigma^{1/2}\Sigma^{1/2}x} \\
&= \frac{x^TAx}{x^T\Sigma^{1/2}\Sigma^{1/2}x} \\
&= \frac{u^T\Sigma^{-1/2}B\Sigma^{-1/2}u}{u^Tu} \quad(use~u = \Sigma^{1/2}x) 
\end{align*}
Then we use \autoref{ch:linearalgebra:th:MaximizingQuadraticFormsOnUnitSpheres}.
\end{proof}


\subsubsection{Gramian matrix}

\begin{definition}[Gramian matrix]\index{Gramian matrix}
Let $B$ be a real-valued matrix. The matrix $A = B^TB$ is called a \textbf{Gramian matrix}.	
\end{definition}

\begin{lemma}[properties of Gramian matrix]
Consider a Gramian matrix denoted by $X^TX$. We have
\begin{itemize}
	\item $X^TX$ is symmetric and $(X^TX)^T = X^TX$.
	\item $X^TX$ is of full rank if and only if $X$ is of full column rank.
	\item 
	$$rank(X^TX) = rank(X)$$
	\item $X^TX$ is non-negative definite.
	\item $X^TX$ is positive definite if and only if $X$ is of full column rank.
	\item $$X^TX = \bm{0} \implies X = 0.$$
\end{itemize}	
\end{lemma}
\begin{proof}
(1)
straight forward.
(2)(3) \autoref{ch:linearalgebra:rankPropertyofXTXmatrix}.
(4) for any vector $a$, we have
$$a^TX^TXa = (Xa)^T(Xa) \geq 0.$$
(5) If $X$ is of full column rank, then for any $a\neq 0$, $Xa\neq 0$. Therefore
$$a^TX^TXa = (Xa)^T(Xa) > 0.$$
(6) Let $A = X^TX$. If $A_{ii} = 0$, then
$$A_{ii} = \sum_k X_{ki}^2 = 0 \implies X_{ki} = 0 \forall k.$$
For all $i$, we have $X = \bm{0}$.
\end{proof}



\subsection{Completing the square}

\begin{theorem}[completing the square]\index{completing square}\cite[407]{banerjee2014linear}
	Let $A\in \R^{n\times n}$ be a symmetric positive definite matrix,let $x,b\in \R^n$, then
	$$x^TAx - 2b^Tx + c = (x-A^{-1}b)^TA(x-A^{-1}b) + c -b^TA^{-1}b$$
\end{theorem}
\begin{proof}
	Direct verification.
\end{proof}



\begin{theorem}[completing the square in general cases]\index{completing square}
	Let $A\in \R^{n\times n}$ be a symmetric matrix,let $x,b\in \R^n$. For completing the squares for $x^TAx + bx$, we have the following situations:
	\begin{itemize}
		\item If $A$ is non-singular, then the completing square exists and is given as
		
		\item If $A$ is singular and $b\in \cR(A)$, then the completing square exists. 
		\item If $A$ is singular and $b\notin \cR(A)$, then the completing square does not exists. 
	\end{itemize}
\end{theorem}
\begin{proof}

\end{proof}

\begin{example}[non-existence of completing squares]
	Consider the case 
	$$x_1^2 + x_1 + 2x_2 +3.$$
	We cannot complete the squares. 
\end{example}


\section{Matrix norm and spectral estimation}
\subsection{Basics}
\begin{definition}[spectral radius]\index{spectral radius}\cite[8]{varga2009matrix}
Let $A \in \C^{n\times n}$ with eigenvalues $\lambda_i,i=1,2,...,n$. Then the spectral radius of the matrix $A$ is defined as
$$\rho(A) = \max_{1\leq i\leq n} \abs{\lambda_i}$$
\end{definition}

\begin{definition}[matrix norm]\index{matrix norm}
	Let $A\in \C^{n\times n}$, then the matrix norm induced by the vector norm is 
	$$\norm{A} = \sup_{x\neq 0} \frac{\norm{Ax}}{\norm{x}}$$
\end{definition}

\begin{theorem}[properties of matrix norm]\cite[9]{varga2009matrix}\label{ch:linearalgebra:th:propertiesMatrixNorm}
If $A$ and $B$ are two $n\times n$ complex matrices, then we have
\begin{itemize}
	\item $\norm{A}>0$ unless $A = 0$.
	\item $\norm{aA} = \abs{a}\norm{A}$ for all $a\in \C$.
	\item $\norm{A + B} \leq \norm{A}+ \norm{B} $
	\item $\norm{A\cdot B} \leq \norm{A}\norm{B}$
	\item $\norm{A^k} \leq \norm{A}^k$
	\item $\norm{Ax}\leq \norm{A}\norm{x}$ and there exists a nonzero vector such that the equality holds.
\end{itemize}
\end{theorem}
\begin{proof}
See \autoref{ch:functional-analysis:th:operatornormproperties} for details. For (6), because the set $\norm{x}=1$ is compact, therefore $\norm{A} = \sup_{\norm{x}=1} \norm{Ax}$ can be achieved.
\end{proof}

\begin{theorem}[relation between matrix norm and spectral radius]\label{ch:linearalgebra:th:spectralradiusandmatrixnorm}
	Let $A\in \C^{n\times n}$
	\begin{itemize}
		\item $\norm{A} \geq \rho(A)$
		\item $\norm{A}_2 = \rho(A^HA)^{0.5}$
		\item If $A$ is Hermitian, then $\norm{A}_2 = \rho(A)$
	\end{itemize}
\end{theorem}
\begin{proof}
	(1) $\norm{A} = \sup_{\norm{x}=1}\norm{Ax} \geq \norm{Au} = \abs{\lambda}$
	where $u$ is a unit vector associated with an eigenvalue of $A$. 
	(2) $\norm{A}_2^2 = \sup_{\norm{x}=1}\norm{Ax}^2 = \lambda_{max}(A^HA)$ where the equality will hold when $x$ is the unit eigenvector of $A^HA$ from \autoref{ch:linearalgebra:th:Raylleighquotient}.
	
	(3) use (2), $A^H=A$, and the eigenvalues of $A^2$ is the square of eigenvalues of $A$. 
\end{proof}

\begin{example}
	Consider the example 
	$$A = \begin{bmatrix}
	1 &100\\
	0 & 1
	\end{bmatrix}.$$
	$\rho(A) = 1$, $\norm{A} = \rho(A^TA)^{0.5} \approx 100 $
\end{example}

\begin{theorem}[Existence of a matrix norm that is arbitrarily close to the spectral radius]\cite[347]{horn2012matrix}\label{ch:linearalgebra:th:existencematrixnormclosetospectralradius}
Let $A\in \C^{n\times n}$ and $\epsilon >0$ be given. Then there exists a matrix norm $\norm{\cdot}$ such that 
$$\rho(A) \leq \norm{A} \leq \rho(A) + \epsilon$$ 	
\end{theorem}

\begin{remark}
The norm can not only be common $L^p, 1\leq p \leq \infty$ norm, it can also be weighted norm.
\end{remark}

\begin{theorem}[convergence]\label{ch:linearalgebra:th:convergenceofmatrixpower}
	Let $A\in \C^{n\times n}$. Then
	$$\lim_{n\to \infty} A^n = 0$$
	if and only if $\rho(A) < 1$. 
\end{theorem}
\begin{proof}
	Use Jordan block decomposition. Also see \autoref{ch:linearalgebra:th:boundedmatrixpower}.
\end{proof}





\subsection{Singularity from matrix norm and spectral radius}
\begin{theorem}[singularity from spectral radius]\index{singularity}\label{ch:linearalgebra:singularityfromspectralradius}
Let $G$ be a square matrix such that $\rho(G) < 1$. Then $I-G$ is nonsingular.
\end{theorem}
\begin{proof}
The eigenvalue of $G$ satisfying the polynomial of $det(\lambda I - G) = 0$, and the eigenvalue of $I-G$ satisfying $det((-\lambda' + 1)I - G) = 0$. Therefore, we have
$\lambda' = 1 - \lambda $. Since $\abs{\lambda} < 1$, we must have $\abs{\lambda'} > 0$. Therefore, $I-G$ is nonsingular.
\end{proof}

\begin{remark}[interpretation]
We have expansion of $(I-G)^{-1} = I + G + G^2 + ...$ when $G^k \to 0 ~as~ k\to \infty$. For $G^k \to 0 ~as~ k\to \infty$, the condition is $\rho(G) < 1$(\autoref{ch:linearalgebra:th:convergenceofmatrixpower}).
\end{remark}


\begin{corollary}
Let $G$ be a square matrix such that $\norm{G} < 1$. Then $I-G$ is nonsingular.	
\end{corollary}
\begin{proof}
Use $\rho(G) \leq \norm{G}$(\autoref{ch:linearalgebra:th:spectralradiusandmatrixnorm}). 
\end{proof}



\subsection{Gerschgorin theorem}
\begin{theorem}[Gerschgorin theorem]\cite[498]{meyer2000matrix}\cite[16]{varga2009matrix}\cite[120]{saad2003iterative}\index{Gerschgorin theorem}\label{ch:linearalgebra:th:Gerschgorintheorem}
The eigenvalues of $A\in \C^{n\times n}$ are contained in the union of the $n$ Gerschgorin circles defined by 
$$\abs{z - a_ii} \leq r_i, r_i = \sum_{j=1,j\neq i}^n \abs{a_{ij}}, for ~ i=1,2,...,n$$
Moreover, since $A$ and $A^T$ have the same eigenvalues, then
the eigenvalues of $A\in \C^{n\times n}$ are contained in the union of the $n$ Gerschgorin circles defined by 
$$\abs{z - a_{jj}} \leq r_j, r_j = \sum_{i=1,i\neq j}^n \abs{a_{ij}}, for ~ j=1,2,...,n$$
\end{theorem}
\begin{proof}
	Let $x$ be an eigenvector such that $\norm{x}_\infty = 1$. Assume the $i$th component $x_i$ satisfying $\abs{x_i} = 1$. Then
	$\lambda x = Ax$ and for the ith row we have $\lambda x_i = \sum_{j=1}^n a_{ij}x_j $
	Finally, we have
	$\abs{\lambda - a_{ii}}\abs{x_i} \leq \sum_{j=1,j\neq i}^n \abs{a_{ij}}$.
	Therefore, $\lambda$ is lying within some circle; in otherwise, all $\lambda$ are lying within the union of all circles.
\end{proof}


\begin{corollary}[diagonally dominant matrix property]\label{ch:linearalgebra:th:diagonallydominantmatrixproperty}\hfill
	\begin{itemize}
		\item 	Any strictly diagonally dominant matrix $A$($a_{ii} > \sum_{i=1}^n \abs{a_{ij}}$) is nonsingular.
		\item Any \textbf{symmetric} and strictly diagonally dominant matrix will be positive definite(and nonsingular). 
	\end{itemize}
\end{corollary}
\begin{proof}
	Its eigenvalues are strictly bounded away from and greater 0.
\end{proof}

\begin{corollary}[spectral properties of stochastic matrix]\label{ch:linearalgebra:th:spectralpropertystochasticmatrix}
For any stochastic matrix(matrices where row sum is 1), its eigenvalues $\lambda$ have $\abs{\lambda} \leq 1$.
\end{corollary}
\begin{proof}
	$$-1\leq a_{ii} - r_i \leq \lambda \leq a_{ii} + r_i \leq 1$$
	where $r_i = \sum_{j\neq i}\abs{a_{ij}}$
\end{proof}



\subsection{Irreducible matrix and stronger results}
\begin{definition}[irreducible matrix]\cite[18]{varga2009matrix}\index{irreducible matrix}
For $n\geq 2$, an $n\times n$ complex matrix $A$ is \textbf{reducible} if there exists an $n\times n$ permutation matrix $P$ such that
$$PAP^T = \begin{bmatrix}
A_{1,1} & A_{1,2}\\
0 & A_{2,2}
\end{bmatrix}$$
where $A_{i,j}$ are block matrices. 
If no such permutation matrix exists, then the matrix is called \textbf{irreducible}.
\end{definition}

\begin{remark}[interpretation]
If we view $A$ as the transition matrix of a Markov chain, then $A$ is reducible if there exists absorbing states(once trapped, cannot get out).
\end{remark}

\begin{theorem}[characterizing irreducibility using directed graph]
An $n\times n$ complex matrix $A$ is irreducible if and only if its directed graph $G$ is strongly connected; that is, for any other two ordered pair of two nodes $i,j$, there exists a directed path connecting them. 
\end{theorem}


\begin{theorem}[Gerschgorin Taussky theorem]\cite[20]{varga2009matrix}\index{Gerschgorin Taussky theorem}\label{ch:linearalgebra:th:GerschgorinTausskytheorem}
Let $A$ be an irreducible $n\times n$ complex matrix. If an eigenvalue $\lambda$ is the on the boundary of the union of all the circles $\abs{z-a_{ii}} \leq r_i$, then for all the $n$ circles, $\abs{\lambda - a_{ii}} = r_i,\forall i$.
\end{theorem}
\begin{proof}
	See reference.
\end{proof}


\begin{remark}
If an eigenvalue $A$ is on the boundary of the circle/interval, and if $A$ is irreducible, then the eigenvalue is on the boundary of all the intervals.	
\end{remark}



\begin{corollary}\cite[197]{holmes2007introduction}
A matrix $A$ is positive definite if the following \textbf{all} holds:
\begin{itemize}
	\item $a_{ii} \geq \sum_{j=1,j\neq i}^n \abs{a_{ij}}, \forall i$
	\item $0 < a_{ii},\forall i$
	\item There is at least one row where $a_{ii} > \sum_{j=1,j\neq i}^n \abs{a_{ij}}$
	\item $A$ is irreducible.
\end{itemize}
\end{corollary}
\begin{proof}
	(1)(2) make sure that all eigenvalues are at least non-negative. (4) makes sure that all eigenvalues must be bounded away from 0.
\end{proof}





\section{Pseudoinverse of matrix}

\subsection{Pseudoinverse for full rank system}
\begin{definition}[pseudoinverse for full rank system]\index{pseudoinverse}\label{ch:linearalgebra:def:psedudoinverseOfFullRankSystem}
Let $A\in \R^{m\times n}$. 
\begin{itemize}
	\item If $A$ has full column rank, then we define its pseudoinverse as
	$$A^+ = (A^TA)^{-1}A^T$$
	such that $A^+\in \R^{n\times m}, A^+A = I_n$.
	\item If $A$ has full row rank, then we define its pseudoinverse as
	$$A^+ = A^T(AA^T)^{-1}$$
	such that $A^+\in \R^{n\times m}, AA^+ = I_m$.
\end{itemize}	
\end{definition}

\begin{lemma}[basic properties of pseudoinverse]\label{ch:linearalgebra:th:BasicPropertyPseudoInverse}
Let $A\in \R^{m\times n}$ with either $rank(A) = m$ or $rank(A) = n$. It follows that
\begin{itemize}
	\item If $m=n$, then $A^+ = A^{-1}$.
	\item If $A$ has full column rank, then $A^+$ has full row rank; If $A$ has full row rank, then $A^+$ has full column rank;
	\item $(A^+)^+ = A.$
	\item Let $A$ has full column rank such that $A^T$ has full row rank, then
	$$(A^T)^+ = (A^+)^T.$$
	\item For matrix $A$ with either full column rank or full row rank, we have
	$$A^+AA^+ = A^+, AA^+A = A.$$
	\item 	$A^+A$ and $AA^+$ are symmetric.
	 
\end{itemize}	
\end{lemma}
\begin{proof}
(1) If $A$ has full column rank, then
$$(A^TA)^{-1}A^T = A^{-1}A^{-T}A^T = A^{-1}.$$
 If $A$ has full row rank, then
 $$A^T(AA^T)^{-1} = A^{T}A^{-T}A^{-1} = A^{-1}.$$
(2)	Let $A$ has full column rank, then $$rank(A^+) = rank((A^TA)^{-1}A^T) = rank((A^TA)^{-1})=rank(A^TA) = n, $$
where we use results in ranks of matrix products 
\autoref{ch:linearalgebra:th:rankOfMatrixProducts} and \autoref{ch:linearalgebra:ranklemmatwo}.

We can similarly prove the other case.
(3)
Let $A$ have full column rank, then $A^+ = (A^TA)^{-1}A^T$ has full row rank. Then
\begin{align*}
(A^+)^+ &= [(A^TA)^{-1}A^T]^T((A^TA)^{-1}A^T [(A^TA)^{-1}A^T)]^T)^{-1}\\
&=A(A^TA)^{-T} (A^TA)^T \\
&=A
\end{align*}
We can similarly prove the other case.
(4) straight forward.
(5) Let $A^+ =  A^T(AA^T)^{-1}$, we have
$$(A^+A)^T =  (A^T(AA^T)^{-1} A)^T = A^T(AA^T)^{-1} A = A^+A.$$
We can similarly prove the other case.
\end{proof}


\begin{lemma}[projector properties from pseudoinverse]\label{ch:linearalgebra:th:ProjectorPropertyFromPseudoInverse}\hfill
\begin{itemize}
	\item Let $A$ have full column rank, then $P = AA^+ = A(A^TA)^{-1}A^T$ has the following properties
	\begin{itemize}
		\item $P$ is an orthogonal projector such that $P^T = P, PP = P.$
		\item $P$ is the orthogonal projector into $\cR(A)$; or equivalently, $$PA = A, P^TN_{A^T} = 0,$$
		where $N_{A^T}$ is the basis matrix of $\cN(A^T)$.
	\end{itemize}
	\item Let $A$ have full row rank, then $Q = A^+A = A^T(AA^T)^{-1}A$ has the following properties
\begin{itemize}
	\item $Q$ is an orthogonal projector such that $Q^T = Q, QQ = Q.$
	\item $Q$ is the orthogonal projector into $\cR(A^T)$; or equivalently, $$QA^T = A^T, Q^TN_{A} = 0,$$
	where $N_{A}$ is the basis matrix of $\cN(A)$.
\end{itemize}
\end{itemize}	
\end{lemma}
\begin{proof}
(1) From \autoref{ch:linearalgebra:th:BasicPropertyPseudoInverse}, $P$ is symmetric and 
$$PP = AA^+AA^+ =(AA^+A)A^+= AA^+ = P.$$
(2) $PA = A(A^TA)^{-1}A^TA = A$. Let $y \in \cN(A^T)$ such that $A^Ty = 0$. Then
$$P^Ty = Py = A(A^TA)^{-1}A^Ty = 0.$$

(3)(4) Similar to (1)(2).
\end{proof}

\begin{lemma}[pseudoinverse for special matrices]\hfill
\begin{itemize}
	\item Let $A$ have full column rank and columns are orthonormal $A^TA = I$. Then
	$$A^+ = A^T.$$
	\item Let $A$ have full row rank and rows are orthonormal $AA^T = I$. Then
	$$A^+ = A^T.$$
	\item Let diagonal matrix $D\in \R^{m\times n}, m \geq n$ with nonzero diagonal elements $d_1,d_2,...,d_n$, then
			 $D^+ \in \R^{n\times m}$ is diagonal with diagonal elements $1/d_1,1/d_2,...,1/d_n$.
	\item Let diagonal matrix $D\in \R^{m\times n}, m \leq n$ with nonzero diagonal elements $d_1,d_2,...,d_m$, then
	$D^+ \in \R^{n\times m}$ is diagonal with diagonal elements $1/d_1,1/d_2,...,1/d_m$.		 
\end{itemize}	
\end{lemma}
\begin{proof}
(1)$A^+ = (A^TA)^{-1}A^T = A^T.$ (2)(3)(4) straight forward.
	
\end{proof}


\begin{theorem}[SVD and pseudoinverse]\label{ch:linearalgebra:th:SVDAndPseudoinverse}
Let $A \in \R^{m\times n}$ (full column rank and full row rank)have the SVD(\autoref{ch:linearalgebra:th:SVD}) given by $A = U\Lambda V^T$, then
$$A^+ = V\Lambda^+ U^T.$$	
\end{theorem}
\begin{proof}
Note that
\begin{align*}
A^+ &= (A^TA)^{-1}A^T = (V\Lambda^2 V^T)^{-1}   V\Lambda^T U\\
&= (A^TA)^{-1}A^T = (V\Lambda \Lambda^T V^T)^{-1}V\Lambda^T U^T \\
&= (V(\Lambda \Lambda^T)^{-1} V^T)V\Lambda^T U^T \\
&= V\lambda^+ U^T.
\end{align*}
where we use the that $V\Lambda \Lambda^T V^T$ can be viewed as an eigen-decomposition and its inverse is given by $(V(\Lambda \Lambda^T)^{-1} V^T)$
\end{proof}



\subsection{Pseudoinverse for general matrix}
\begin{definition}[pseudoinverse for general]\index{pseudoinverse}
	Let $A\in \R^{m\times n}$ and its SVD given by $A = U\Lambda V^T$, then
we define the pseudoinverse of $A$ by
$$A^+ = V\Lambda^+ U^T.$$
where we define $\Lambda^+$ as the transpose of $\Lambda$ and the diagonal elements in $\Lambda^+$ is the inverse of the diagonal elements in $\Lambda$  such that $\Lambda^+\Lambda = I_r\otimes 0_{n-r} , \Lambda\Lambda^+ = I_m\otimes 0_{n-r},$
where 
$$I_r\otimes 0_{n-r} = \triangleq\begin{bmatrix}
1 &  &  &  &  &  & \\ 
& 1 &  &  &  &  & \\ 
&  & \ddots &  &  &  & \\ 
&  &  & 1 &  &  & \\ 
&  &  &  & 0 &  & \\ 
&  &  &  &  & \ddots & \\ 
&  &  &  &  &  & 0
\end{bmatrix}.$$
where there are $r$ elements of 1 in the diagonal. 	
\end{definition}

\begin{note}[existence and uniqueness]
Because a unique SVD always exists for any matrix, a unique pseudoinverse always exists for any matrix. 	
\end{note}


\begin{lemma}[basic properties of pseudoinverse of general matrix]\label{ch:linearalgebra:th:BasicPropertyPseudoInverseGeneralMatrix}
Let $A\in \R^{m\times n}$ with rank $r\leq \min(m,n)$. Let its SVD be $A = U\Lambda V^T$. It follows that
\begin{itemize}
	\item $A^+$ has rank $r$.
	\item $(A^+)^+ = A.$
	\item 
	$$(A^T)^+ = (A^+)^T.$$
	\item 
	$$A^+AA^+ = A^+, AA^+A = A.$$
	\item 	$A^+A$ and $AA^+$ are symmetric.
	
\end{itemize}	
\end{lemma}
\begin{proof}
(1) Note that $V\Lambda^+ U^T$ is still SVD form, and it has $r$ non-zero elements in $\Lambda^+$. (2) $$(A^+)^+ = (V\Lambda^+ U^T)^+ = U(\Lambda^+)^+V^T = U\Lambda V^T = A.$$
(3) $$(A^T)^+ = (V\Lambda^T U^T)^+ = (V\Lambda^T U^T)^+ = U(\Lambda^T)^+V^T$$
and 
$$(A^+)^T = = (V\Lambda^+ U^T)^T = U(\Lambda^+)^TV^T.$$
Further note that $(\Lambda^+)^T = (\Lambda^T)^+$.
(4)
$$AA^+A = U\Lambda V^T V\Lambda^+ U^T U \Lambda V^T =U\Lambda(I_r\otimes 0_{m-r}) V^T  =U\Lambda V^T = A.$$
similarly for the other.
(5)
Note that $A^+A =  V\Lambda^+ U^T U \Lambda V^T = V(I_r\otimes 0_{n-r})V^T,$
a symmetric matrix.
\end{proof}


\begin{lemma}[projector properties from pseudoinverse]\label{ch:linearalgebra:th:ProjectorPropertyFromPseudoInverseGeneralSystem}\hfill
Let $A\in \R^{m\times n}$ and its SVD given by $A = U\Lambda V^T$	

		Then $P = AA^+ = U(I_r\otimes 0_{m-r})U^T = U_rU^T_r$ has the following properties:
		\begin{itemize}
			\item $P$ is an orthogonal projector such that $P^T = P, PP = P.$
			\item $P$ is the orthogonal projector into $\cR(A) = \cR(U_r)$; or equivalently, $$PA  = A, PU = U_r.$$
		\end{itemize}
\end{lemma}
\begin{proof}
	(1) 
	Note that $AA^+ =  U\Lambda V^T V \Lambda^+ U^T = U(I_r\otimes 0_{m-r})U^T,$
	a symmetric matrix.
	Also from \autoref{ch:linearalgebra:th:BasicPropertyPseudoInverseGeneralMatrix}, $P$ is symmetric and 
	$$PP = AA^+AA^+ =(AA^+A)A^+= AA^+ = P.$$
	(2) $$PA = U(I_r\otimes 0_{m-r})U^T U\Lambda V^T = U(I_r\otimes 0_{m-r})\Lambda V^T =U\Lambda V^T = A$$
	$$PU = U(I_r\otimes 0_{m-r})U^T U = U_r$$
\end{proof}

\subsection{Application in linear systems}

\begin{lemma}[solution for full rank system]
Let $A \in \R^{m\times n}$ have either full column rank or full row rank. If the linear system $Ax = b$ has solution, then the solution is given by
$$x = A^+b + (I_n - A^+A)z, z\in \R^n,$$
where $I_n - A^+A$ being the $\cN(A)$ basis matrix.
Among all solutions, the minimum norm/length solution is $A^+b$.

If $Ax = b$ does not have a solution, then
$$x = A^+b + (I_n - A^+A)z, z\in \R^n.$$
is the solution set of minimum error, with $A^+b$ being the minimum nomr/length solution.
\end{lemma}
\begin{proof}
See SVD approach to linear system \autoref{ch:linearalgebra:linearEquationSolutionSVDmethod} and the relationship between SVD and pseudoinverse(\autoref{ch:linearalgebra:th:ProjectorPropertyFromPseudoInverse}). Note that when $A$ has full column rank $A^+A =I_n$. 
To show  $I_n - A^+A$ is the null space basis matrix, we have
$$A(I_n - A^+A) = A - AA^+A = A - A = 0.$$

To show $A^+b$ is of the minimum length, we have
\begin{align*}
&\norm{A^+b + (I_n - A^+A)z}^2\\
=& \norm{A^+b}^2+\norm{(I_n - A^+A)z}^2+2z(I_n - A^+A)^T(A^+b)\\
=& \norm{A^+b}^2+\norm{(I_n - A^+A)z}^2+2z(A^+ - A^+AA^+)b\\
=& \norm{A^+b}^2+\norm{(I_n - A^+A)z}^2+2z(A^+ - A^+)b\\
=& \norm{A^+b}^2+\norm{(I_n - A^+A)z}^2
 \geq \norm{A^+b}^2
\end{align*}
where we use the basic property $A^+AA^+=A^+. $
\end{proof}


\begin{remark}[interpretation]\hfill
\begin{itemize}
	\item If $A$ has full column rank, then $A^+ = (A^TA)^{-1}A^T$, $AA^+b$ is the orthogonal projection(\autoref{ch:linearalgebra:th:ProjectorPropertyFromPseudoInverse}) of $b$ into $\cR(A)$. Also, $A^+A =I_n$ implies the null space is 0 dimensional.
	\item If $A$ is full row rank, then $A^+ = A^T(AA^T)^{-1}$, $AA^+b = I_mb = b$ is the solution. 
	
	 and also the orthogonal projection(\autoref{ch:linearalgebra:th:ProjectorPropertyFromPseudoInverse}) of $b$ into $\cR(A)$. Also, $A^+A =I_n$ implies the null space is 0 dimensional.	
\end{itemize}	
	
\end{remark}



\begin{theorem}[solution for general linear system]\label{ch:linearalgebra:th:solutionForGeneralLinearSystemPseudoinverseMethod}
	Let $A \in \R^{m\times n}$ with SVD $A = U\Lambda V^T$ and $rank(A) = r$. Let $A^+  = V \Lambda^+ U^T$ be its pseudoinverse. If the linear system $Ax = b$ has solution, then the solution is given by
	$$x = A^+b + (I_n - A^+A)z, z\in \R^n.$$
	where $I_n - A^+A$ being the $\cN(A)$ basis matrix.
	Among all solutions, the minimum norm/length solution is $A^+b$.
	
	If $Ax = b$ does not have a solution, then
	$$x = A^+b + (I_n - A^+A)z, z\in \R^n.$$
	is the solution set of minimum error, with $A^+b$ being the minimum norm/length solution.
\end{theorem}
\begin{proof}
	See SVD approach to linear system \autoref{ch:linearalgebra:linearEquationSolutionSVDmethod} and the relationship between SVD and pseudoinverse(\autoref{ch:linearalgebra:th:ProjectorPropertyFromPseudoInverseGeneralSystem}) and above proof.
	Note that $AA^+$ is orthogonal projector into $\cR(A)$. To show  $I_n - A^+A$ is the null space basis matrix, we have
	$$A(I_n - A^+A) = A - AA^+A = A - A = 0.$$
\end{proof}


\section{Multilinear form and Determinants}
\subsection{Bilinear forms}
\begin{definition}[bilinear form]\index{bilinear form}
	Let $V$ be a vector space over the filed $\F$. The map $$\phi: V\times V  \to \F$$
	is called \textbf{bilinear form} on $V$ if for any $u,v,w\in V$ and any scalar $\lambda\in \F$ we have
	\begin{itemize}
		\item $$\phi(u+v,w) = \phi(u,w) + \phi(v,w), \phi(\lambda v,w) = \lambda \phi(v,w).$$
		\item $$\phi(u,v+w) = \phi(u,w) + \phi(u,v), \phi(v,\lambda w) = \lambda \phi(v,w).$$
	\end{itemize}	
\end{definition}


\begin{lemma}[representation of bilinear form]
	For any bilinear form $\phi$ defined on $\R^n$, there exists a matrix $A\in\R^{n\times n}$ such that $\phi$ can be represented by 
	$$\phi(x,y) = x^TAy, \forall x,y\in \R^n.$$	
\end{lemma}
\begin{proof}
	Let $A_{ij} = \phi(e_i,e_j)$. Then any $x = \sum_{i=1}^n x_ie_i, y=\sum_{j=1}^n y_ie_i,$ we have
	$$\phi(x,y) = \sum_{i=1}^n\sum_{j=1}^n x_iy_j \phi(e_i,e_j) = \sum_{i=1}^n\sum_{j=1}^n x_iy_iA_{ij} = x^TAy.$$
\end{proof}




\begin{definition}[symmetric, skew symmetric, and alternating bilinear forms]
	Let $U$ be a $F$-vector space.
	\begin{itemize}
		\item A bilinear form $\phi$ is called \textbf{symmetric} if for any $u_1,u_2\in U$ 
		$$\phi(u_1,u_2) = \phi(u_2,u_1).$$
		\item A bilinear form $\phi$ is called \textbf{skew-symmetric} if for any $u_1,u_2\in U$ï¼Œ
		$$\phi(u_1,u_2) = -\phi(u_2,u_1).$$
		\item A bilinear form $\phi$ is called \textbf{alternating} if for any $u \in U$ we have
		$$\phi(u,u) = 0.$$	
	\end{itemize}	
\end{definition}




\subsection{Multilinear forms}\index{multilinear form}
\begin{definition}[$k$-linear form]
Let $V$ be a vector space over the filed $\F$. The map $$\phi: \underbrace{V\times V \cdots V}_{k}  \to \F$$
is called \textbf{$k$-linear form} on $V$ if for any $u_i,v_i\in V,i=1,2,...,k$ and any scalar $\lambda\in \F$ we have
\begin{itemize}
	\item $$\phi(u_1,...,u_i+v_i,...+u_k) = \phi(u_1,...,u_i,...,u_k) + \phi(u_1,...,v_i,...,u_k).$$
	\item $$\phi(u_1,...,\lambda u_i,...+u_k) = \lambda \phi(u_1,...,u_i,...,u_k) + \phi(u_1,...,v_i,...,u_k).$$
\end{itemize}	
\end{definition}

\begin{example}
Let $A\in \R^{n\times n}$. Define $\phi:\R^n\times \R^n\to \R$ such that
$$\phi(x,y) = x^TAy, \forall x,y\in \R^n.$$

We can see that
\begin{itemize}
	\item $$\phi(x+z,y) = (x+z)^TAy = x^TAy + z^TAy = \phi(x,y) + \phi(z,y)$$.
	\item $$\phi(\lambda x, y) = (\lambda x)^TAy = \lambda x^TAy = \lambda \phi(x,y).$$
\end{itemize}	

Therefore, $\phi$ is a bi-linear form on $\R^n$.
\end{example}


\begin{definition}[symmetric, skew symmetric, and alternating]
Let $U$ be a $F$-vector space.
\begin{itemize}
	\item A $k$-linear form $\phi$ is called \textbf{symmetric} if for any $u_1,u_2,...,u_k\in U$ and any permutation $\sigma\in S_k$ we have
	$$\phi(u_{\sigma(1)},u_{\sigma(2)},...,u_{\sigma(k)}) = \phi(u_{1},u_{2},...,u_{k}).$$
	\item A $k$-linear form $\phi$ is called \textbf{skew-symmetric} if for any $u_1,u_2,...,u_k\in U$ and any permutation $\sigma\in S_k$ we have
	$$\phi(u_{\sigma(1)},u_{\sigma(2)},...,u_{\sigma(k)}) = sign(\sigma)\phi(u_{1},u_{2},...,u_{k}).$$
	or equivalently(swap will change sign), 
	$$\phi(u_1,...,u_i,...,u_j,...,u_k) = -\phi(u_1,...,u_j,...,u_i,...,u_k).$$
	\item A $k$-linear form $\phi$ is called \textbf{alternating} if for any $u_1,u_2,...,u_k\in U$ we have
	$$\phi(u_{1},u_{2},...,u_{k}) = 0,$$
whenever $u_i=u_j,i\neq j.$	
\end{itemize}	
\end{definition}



\begin{remark}[simplified condition for checking skew-symmetric]
Note that the sufficient condition for checking a $k$-linear form is to examine all permutations $\sigma$. A simplified condition is to only check whether a simple swap will change sign.

To show that these two conditions are equivalently, we have
\begin{itemize}
	\item 
	\begin{align*}
	\phi(u_{\sigma(1)},u_{\sigma(2)},...,u_{\sigma(k)}) &= sign(\sigma)\phi(u_{1},u_{2},...,u_{k}) \\
	\implies \phi(u_1,...,u_i,...,u_j,...,u_k) &= -\phi(u_1,...,u_j,...,u_i,...,u_k)
	\end{align*}
	since a simple swap has sign -1.
	\item We note that any permutation can be decomposed as compositions of simple swap. And the sign of the permutation equals the number of simple swaps. 
	Define
	$$\sigma\circ \phi(u_1,u_2,...,u_k) = \phi(u_{\sigma(1)},u_{\sigma(2)},...,u_{\sigma(k)}),$$
	and suppose we have decomposition $\sigma = \sigma_1\circ \sigma_2 \cdots \sigma_m$, where $\sigma_i$s are simple swaps.
	Then
	\begin{align*}
	\phi(u_{\sigma(1)},...,u_{\sigma(k)}) &= \sigma_1\circ \sigma_2 \cdots \circ\sigma_m \phi(u_1,...,u_k)\\
	& = (-1)^m \phi(u_1,...,u_k) \\
	&= sign(\sigma) \phi(u_1,...,u_k)
	\end{align*}
\end{itemize}
\end{remark}

\begin{lemma}[skew symmetric and alternating are equivalent]\label{ch:linearalgebra:th:MultilinearFormSkewSymmetricisEquivalenttoAlternating} 
Let $U$ be a $F$-vector space. Let  $\phi$ be a $k$-linear form. Then $\phi$ is alternating if and only if $\phi$ is skew-symmetric.	
\end{lemma}
\begin{proof}
(1)(alternating implies skew-symmetric)
For all $x,y\in U$, we have 
\begin{align*}
0=\phi(...,x+y,...,x+y,...) &= \phi(...,x,...,x,...)+\phi(...,y,...,y,...)\\
&+\phi(...,y,...,x,...)+\phi(...,x,...,y,...) \\
&=0 + 0 + \phi(...,y,...,x,...)+\phi(...,x,...,y,...) \\
&=\phi(...,y,...,x,...)+\phi(...,x,...,y,...) \\
\implies \phi(...,y,...,x,...)&=-\phi(...,x,...,y,...)
\end{align*}
(2)(skew-symmetric implies alternating)
For any $x\in U$, the skew-symmetry properties implies that
$$\phi(...,x...,x,...) = -\phi(...,x...,x,...);$$
rearrange and we will get
$$2\phi(...,x...,x,...) = 0.$$
\end{proof}

\begin{example}
Consider the following bilinear forms on $\R^4$. Let $x,y\in \R^4$ 
\begin{itemize}
	\item $f(x,y) = x_1y_2 - x_2y_1 + x_1y_1$ is not alternating since
	$$f(x,x) = x_1x_2 - x_2x_2 + x_1^2 = x_1^2 \geq 0.$$
That is $f(x,x)$ does not equal 0 for all $x\in \R^4$.
\item $g(x,y) = x_1y_3 - x_3y1$ is alternating since
$$g(x,x) = x_1x_3 - x_3x_1 = 0 \forall x\in \R^4.$$	
\end{itemize}	
\end{example}

\subsection{Determinant}


\begin{definition}[determinant]\cite[279]{banerjee2014linear}
	The \textbf{determinant} of an $n\times n$ matrix $A = a_{ij}$ is defined by
	$$det(A) = \sum_{\sigma \in S_n} sign(\sigma)a_{1\sigma(1)}\cdots a_{n\sigma(n)} = \sum_{\sigma \in S_n} sign(\sigma)a_{\sigma(1)1}\cdots a_{\sigma(n)n},$$
where we are summing up all $n!$ permutation in the symmetric group $S_n$.	
\end{definition}



\begin{example}
Consider a $2\times 2$ matrices. Let 
$$A = \begin{pmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22} \\
\end{pmatrix},$$
then 
$$\abs{A} = \sigma(1,2)a_{11}a_{22} + \sigma(2,1)a_{12}a_{21} = a_{11}a_{22}-a_{12}a_{21}.$$	
\end{example}

\begin{example}
	Consider a $3\times 3$ matrices. Let 
	$$A = \begin{pmatrix}
	a_{11} & a_{12} & a_{13}\\
	a_{21} & a_{22} & a_{23}\\
	a_{31} & a_{32} & a_{33}
	\end{pmatrix},$$
	then 
	\begin{align*}
	\abs{A} &= \sigma(1,2,3)a_{11}a_{22}a_{33} + \sigma(1,3,2)a_{11}a_{23}a_{32}+\sigma(2,1,3)a_{12}a_{21}a_{33}\\
	&+ \sigma(2,3,1)a_{12}a_{21}a_{33}+\sigma(3,1,2)a_{13}a_{21}a_{32}+\sigma(3,2,1)a_{13}a_{22}a_{31}\\
	&=a_{11}a_{22}a_{33} + -a_{11}a_{23}a_{32}+-a_{12}a_{21}a_{33}\\
	&+ a_{12}a_{21}a_{33}+a_{13}a_{21}a_{32}+-a_{13}a_{22}a_{31}
	\end{align*}	
\end{example}

\begin{theorem}[the equivalence of $det A$ and $det A^T$.]
	The \textbf{determinant} of an $n\times n$ matrix $A = a_{ij}$ is defined by
	
	where we are summing up all $n!$ permutation in the symmetric group $S_n$.
	\begin{itemize}
		\item $$det(A) = \sum_{\sigma \in S_n} sign(\sigma)a_{1\sigma(1)}\cdots a_{n\sigma(n)} = \sum_{\sigma \in S_n} sign(\sigma)a_{\sigma(1)1}\cdots a_{\sigma(n)n},$$
		\item $$det(A) = \sum_{\sigma \in S_n} sign(\sigma)a_{1\sigma(1)}\cdots a_{n\sigma(n)} = \sum_{\sigma \in S_n} sign(\sigma)a_{\sigma(1)1}\cdots a_{\sigma(n)n},$$
	\end{itemize}	
\end{theorem}

\begin{lemma}[determinant and multilinear forms]\hfill\label{ch:linearalgebra:th:DeterminantAndMultilinearForm}
\begin{itemize}
	\item For any matrix $A\in \R^{n\times n}, A = [a_1,a_2,...,a_n]$,the determinant of $A$ given by $$det(A) \triangleq det(a_1,a_2,...,a_n)$$
	is the $n$-linear form mapping from $\R^{n\times n}$ to $\R$.
	\item $det(A)$ is both alternating and skew-symmetric; Specifically, 
	\begin{itemize}
		\item (skew-symmetric) For any $a_1,a_2,...,a_k\in \R^n$ we have
		$$det(u_{\sigma(1)},u_{\sigma(2)},...,u_{\sigma(n)}) = sign(\sigma)\phi(u_{1},u_{2},...,u_{k}).$$
		\item (alternating) For any $a_1,a_2,...,a_k\in \R^n$ we have
		$$det(u_{1},u_{2},...,u_{n}) = 0,$$
		whenever $u_i=u_j,i\neq j.$	
	\end{itemize}
\end{itemize}	
\end{lemma}
\begin{proof}
(1)	
We can show that
(a)
\begin{align*}
det(a_1,...,a_i+b_i,...,a_n) &= \sum_{\sigma \in S_n} sign(\sigma)a_{1\sigma(1)}\cdots (a_{1\sigma(i)}+b_{1\sigma(i)})a_{n\sigma(n)} \\
&= \sum_{\sigma \in S_n} sign(\sigma)a_{1\sigma(1)}\cdots (a_{1\sigma(i)})a_{n\sigma(n)} + \sum_{\sigma \in S_n} sign(\sigma)a_{1\sigma(1)}\cdots (b_{1\sigma(i)})a_{n\sigma(n)} \\
&= det(a_1,...,a_i,...,a_n)+det(a_1,...,b_i,...,a_n)
\end{align*}
(b)\begin{align*}
det(a_1,...,\lambda a_i,...,a_n) &= \sum_{\sigma \in S_n} sign(\sigma)a_{1\sigma(1)}\cdots (\lambda a_{1\sigma(i)})a_{n\sigma(n)} \\
&= \lambda \sum_{\sigma \in S_n} sign(\sigma)a_{1\sigma(1)}\cdots (a_{1\sigma(i)})a_{n\sigma(n)}  \\
&= \lambda det(a_1,...,a_i,...,a_n)
\end{align*}
(2) We only need to show skew-symmetric since skew-symmetric and alternating are equivalent(\autoref{ch:linearalgebra:th:MultilinearFormSkewSymmetricisEquivalenttoAlternating}).
\end{proof}



\begin{theorem}[determinants for matrices after column(row) operation]\cite[282]{banerjee2014linear}
	Let $B$ be the matrix obtained from an $n\times n$ matrix $A=[a_1,a_2,...,a_n]$ by applying one of the three elementary column(row) operations:
	\begin{itemize}
		\item (type I) interchange two columns(rows) of $A$, then
		$$det{B} = -det{A}.$$
		\item (type II): multiply a column(row) of $A$ by a scalar $\alpha$, then
		$$det{B} = \alpha det{A}.$$
		\item (type III): add $\alpha$ times a given column(row) of $A$ to another column(row), then
		$$det{B} = det{A}.$$
	\end{itemize}
\end{theorem}
\begin{proof}
(1)(2) Since $det$ is skew-symmetric (\autoref{ch:linearalgebra:th:DeterminantAndMultilinearForm}),
$$det(B) = det(a_1,...,a_j,...,a_i,...a_n) = -det(a_1,...,a_i,...,a_j,...a_n) = det(A).$$
and
$$det(B) = det(a_1,...,\alpha a_i...a_n) = \alpha det(a_1,...,a_i,...a_n) = \alpha det(A).$$
(3)$$det(B) = det(a_1,...,a_j,...,a_i,...a_n) = -det(a_1,...,a_i,...,a_j,...a_n) = det(A).$$
and
$$det(B) = det(a_1,...,a_i + \alpha a_j,,...a_n) = det(a_1,...,a_i,...,a_n)+det(a_1,...,\alpha a_j,...,a_j,...a_n) = det(A) + 0 = det(A).$$
\end{proof}


\begin{lemma}[determinant of triangular matrix]\hfill\label{ch:linearalgebra:th:DeterminantOfTriangularMatrix}
\begin{itemize}
	\item If $A\in \R^{n\times n}$ is an upper(lower) triangular matrix, then $det(A)$ is the product of the diagonal entries.
	\item For the identity matrix $I_{m}$, $det(I_m) = 1$.
\end{itemize}	
\end{lemma}
\begin{proof}
From the definition
	$$det(A) = \sum_{\sigma \in S_n} sign(\sigma)a_{1\sigma(1)}\cdots a_{n\sigma(n)},$$
we know that the only nonzero terms in the summation is the permutation such that
$$\sigma(i) = i, i=1,2....,n.$$
Then
	$$det(A) = a_{11}\cdots a_{nn}.$$
\end{proof}



\begin{lemma}[determinant and invertibility of a matrix]\hfill
\begin{itemize}
	\item Consider a matrix $A\in \R^{n\times n}$. $A$ is invertible if and only if $det(A) \neq 0$.
	\item Consider a upper(or lower) triangular matrix $A\in \R^{n\times n}$. $A$ is invertible if and only if \textbf{all diagonal entries} in $A$ are nonzero.
\end{itemize}	
\end{lemma}
\begin{proof}
(1)A matrix $A$ is invertible if and only if by performing elementary row operations we can reduce to an upper triangular matrix $B$ whose diagonal entries are nonzero,i.e., $det B \neq 0$.
(2)Note that for a triangular matrix, its determinant is the product of its diagonal entries(\autoref{ch:linearalgebra:th:DeterminantOfTriangularMatrix}). 
\end{proof}

\begin{lemma}[determinant of matrix product]\hfill
\begin{itemize}
	\item If $A,B$ are $n\times n$ matrices, then
	$$det(AB) = det(A)det(B).$$	
	\item If $A$ is invertible, then
	$$det A^{-1} = \frac{1}{det A}.$$
\end{itemize}	
\end{lemma}
\begin{proof}
(1)	
Note that for the product $(AB)_{ij} = \sum_{k=1}^n A_{ik}B_{kj}.$
The column $j$ of $AB$ is given by
$$\sum_{k=1}^n a_kB_{kj}.$$
Therefore,
\begin{align*}
det(AB) &= det(\sum_{i_i=1}^n a_{i_1}B_{i_11},...,\sum_{i_n=1}^n a_{i_n}B_{i_nn}) \\
&=\sum_{i_1}^n...\sum_{i_n=1}^n B_{i_11}...B_{i_nn} det(a_1,a_2,...,a_n) \\
&=\sum_{i_1}^n...\sum_{i_n=1}^n B_{i_11}...B_{i_nn} det(a_{i_1},a_{i_2},...,a_{i_2}) \\
\end{align*}
Because of the alternating properties of determinant, the only non-zero terms in the above summation correspond to choices of pairwise distinct indices $i_1,...,i_n$. For such a choice, the sequence $i_1,...,i_n$ describes a permutation from $S_n$. We then have
\begin{align*}
det(AB) &= \sum_{i_1}^n...\sum_{i_n=1}^n B_{i_11}...B_{i_nn} det(a_{i_1},a_{i_2},...,a_{i_2}) \\
&=\sum_{\sigma\in S_n} B_{\sigma(1)1}...B_{\sigma(n)n} det(a_{\sigma(1)},...,a_{\sigma(n)}) \\
&=\sum_{\sigma\in S_n} B_{\sigma(1)1}...B_{\sigma(n)n} sign(\sigma)det(a_1,...,a_n) \\
&= det(B)det(A),
\end{align*}
where we use the skew-symmetry property of determinant(\autoref{ch:linearalgebra:th:DeterminantAndMultilinearForm}) such that
$$det(a_{\sigma(1)},...,a_{\sigma(n)}) =sign(\sigma)det(a_1,...,a_n). $$
(2) 
If 
$$det AA^{-1} = det A det A^{-1} = 1 \implies det A^{-1} = \frac{1}{det A}.$$
\end{proof}


\begin{lemma}[determinant of block matrix]\hfill
	\begin{itemize}
		\item Let $$M = \begin{bmatrix}
		A& 0\\
		0& I_m
		\end{bmatrix}, A\in\R^{n\times n}, .$$
		Then
		$$det(M) = det(A)$$
		\item Let $$M = \begin{bmatrix}
		A& 0\\
		C& D
		\end{bmatrix}.$$
		Then
		$$det(M) = det(A)det(D). $$
		\item Let $$M = \begin{bmatrix}
		A& B\\
		C& D
		\end{bmatrix}.$$
		Then
		$$det(M) = det(A)det(D - CA^{-1}B). $$
	\end{itemize}	
\end{lemma}
\begin{proof}
(1)	From the definition of determinant, we have
$$det(M) = \sum_{\sigma \in S_{m+n}} sign(\sigma)M_{1\sigma(1)}\cdots M_{n+m\sigma(n+m)}.$$
The non-zero terms in the above summation correspond to the choices where $\sigma(i) = i,i=n+1,...,n+m$. Then we can simplify
$$det(M) = \sum_{\sigma \in S_n} sign(\sigma)a_{1\sigma(1)}\cdots a_{n\sigma(n)} = det(A).$$
	(2) Note that
	$$\begin{bmatrix}
	A& 0\\
	C& D
	\end{bmatrix} = \begin{bmatrix}
	A& 0\\
	C& I_m
	\end{bmatrix} \begin{bmatrix}
	I_n& 0\\
	0& D
	\end{bmatrix};$$
	Then $$det \begin{bmatrix}
	A& 0\\
	C& I_m
	\end{bmatrix} \begin{bmatrix}
	I_n& 0\\
	0& D
	\end{bmatrix} = det  \begin{bmatrix}
	A& 0\\
	C& I_m
	\end{bmatrix} det \begin{bmatrix}
	I_n& 0\\
	0& D
	\end{bmatrix} = det(A)det(C) $$	
	(3) Note that
	$$\begin{bmatrix}
	A& B\\
	C& D
	\end{bmatrix} = \begin{bmatrix}
	A& 0\\
	C& I_m
	\end{bmatrix} \begin{bmatrix}
	I_n& A^{-1}B\\
	0& D-CA^{-1}B
	\end{bmatrix};$$
	then use (2).
\end{proof}

\subsection{Vandermonde matrix and determinant}\index{Vandermonde matrix}

\begin{definition}  For any list of complex numbers $(x_1,x_2,...,x_n)$, the associated following matrix
	$$V_n(x_1,x_2,...,x_n) = \begin{pmatrix}
	1 & 1 & \cdots & 1\\ 
	x_1 & x_2 & \cdots & x_n\\ 
	\vdots  & $\vdots$ & \ddots & \vdots\\ 
	x_1^{n-1} & x_2^{n-1} & \cdots & x_n^{n-1}
	\end{pmatrix},$$
is called \textbf{Vandermonde matrix}.	
\end{definition}

\begin{lemma}[determinant of Vandermonde matrix]\label{ch:linearalgebra:th:determinantOfVandermondeMatrix}
Consider a Vandermonde matrix associated with $n$ complex numbers $(x_1,x_2,...,x_n)$.
It follows that
\begin{itemize}
	\item 
	$$det V_n(x_1,x_2,...,x_n) = \prod_{1\leq i< j\leq n} (x_j-x_i).$$
	note that there are $n(n-1)/2$ terms in the product.
	\item 
	If $x_1,x_2,...,x_n$ are not pairwise distinct, then
	$$det V_n(x_1,x_2,...,x_n) = 0.$$
\end{itemize}
\end{lemma}

\begin{example}\hfill
\begin{itemize}
	\item 
	$$det V_2(x_1,x_2) = det \begin{pmatrix}
	1 & 1\\
	x_1 & x_2
	\end{pmatrix} = (x_2-x_1).$$
	\item 
	$$det V_3(x_1,x_2,x_3) = det \begin{pmatrix}
	1 & 1 & 1\\
	x_1 & x_2 & x_3\\
	x_1^2 & x_2^2 & x_3^2
	\end{pmatrix} = (x_2-x_1)(x_3-x_2)(x_3-x_1).$$
\end{itemize}	
\end{example}


\begin{lemma}[application example: existence of polynomial passing points]
If $(x_1,y_1),...,(x_n,y_n)$ are \textbf{distinct} complex number pairs. Then there exists a polynomial of degree $\leq n-1$ uniquely determined by the conditions
$$P(x_1)=y_1, P(x_2) = y_2,...,P(x_n) = y_n.$$ 	
\end{lemma}
\begin{proof}
Consider a polynomial with degree less than $n-1$ given by
$$P(x) = a_0 +a_1x + \cdots + a_{n-1} x^{n-1},$$
where the coefficients $a_0,a_1,...,a_{n-1}$ are to be determined. The conditions
$$P(x_1)=y_1, P(x_2) = y_2,...,P(x_n) = y_n,$$ 
gives the following linear systems
\begin{align*}
a_0 + a_1x_1 + a_2x_1^2 + ... + a_{n-1}x_{1}^{n-1} &= y_1\\
a_0 + a_1x_2 + a_2x_2^2 + ... + a_{n-1}x_{2}^{n-1} &= y_2\\
\cdots & \cdots \\
a_0 + a_1x_n + a_2x_n^2 + ... + a_{n-1}x_{n}^{n-1} &= y_n,
\end{align*}
which can be written as matrix form as
$$\underbrace{\begin{pmatrix}
1 & x_1 & \cdots & x_1^{n-1}\\ 
1 & x_2 & \cdots & x_2^{n-1}\\ 
\vdots  & \vdots & \ddots & \vdots\\ 
1 & x_n & \cdots & x_n^{n-1}
\end{pmatrix}}^{V_n(x_1,x_2,...,x_n)^T} \cdot \begin{pmatrix}
a_0\\
a_1\\
\vdots\\
a_{n-1}
\end{pmatrix} = \begin{pmatrix}
y_1\\
y_2\\
\vdots\\
y_n
\end{pmatrix}$$

Because $x_1,x_2,...,x_n$ are distinct, then from \autoref{ch:linearalgebra:th:determinantOfVandermondeMatrix}, $det V_n^T = det V_n \neq 0;$ that is we can uniquely solve for $a_0,a_1,...,a_{n-1}$.
\end{proof}


\section{Numerical iteration analysis}
\subsection{Numerical linear equation solution}
\subsubsection{Goals and general principles}
\begin{mdframed}
\begin{itemize}
	\item The \textbf{goal} is to solve linear system $Ax=b$, where $A$ is nonsingular,  using iterative method.
	\item The \textbf{general principle} is to decompose $A=M-N$ with $M$ being nonsingular, then 
	$$Mx = b + Nx \Rightarrow x = M^{-1}(b+Nx)$$
	We then use Banach fixed point theorems(\autoref{ch:functional-analysis:sec:contraction-mapping-and-fixed-point-theorems}) to obtain convergence condition for $M, N$ and $A$. 
\end{itemize}
\end{mdframed}


\begin{theorem}[general convergence condition]\cite[614]{golub2013matrix}
The iteration $x = M^{-1}(b+Nx)$ converges to $x^* = A^{-1}b$ for all initial starting vector $x^0$ if $\rho(G) < 1$, where $G=M^{-1}N$
\end{theorem}
\begin{proof}
	Let $T$ be the operator such that $x = M^{-1}(b+Nx)$. Then $Tx^* = x^*$ indicting that $x^*$ is the fixed point. 
	$Tx-Tx^* = G(x-x^*) \Rightarrow T^nx - T^n x^* \to 0$ as $n\to \infty$ as $\rho(G) < 1$ implies $G^n\to 0$.
	(We can also use \autoref{ch:linearalgebra:th:existencematrixnormclosetospectralradius} to show there is a matrix norm such that $\rho(G) < 1 \implies \norm{G} < 1$.)
\end{proof}




\subsubsection{Jacobi algorithm}
\begin{definition}[Jacobi algorithm]\index{Jacobi algorithm}
Let $A = D + L + U$. Then the Jacobi algorithm is using the following iteration:
$$x = D^{-1}(b - (L+U)x)$$
\end{definition}

\begin{lemma}[sufficient condition for convergence]\cite[615]{golub2013matrix}
	The Jacobi algorithm will converge for any initial $x^0$ if $A$ is strictly diagonally dominant.
\end{lemma}
\begin{proof}
	It can be showed that the row sum of the $M$ matrix ($M = D^{-1}(L + U)$)is less than 1 and the diagonal entry of $M$ is 0. Then we can use \autoref{ch:linearalgebra:th:Gerschgorintheorem} to show the $\rho(M)<1$.
\end{proof}

\subsubsection{Gauss seidel algorithm}
\begin{definition}[Gauss seidel algorithm]\index{Gauss seidel algorithm}
	Let $A = D + L + U$. Then the Gauss seidel algorithm is using the following iteration:
	$$x = (D + L)^{-1}(b - Ux)$$
\end{definition}

\begin{lemma}[sufficient condition for convergence]\cite[615]{golub2013matrix}
	The Gauss Seidel algorithm will converge for any initial $x^0$ if $A$ is symmetric positive definite.
\end{lemma}


\begin{lemma}\cite[122]{saad2003iterative}
	The Gauss Seidel algorithm will converge for any initial $x^0$ if $A$ is strictly diagonally dominant.
\end{lemma}
\begin{proof}
Let $M = (D+L)^{-1}U$. 
Let $x$ be an eigenvector of $M$ such that $\norm{x}_\infty = 1$. Assume the $i$th component $x_i$ satisfying $\abs{x_i} = 1$. Then
$Ux = \lambda(D+L)$ and for the $i$th row we have $$\sum_{j<i} a_{ij}x_j  = \lambda(a_{ii} + \sum_{j>i} a_{ij}x_j)$$
Further we have
$$\abs{\lambda} = \abs{\frac{\sum_{j<i} a_{ij}x_j}{(a_{ii} + \sum_{j>i} a_{ij}x_j)}} \leq \frac{\sum_{j<i} \abs{a_{ij}}}{a_{ii} - \sum_{j>i} \abs{a_{ij}}} <1.$$
Therefore, $\rho(M) < 1$.
\end{proof}


\subsection{Numerical eigen-decomposition}
\begin{theorem}[power method for top eigenvector]\cite[115]{ma2002generalized}\index{power method}\cite[451]{golub2013matrix}
Let $A\in \R^{N\times N}$ be a real symmetric positive definite matrix with eigenvector $\{u_1,...,u_N\}$ and eigenvalues $\{\lambda_1,...,\lambda_N\}$ sorted in descending order. Assume $\lambda_1 > \lambda_2$ and let $u^0\in \R^N$ be an arbitrary vector has nonzero projection $a_1$ on $u_1$. Consider the sequence of vectors
$$u^{k+1} = \frac{Au^k}{\norm{Au^k}}.$$
We have 
\begin{itemize}
	\item $u^k$ converges to $\frac{a_1}{\abs{a_1}}u_1$ with rate $\frac{\lambda_2}{\lambda_1}$. That is, there exist a constant $C > 0$ such that for all 
	$k\geq 0$, 
	$$\norm{u^k - \frac{a_1}{\abs{a_1}}u_1} \leq C(\frac{\lambda_2}{\lambda_1})^k.$$
\end{itemize}	
\end{theorem}
\begin{proof}
Note that The iterate $u^k$ is a multiple of $A^k u^0$ with length 1.Let $u^0 = sum_{i=1}^N a_1 u_1$. Let $A^k$ has eigendecomposition of $A^k = \sum_{i=1}^N \lambda_i^ku_iu_i^T$, then
\begin{align*}
u^k &= \frac{A^k u^0}{\norm{A^k u^0}} \\
&= \frac{\sum_{i=1}^N \lambda^k_i a_i u_i}{\sqrt{\lambda^{2k}_i a_i^2}}\\
&\leq \frac{ \lambda^k_1 a_1 }{\lambda^k_1 \abs{a_1}}+\frac{\sum_{i=2}^N \lambda^k_i a_i u_i}{\lambda^k_1 \abs{a_1}}
\end{align*}
Then $$\norm{u^k - \frac{a_1}{\abs{a_1}} u_1} \leq C\frac{\lambda_2^k}{\lambda_1^k}.$$
\end{proof}


\begin{remark}[not a contraction mapping]
	It can be showed that $\frac{a_1}{\abs{a_1}}u_1$ is a fixed point for the mapping $$T(u) = \frac{Au}{\norm{Au^k}}.$$
	However, for any vector on the subspace spanned by $u_1$, the mapping will not shrink it; therefore, the mapping is not a contraction. 
\end{remark}

\begin{corollary}[extension to real symmetric matrix]
Let $A\in \R^{N\times N}$ be a real symmetric positive definite matrix with eigenvector $\{u_1,...,u_N\}$ and eigenvalues $\{\lambda_1,...,\lambda_N\}$ sorted in descending order.Then  the sequence of vectors generated by
$$u^{k+1} = \frac{Au^k}{\norm{Au^k}}$$
will converge to the eigenvector(up to scale) associated with eigenvalue with largest absolute value. 
\end{corollary}


\begin{theorem}[power method for top d eigenvectors, orthogonal iteration]\cite[115]{ma2002generalized}\index{power method}\index{orthogonal iteration}\cite[454]{golub2013matrix}
	Let $A\in \R^{N\times N}$ be a real symmetric positive definite matrix with eigenvector $\{u_1,...,u_N\}$ and eigenvalues $\{\lambda_1,...,\lambda_N\}$ sorted in descending order. Assume that $\lambda_d > \lambda_{d+1}$ and let $U^0\in \R^{N\times d}$ be an arbitrary matrix whose column space is not orthogonal to the subspace $\{u_1,...,u_d\}$ spanned by the top $d$ eigenvectors of $A$. Consider the sequence of the matrices
	$$U^{k+1} = AU^k(R^k)^{-1}$$
	where $Q^kR^k = AU^k$ is the QR decomposition of $AU^k$.
	We have 
	\begin{itemize}
		\item  $U^k$ converges to a matrix $U$ whose columns are the top $d$ eigenvectors of $A$ with rate of convergence $\frac{\lambda_{d+1}}{\lambda_d}$.
	\end{itemize}	
\end{theorem}



\section{Graph Laplacian theory}
\begin{definition}[Laplacian matrix]\index{Laplacian matrix}\index{graph Laplacian}\label{ch:linearalgebra:def:graphLaplacian}
	Let $W\in \R^{N\times N}_+$ be a symmetric matrix, let $D\in \R^{N\times N}$ be a diagonal matrix with $d_{jj}=\sum_{i=1}^N w_{ij}$, then the symmetric matrix $L=D-W$ is called Laplacian matrix.
\end{definition}

\begin{theorem}[spectral property of Laplacian matrix]\label{ch:linearalgebra:def:spectrumgraphLaplacian}
	\cite[140]{vidal2016generalized}\cite{von2007tutorial}Let $L$ be a Laplacian matrix, then
	\begin{itemize}
		\item $L$ is positive semi-definite.
		\item The vector of all ones $\bm{1}$ is the eigenvector associated with zero eigenvalue, that is $L\bm{1} = 0$
		\item $L$ is diagonalizable but $L$ is singular matrix.
		\item $L$ has $n$ non-negative, real-valued eigenvalues $$0 = \lambda_1 \leq \lambda_2 \leq \dots \leq \lambda_n.$$
	\end{itemize}
\end{theorem}
\begin{proof}
	(1) Let $y\in \R^N$ be a nonzero vector, we have
	$$y^TDy = \sum_{i}y^i(y_i\sum_j w_{ji})=\sum_{i}\sum_{j}y^2_i w_{ji} = \sum_{i}\sum_{j}y^2_j w_{ij}$$
	$$y^TWy=\sum_{i}\sum_{j}y_iy_j w_{ij}$$
	where we use the fact that $W$ is symmetric. Then we have
	$$y^T(D-W)y=y^TLy = \frac{1}{2}\sum_{i}\sum_{j}w_{ij}(y_i-y_j)^2 \geq 0.$$
	We can also directly prove (1) using \autoref{ch:linearalgebra:th:GerschgorinTausskytheorem}\autoref{ch:linearalgebra:th:Gerschgorintheorem}.
	
	(2) Directly from the definition of $L$
	(3) $L$ is real symmetric and therefore diagonalzable. $L$ is singular since it has eigenvalue 0. (4) from (1)(3).
\end{proof}  


\begin{lemma}[number of connected components from spectrum of $L$]\cite{von2007tutorial}
	Let $G$ be an undirected graph with non-negative weights with Laplacian $L$. Then multiplicity $k$ of eigenvalue 0 of $L$ equals the number of connected components $A_1,...,A_k$ in the graph. The eigenspace associated eigenvalue 0 has dimensionality $k$ spanned by the indicator vectors $1_{A_1},...,1_{A_k}$ of these components.  
\end{lemma}
\begin{proof}
	It is easy to show that $1_{A_1},...,1_{A_k}$ satisfy $L1_{A_i} = 0$. Note that from  \autoref{ch:linearalgebra:th:symmetricdecomposition}, we note that the geometric multiplicity is the same as the algebraic multiplicity. Therefore,  $1_{A_1},...,1_{A_k}$ will span the eigenspace. 
\end{proof}

\begin{remark}[eigenvectors]
	The eigenvector calculated from software will usually be scaled version of the indicator vectors. 
\end{remark}


\begin{definition}[normalized graph Laplacian]\cite{von2007tutorial}\label{ch:linearalgebra:def:normalizedgraphLaplacian}
	Given a Laplacian $L$ associated with a graph $G$, we can defined normalized Laplacian as:
	\begin{itemize}
		\item $L_{sym} = D^{-1/2}LD^{-1/2} = I - D^{-1/2}WD^{-1/2}$
		\item $L_{rw} = D^{-1}L = I - D^{-1}W.$
	\end{itemize}
	where $D=diag(d_1,...,d_N), d_i = \sum_{ij}w_j$.
\end{definition}


\begin{remark}[connection to Markov chain transition matrix]
	The transition matrix $P$ of a Markov chain is given as
	$$P = D^{-1}W = I - L_{rw}.$$
\end{remark}


\begin{theorem}[spectrum properties of normalized Laplacian]\label{ch:linearalgebra:th:spectrumnormalizedgraphLaplacian}
	The normalized Laplacians satisfy the following properties:
	\begin{itemize}
		\item For every $v\in \R^N$, we have
		$$v^TL_{sym}v = \frac{1}{2}\sum_{i,j} w_{ij}(\frac{v_i}{\sqrt{d_i}}-\frac{v_j}{\sqrt{d_j}})^2.$$
		\item $\lambda$ is an eigenvalue of $L_{rw}$ with eigenvector $u$ if and only if $\lambda$ is an eigenvalue of $L_{sym}$ with eigenvector $w = D^{1/2}u$.
		\item $\lambda$ is an eigenvalue of $L_{rw}$ with eigenvector $u$ if and only if $\lambda$ and $u$ satisfy $$Lu = \lambda Du.$$
		\item 0 is an eigenvalue $L_{rw}$ with has the eigenvector of the constant one vector $1$. 0 is an eigenvalue of $L_{sym}$ with eigenvector $D^{1/2}1$.
		\item $L_{sym}$ and $L_{rw}$ are positive semi-definite and have $n$ non-negative real-valued eigenvalues $0\leq \lambda_1\leq...\leq \lambda_n.$
	\end{itemize}
\end{theorem}
\begin{proof}
	Straight forward from definitions of normalized graph Laplacian.
\end{proof}
\begin{remark}[short summary]\hfill
	\begin{itemize}
		\item $L_{sym}$ and $L_{rw}$ have exactly the same eigenvalues.
		\item $L$ has different nonzero eigenvalues from $L_{sym}$ and $L_{rw}$. However, they have the same number of zero eigenvalues.
	\end{itemize}	
\end{remark}


\begin{corollary}[number of connected components from spectrum of $L$]\cite{von2007tutorial}
	Let $G$ be an undirected graph with non-negative weights with Laplacian $L$. Then multiplicity $k$ of eigenvalue 0 of $L,L_{sym},L_{rw}$ equals the number of connected components $A_1,...,A_k$ in the graph. The eigenspace associated eigenvalue 0 of $L,L_{rw}$ has dimensionality $k$ spanned by the indicator vectors $1_{A_1},...,1_{A_k}$ of these components. The eigenspace associated eigenvalue 0 of $L_{sym}$ has dimensionality $k$ spanned by the indicator vectors $D^{1/2}1_{A_1},...,D^{1/2}1_{A_k}$ of these components.   
\end{corollary}



\section{Notes on bibliography}

For comprehensive treatment on both theory and applications of linear algebra and functional analysis on signal processing, see \cite{moon2000mathematical}.


For introductory level treatment in linear algebra, see \cite{meyer2000matrix}.
For intermediate to advanced treatment, see \cite{axler2015linear}\cite{banerjee2014linear}\cite{horn2012matrix}\cite{hom1991topics}.


For positive matrix theory, see \cite{luenberger1979introduction}\cite{horn2012matrix}.

For numerical linear algebra, see \cite{saad2003iterative}\cite{golub2013matrix}


\printbibliography

\stopcontents[chapters]
\end{refsection}