
\begin{refsection}
	\startcontents[chapters]	

\chapter{Regression Analysis}\label{ch:regression-analysis}
%\minitoc
	\printcontents[chapters]{}{1}{}


\section{Linear regression analysis: basics}\label{ch:theory-of-statistics:sec:linear-regression-analysis}\index{linear regression}
\begin{mdframed}
	\textbf{notations:}
	\begin{itemize}
		\item $\bm{1}$ is the vector of all 1.
		\item $J$ is a square matrix with all 1.
	\end{itemize}
\end{mdframed}


\subsection{linear regression models}
\begin{definition}[simple linear regression]
	The simple linear regression model \textbf{assumes} that a random variable $Y$ has a linear dependency on a non-random variable $X \in \R$ given as
	$$Y = \beta_0 + \beta_1 X + \epsilon$$
	where $\beta_0,\beta_1$ are unknown model parameters, and $\epsilon$ is a random variable. 
	Given the observed sample pairs $(x_1,y_1),(x_2,y_2),..., (x_n,y_n)$ as $y_i = \beta_0 + \beta_1 x_i + \epsilon_i$ and we \textbf{further make the following assumptions on $\epsilon$} as
	\begin{itemize}
		\item $E[\epsilon_i] = 0,\forall i$.
		\item $Var[\epsilon_j] = \sigma^2,\forall i$; and $\sigma^2$ is unknown.
		\item $cov(\epsilon_i,\epsilon_j) =\sigma^2 \delta_{ij},\forall i,j$.
	\end{itemize} 	
\end{definition}


\begin{definition}[multiple linear regression model]\label{ch:statistical-models:def:multipleLinearRegressionModel}
	The multiple linear regression model \textbf{assumes} that a random variable $Y$ has a linear dependency on a non-random vector $X = (X_1,X_2,...,X_{p-1}) \in \R^{p-1}$ given as
	$$Y = \beta_0 + \beta_1 X_1 +\beta_2 X_2 + ... +\beta_{p-1} X_{p-1} + \epsilon$$
	where $\beta_0,\beta_1, ...,\beta_p$ are unknown model parameters, and $\epsilon$ is a random variable. 
	Given the observed sample pairs $(x_1,y_1),(x_2,y_2),..., (x_n,y_n), x\in \R^{p-1}, y\in \R$ as $y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + ... + \epsilon_i$ and we \textbf{further make the following assumptions on $\epsilon$} as
	\begin{itemize}
		\item $E[\epsilon_i] = 0,\forall i$
		\item $cov(\epsilon_i,\epsilon_j) = \sigma^2\delta_{ij}$ and $\sigma^2$ is unknown.
	\end{itemize} 	
\end{definition}

\begin{assumption}[standard assumptions of linear regression model]\label{ch:regression-analysis:assumption:LinearRegressionStandardAssumption} \cite[17]{greene2017econometric}
The standard assumption of a linear regression model consists of
\begin{description}
	\item[A1. Linear model assumption] The random variable $Y$ has a linear dependency on $X$ give by
	$$Y = \beta_0 + \beta_1 X_1 +\beta_2 X_2 + ... +\beta_{p-1} X_{p-1} + \epsilon.$$
	\item[A2. Linear independence of regressors] There is no linear dependency exists among regressors $X_1, X_2,...,X_{p-1}$.  
	\item[A3. Indepedence between regressor and noise] The regressors $(X_1,X_2,...,X_{p-1})$ are independent of the noise term $\epsilon$.
	\item[A4. Homoscedaticity] Homoscendaticity refers to that the conditional variance $Var[\epsilon|X_i],i=1,...,p-1$ is a constant given the observation of the regressors. This can be written by
	$$ Var[\epsilon | X_i] = \sigma^2, \forall i.$$  
	\item[A5. Data generation of regressors] The regressor data $(x_1,x_2,...)$ can be either constants from experimental design or realizations of random variables. 
	\item[A6. Normal distribution of noise] The noise random samples $\epsilon_1, \epsilon_2,...$ have normal distribution and independent of each other, which can be written by
	$$\epsilon | X_i \sim N(0, \sigma^2 I), \forall i.$$ 
\end{description}	
\end{assumption}



\begin{remark}[understand data generation process]\cite[25]{greene2017econometric}
In the application of linear regression model, there are usually two types of data generation processses on how we obtain the regressor observations.	
\begin{itemize}
	\item In a physics experiment, the experimentalist will choose different regressor values and observe the output $y$. In this case, regressor values are certainly not sampled from a distribution.
	\item In a social study where social scientist usually cannot design experiments like physicist. In this case, we assume regressors are random variables and regressor observations are sampled from a distribution.  
\end{itemize}
\end{remark}


\begin{note}[understand linear regression from the perspective of approximation]\hfill
\begin{itemize}
	\item In the linear regression model, our ultimate goal is to model the \textbf{conditional distribution} of random variable $Y$ given by the observations of random variables $X_1,X_2,...,X_p$, that is
	$$P(Y|X_1,X_2,...,X_p).$$
	\item  We know that the distribution $P(Y|X_1,X_2,...,X_p)$ is fully determined by all of its moments given by
	\begin{align*}
	&E[Y|X_1,X_2,...,X_p] \\
	&E[Y^2|X_1,X_2,...,X_p] \\
	&E[Y^3|X_1,X_2,...,X_p] \\
	&\cdots
	\end{align*} 
	\item In the linear model, we \textbf{assume} 
	$$Y = \beta_0 + \beta_1 X_1 +\beta_2 X_2 + ... +\beta_{p} X_p + \epsilon, \epsilon \in N(0,\sigma^2).$$
	In this way, the conditional distribution $P(Y|X_1,...,X_p)$ is fully determined by the vector $\beta$ and the variance coefficent $\sigma^2$. 
	In particular
	\begin{itemize}
		\item the first moment
	$$E[Y|X_1,...,X_p] = f(X_1,...,X_p)=\beta_0 + \beta_1 X_1 +\beta_2 X_2 + ... +\beta_{p} X_p.$$ 
	\item the second moment(variance)
	$$Var[Y|X_1,...,X_p] = g(X_1,...,X_p) = \sigma^2.$$ 
		By assuming $E[\epsilon_i\epsilon_j]=\sigma^2\delta_{ij}$ we are assuming $Var[Y|X_1,X_2,...,X_p]$ is homogeneous, i.e., has no dependence on $X_1,X_2,...,X_p$
	\end{itemize} 
	\item In our linear model, we select coefficients $\beta$ to maximize the likelihood of observation $y_1,y_2,...,y_n$ given observations of $X_1,...,X_p$.
\end{itemize}	
\end{note}



\subsection{Least square solutions}
\subsubsection{Orthogonal projection and the least square result}
\begin{lemma}[essential properties of orthogonal projection]\label{ch:theory-of-statistics:th:propertylinearregression}\index{orthogonal projection}
	Let $X$ be a matrix of size $n\times p$. 
	Define $$H = X(X^TX)^{-1}X^T,$$ we have
	\begin{itemize}
		\item $(X^TX)^{-1}$ exists if $X$ has full column rank.
		\item $rank((X^TX)) = p$ and $rank(H) = p$.
		\item $H$ is symmetric and idempotent; in other words, $H$ is an orthogonal projector onto the subspace spanned by columns of $X$.
		\item $I-H$ is symmetric and idempotent; in other words, $I-H$ is an orthogonal projector onto the orthogonal complementary subspace spanned by columns of $X$.
		\item The prediction is given as $HY$
		\item The residual is given as $(I-H)Y$
	\end{itemize}
\end{lemma}
\begin{proof}
	(1) Use the fact that $\cN(X^TX) = \cN(X)$(\autoref{ch:linearalgebra:ranklemmaone}). If $X$ is invertible then $X^TX$ is invertible.
	(2) $X^TX$ is invertible, therefore has full rank of $p$.  Use \autoref{ch:linearalgebra:th:rankOfMatrixProducts}. $$rank(X(X^TX)^{-1}) = rank((X^TX)^{-1}) = p, rank(X(X^TX)^{-1}X^T) = rank((X^TX)^{-1}) = p.$$
	(3) Direct verification. \autoref{ch:linearalgebra:th:characterizationoforthogonalprojector}.
	(4) $(I-H)^T = I - H$, and $(I-H)(I-H) = (I - 2H + H^2) = (I - 2H + H) = I - H$.
	(5) $\hat{Y} = X(X^TX)^{-1}X^TY = HY$.
	(6) $\epsilon = Y - \hat{Y} = Y - HY = (I-H)Y$.
\end{proof}



\begin{theorem}[least square solution: general case ]\label{ch:statistical-models:th:leastSquareSolution}\label{ch:statistical-models:th:OrdinaryLinearRegressionleastSquareSolution}\index{least square}
	The multiple linear regression with n samples can be written as
	\begin{align*}
	\begin{bmatrix}
	y_1\\
	y_2\\
	\vdots\\
	y_n
	\end{bmatrix} = \begin{bmatrix}
	1 & x_{11} & x_{12} & \dots & x_{1(p-1)}\\
	1 & x_{21} & x_{22} & \dots & x_{2(p-1)}\\
	\vdots & \vdots & \vdots & \vdots & \vdots \\
	1 & x_{n1} & x_{n2} & \dots & x_{n(p-1)}\\
	\end{bmatrix}
	\begin{bmatrix}
	\beta_0\\
	\beta_1\\
	\vdots\\
	\beta_{p-1}
	\end{bmatrix}
	+ \begin{bmatrix}
	\epsilon_1\\
	\epsilon_2\\
	\vdots\\
	\epsilon_{n}
	\end{bmatrix}
	\end{align*}
	with matrix form
	$$Y = X\beta + \epsilon.$$
Assume the standard assumptions (\autoref{ch:regression-analysis:assumption:LinearRegressionStandardAssumption}) hold.	
	The \textbf{unique minimizer} to the problem 
	$$\min_\beta (Y-X\beta)^T(Y - X\beta)$$
	is given as
	$$\hat{\beta} = (X^TX)^{-1}X^TY,$$
	in particular, $$\hat{\beta}_0 = \mean{y} - \sum_{i=1}^{p-1} \hat{\beta}_i \mean{x}_i$$
	$$\hat{y}_i - \mean{y} =\sum_{i=1}^{p-1} \hat{\beta}_j (x_{ij}-\mean{x}_j) $$
	Moreover, we have
	\begin{itemize}
		\item $E[\hat{\beta}] = \beta$.
		\item If $Cov[Y] = \sigma^2I$, then $Cov[\hat{\beta}] = \sigma^2(X^TX)^{-1}$.($\hat{\beta}$ is not necessarily normal).
		\item To get each individual coefficient, we have 
		$$\hat{\beta}_i = \frac{X_i^T(I - H_{-i})Y}{X_i^T(I-H_{-i})X_i},$$
		where $H_{-i} = X_{-i}(X_{-i}^TX_{-i})^{-1} X_{-i}^T$, $X_{-i}$ is the matrix without column $i$.
	\end{itemize}
\end{theorem}
\begin{proof}
	See \autoref{ch:functional-analysis:th:normalequation}.
	(1) (unbiasedness) $E[\hat{\beta}] = (X^TX)^{-1}X^T E[Y] = (X^TX)^{-1}X^TX\beta = \beta.$
	(2) (variance) $Cov[\beta] = (X^TX)^{-1}X^TCov[Y]((X^TX)^{-1}X^T)^T = \sigma^2(X^TX)^{-1}$.
	(3) See \autoref{ch:functional-analysis:th:normalequation}. We can interpret as first projecting $Y$ into the null space of $(I-H_{-i})X_{-i}$ and project onto $X_i$. 
\end{proof}

\begin{theorem}[least square solution: demean case ]\index{least square}\label{ch:statistical-models:th:leastSquareSolutionDemeanCase}
Consider a multiple linear regression problem where $y$ and $x$ are demeaned; that is, $\sum_i y_i = 0, \sum_{i}x_{i,j} = 0, j=1,2,...,p$. 	The multiple linear regression with n samples can be written as
\begin{align*}
\begin{bmatrix}
y_1\\
y_2\\
\vdots\\
y_n
\end{bmatrix} = \begin{bmatrix}
\beta_0\\
\beta_0\\
\vdots\\
\beta_0
\end{bmatrix}+\begin{bmatrix}
x_{11} & x_{12} & \dots & x_{1(p)}\\
x_{21} & x_{22} & \dots & x_{2(p)}\\
\vdots & \vdots & \vdots & \vdots \\
x_{n1} & x_{n2} & \dots & x_{n(p)}\\
\end{bmatrix}
\begin{bmatrix}
\beta_0\\
\beta_1\\
\vdots\\
\beta_{p}
\end{bmatrix}
+ \begin{bmatrix}
\epsilon_1\\
\epsilon_2\\
\vdots\\
\epsilon_{n}
\end{bmatrix}.
\end{align*}
	with matrix form
	$$Y = \beta_0\bm{1} + X\beta + \epsilon$$
	The \textbf{unique minimizer} to the problem 
	$$\min_\beta (Y-\beta_0\bm{1} -X\beta)^T(Y - \beta_0\bm{1} - X\beta)$$
	is given as
	$$\hat{\beta}_0 = \mean{y} - \sum_{i=1}^{p-1} \hat{\beta}_i \mean{x}_i,$$
	and
	$$\hat{\beta} = (\tilde{X}^T\tilde{X})^{-1}\tilde{X}^T\tilde{Y},$$
	where $\tilde{X}_{ij} = X_{ij} - \mean{x}_j, \tilde{Y} = Y - \mean{y}.$
	
	Eventually, we can write
	$$\hat{y}_i - \mean{y} =\sum_{j=1}^{p-1} \hat{\beta}_j (x_{ij}-\mean{x}_j) $$
	Moreover, we have
	\begin{itemize}
		\item $E[\beta_0] = \beta_0, E[\hat{\beta}] = \beta$.
		\item (to do)If $Cov[Y] = \sigma^2I$, then $Cov[\hat{\beta}] = \sigma^2(X^TX)^{-1}$.($\hat{\beta}$ is not necessarily normal)
	\end{itemize}
\end{theorem}
\begin{proof}
(1)We can use the projection theorem \autoref{ch:functional-analysis:th:normalequation} or use the following optimization method. 

$$\min f = (Y - \beta_0\bm{1} - X\beta)^T(Y - \beta_0\bm{1} - X\beta)$$
over $\beta_0, \beta_1$, we have
\begin{align*}
f(\beta_0,\beta_1) &= Y^TY + n^2\beta_0^2 + (\beta^TX^TX\beta)+ 2n\beta_0\bm{1}^TX\beta - 2n\beta_0\bm{1}^TY- 2Y^TX\beta \\
&= Y^TY + n^2\beta_0^2 + (\beta^TX^TX\beta)+ 2n\beta_0\bm{1}^TX\beta - 2n\beta_0\sum_{i=1}^n y_i - 2Y^TX\beta 
\end{align*} 
The first order condition on $\beta_0$ gives that
$$\beta_0 = \frac{1}{n}\sum_{i=1}^n y_i - \frac{1}{n} \bm{1}^TX\beta;$$
Plug in $\beta_0$(note that $(I - \frac{1}{n}\bm{1}\bm{1}^T)X = \tilde{X}$), we can transform the minimization problem to
$$f(\beta_0,\beta_1) = (\tilde{Y} - \tilde{X}\beta)^T(\tilde{Y} - \tilde{X}\beta).$$
Then we can use the results in the general case(\autoref{ch:statistical-models:th:leastSquareSolution}) to obtain the estimator for $\beta$.
	(2) (unbiasedness) 
	\begin{align*}
	E[\hat{\beta}_0] &= E[ \mean{y} - \sum_{i=1}^{p-1} \hat{\beta}_i \mean{x}_i] \\
	&= E[ \mean{y}] - \sum_{i=1}^{p-1} E[\hat{\beta}_i \mean{x}_i] \\
	&= E[\frac{1}{n}\bm{1}^TY] - \sum_{i=1}^{p-1} E[\hat{\beta}_i ]\mean{x}_i \\
	&= \frac{1}{n}\bm{1}^TE[Y] - \sum_{i=1}^{p-1} \frac{1}{n}\bm{1}X_i\beta_i\\
	&=\frac{1}{n}\bm{1}^T(Y - X\beta)\\
	&=\frac{1}{n}\bm{1}^T(\beta_0\bm{1} + X\beta + \epsilon - X\beta)\\
	& = \frac{1}{n}\bm{1}^T\beta_0\bm{1} \\
	&=\beta_0.
	\end{align*}
where we use the fact that $E[\hat{\beta}] = \beta$ proved later.
	
	$$E[\hat{\beta}] = (\tilde{X}^T\tilde{X})^{-1}\tilde{X}^T E[Y] = (X^TX)^{-1}\tilde{X}^T\tilde{X}\beta = \beta.$$
\end{proof}

\begin{remark}[another way to see the coefficients in the demean case]
From \autoref{ch:statistical-models:th:leastSquareSolution}, we can see that if we want to get the coefficients $\beta = (\beta_1,\beta_2,...,\beta_{p-1})$, we can use
$$\beta = (X^T(I-H_{0})(I-H_{0})X)^{-1}X^T(I - H_{0})(I - H_{0})Y,$$
where $H_{0} = \frac{1}{n}\bm{1}\bm{1}^T$.
Note that $(I - H_{0})Y$ will generate a demeaned $Y$ and $(I - H_{0})X$ will generate a column-wise demeaned $X$. This is because the $I - H_0$ operator will is a demean operator.
\end{remark}



\begin{corollary}[special cases, linear square solution for simple regression]\hfill
	\begin{itemize}
		\item For the zero order model $y = \beta_0 + \epsilon$, $$\hat{\beta}_0 = \frac{\ip{y,\bm{1}}}{\ip{\bm{1},\bm{1}}} = \frac{1}{n}\sum_i y_i$$
		\item For the first order model $y = \beta_0 +\beta_1 x+ \epsilon$, $$\hat{\beta} = [X^TX]^{-1}X^TY$$
		we have $$\hat{\beta}_0 = \mean{y} - \hat{\beta}_1\mean{x}$$ and
		\begin{align*}
		\hat{\beta}_1 &= \frac{\sum_i (x_{i}-\mean{x})(y_i - \mean{y})}{\sum_i (x_{i}-\mean{x})^2} \\
		& = \frac{S_{XY}}{S_{XX}} \\
		& = \frac{\sum_i (x_{i}-\mean{x})y_i }{\sum_i (x_{i}-\mean{x})(x_i)} \\
		& = \frac{\sum_i x_{i}(y_i - \mean{y})}{\sum_i (x_{i}-\mean{x})x_i} 
		\end{align*}
		In summary, $$\hat{y}_i = \mean{y} + \hat{\beta}_1(x_i - \mean{x}).$$
		\item In the first order model, we have conditional mean and  unconditional variance of $y$ given by
		$$E[y|x] = \beta_0+\beta_1x, Var[y] = \sigma^2, Var[\mean{y}]=\frac{\sigma^2}{n}.$$
		\item In the first order model, we have unbiased coefficient estimator
		$$E[\hat{\beta}_1] = \beta_1, E[\beta_0] = \beta_0.$$
		\item In the first order model, we have 
		$$Var[\hat{\beta}_1] = \frac{\sigma^2}{S_{XX}}, Var[\hat{\beta}_0] = \sigma^2(\frac{1}{n} + \frac{\mean{x}^2}{S_{XX}}),Cov(\hat{\beta}_1,\hat{\beta}_0) = -\frac{\sigma^2\mean{x}}{S_{XX}}$$
		where
		$$S_{XX} = \sum_i (x_{i}-\mean{x})^2=\sum_i (x_{i}-\mean{x})x_i.$$
	\end{itemize}
\end{corollary}
\begin{proof}
(2)
$$Var[y] = E[(y-E[y])^2] = E[(y-E[y|x])^2] = E[\epsilon^2] = \sigma^2.$$	
(3)Note that 
\begin{align*}
\sum_i (x_{i}-\mean{x})^2 &= x^T(I-\frac{1}{n}J)(I-\frac{1}{n}J)x \\
&= x^T(I-\frac{1}{n}J)x \\
&= x^T(x-\mean{x}\bm{1}) 
\end{align*}
and
\begin{align*}
\sum_i (x_{i}-\mean{x})(y_i - \mean{y}) &= x^T(I-\frac{1}{n}J)(I-\frac{1}{n}J)y \\
&= x^T(I-\frac{1}{n}J)y \\
&= x^T(y-\mean{y}\bm{1}) \\ 
&= y^T(x-\mean{x}\bm{1})
\end{align*}
(4)
\begin{align*}
E[\hat{\beta}_1] &= E[\frac{\sum_i (x_{i}-\mean{x})y_i }{\sum_i (x_{i}-\mean{x})(x_i)}] \\
&= E[\frac{\sum_i (x_{i}-\mean{x})(\beta_0+\beta_1x_i+\epsilon_i)}{\sum_i (x_{i}-\mean{x})(x_i)}] \\
&=\frac{\sum_i (x_{i}-\mean{x})(\beta_0+\beta_1x_i)}{\sum_i (x_{i}-\mean{x})(x_i)} \\
&=\frac{\sum_i (x_{i}-\mean{x})(\beta_0)}{\sum_i (x_{i}-\mean{x})(x_i)} + \frac{\sum_i (x_{i}-\mean{x})(\beta_1x_i)}{\sum_i (x_{i}-\mean{x})(x_i)} \\
&= 0 + \beta_1 = \beta_1.
\end{align*}
and
\begin{align*}
E[\hat{\beta}_0] &= E[\mean{y} - \hat{\beta}_1\mean{x}] \\
 &= \frac{1}{n}\sum_i E[y_i - \hat{\beta}_1x_i] \\
 &=\frac{1}{n}\sum_i E[\beta_0 + \beta_1x_i - \hat{\beta}_1x_i] \\
 &=\frac{1}{n}\sum_i \beta_0 \\
 &=\beta_0. 
\end{align*}
(5)
\begin{align*}
Var[\hat{\beta}_1] &= Var[\frac{\sum_i (x_{i}-\mean{x})y_i }{\sum_i (x_{i}-\mean{x})^2}] \\
&= Var[\frac{\sum_i (x_{i}-\mean{x})(\beta_0+\beta_1x_i+\epsilon_i)}{\sum_i (x_{i}-\mean{x})^2}] \\
&= Var[\frac{\sum_i (x_{i}-\mean{x})(\epsilon_i)}{\sum_i (x_{i}-\mean{x})(x_i)}] \\
&=\sigma^2 \sum_i \frac{\sum_i (x_{i}-\mean{x})^2}{(\sum_i (x_{i}-\mean{x})^2)^2}\\
&=\sigma^2 \frac{1}{(\sum_i (x_{i}-\mean{x})^2)}
\end{align*}
and
\begin{align*}
Var[\hat{\beta}_0] &= Var[\mean{y} - \hat{\beta}_1\mean{x}] \\
&=Var[\mean{y}] + \mean{x}^2Var[\hat{\beta}_1] -2\mean{x}Cov(\mean{y},\hat{\beta}_1)\\
&= \frac{1}{n}\sigma^2 + \mean{x}^2\sigma^2/S_{XX} + 0 \\
&=\sigma^2(\frac{1}{n} + \frac{\mean{x}^2}{S_{XX}})
\end{align*}
where $Cov(\mean{y},\hat{\beta}_1) = 0$ since
\begin{align*}
Cov(\mean{y},\hat{\beta}_1) & = E[\frac{1}{n}\sum_{i} \epsilon_i (\sum_{i}c_i\epsilon_i -\beta_1)] \\
&=\frac{1}{n}E[\sum_i c_i \epsilon_i^2] \\
&=\frac{1}{n}\sum_i c_i \sigma^2\\
&=0
\end{align*}
where 
$$c_i =\frac{ (x_{i}-\mean{x})}{\sum_i (x_{i}-\mean{x})(x_i)}, \sum_i c_i = 0.$$

To calculate $Cov(\hat{\beta}_1,\hat{\beta}_0)$, we use the fact that
\begin{align*}
\hat{\beta}_0 &= \mean{y} - \hat{\beta}_1\mean{x} \\
\hat{\beta}_0 + \hat{\beta}_1\mean{x} &= \mean{y}  \\
Var[\hat{\beta}_0 + \hat{\beta}_1\mean{x}] &= Var[\mean{y}] = \frac{\sigma^2}{n} \\
Var[\hat{\beta}_0] + Var[\hat{\beta}_1]\mean{x}^2 + 2\mean{x}Cov(\hat{\beta}_1,\hat{\beta}_0) &= \frac{\sigma^2}{n} 
\end{align*}
then we can get 
$$Cov(\hat{\beta}_1,\hat{\beta}_0) = -\frac{\sigma^2\mean{x}}{S_{XX}}.$$
\end{proof}

\begin{remark}[interpretation]\hfill
\begin{itemize}
	\item We can interpret $x_i-\mean{x}$ and $y_i-\mean{y}$ as the part remove the projection (that is, $\mean{x}\bm{1}, \mean{y}\bm{1}$)onto the constant value subspace of $\bm{1}$. To see this, we have
	$$\sum_{i=1}^{n}(x_i - \mean{x})\mean{x} = n\mean{x}^2-n\mean{x}^2 = 0,$$
	and
		$$\sum_{i=1}^{n}(y_i - \mean{y})\mean{y} = n\mean{y}^2-n\mean{y}^2 = 0.$$
	Therefore, the formula 
	$$\hat{\beta}_1 = \frac{\sum_i (x_{i}-\mean{x})(y_i - \mean{y})}{\sum_i (x_{i}-\mean{x})^2}$$
	is consistent with the formula
	$$\hat{\beta}_i = \frac{X_i^T(I - H_{-i})Y}{X_i^T(I-H_{-i})X_i},$$
	where $H_{-i} = X_{-i}(X_{-i}^TX_{-i})^{-1} X_{-i}^T$, $X_{-i}$ is the matrix without column $i$.
	\item The coefficient $\hat{\beta}_j$ represents the additional contribution from $X_j$ after accounting for the contribution from $\bm{1},X_1,...,X_{j-1}, X_{j+1},...,X_p$.
\end{itemize}	

\end{remark}

\subsubsection{Orthogonal input and successive regression}


\begin{corollary}[multiple linear regression with orthogonal input]
Consider a multiple linear regression problem  with n samples can be written as
\begin{align*}
\begin{bmatrix}
y_1\\
y_2\\
\vdots\\
y_n
\end{bmatrix} = \begin{bmatrix}
\beta_0\\
\beta_0\\
\vdots\\
\beta_0
\end{bmatrix}+\begin{bmatrix}
1 & x_{11} & x_{12} & \dots & x_{1(p-1)}\\
1 & x_{21} & x_{22} & \dots & x_{2(p-1)}\\
\vdots & \vdots & \vdots & \vdots & \vdots \\
1 & x_{n1} & x_{n2} & \dots & x_{n(p-1)}\\
\end{bmatrix}
\begin{bmatrix}
\beta_0\\
\beta_1\\
\vdots\\
\beta_{p-1}
\end{bmatrix}
+ \begin{bmatrix}
\epsilon_1\\
\epsilon_2\\
\vdots\\
\epsilon_{n}
\end{bmatrix}.
\end{align*}
with matrix form
$Y = X\beta + \epsilon$.

\textbf{Further assume columns of $X$ are orthogonal.}\footnote{Note that orthogonality implies $y$ and $x$ are demeaned; that is, $\ip{\bm{1},y} = 0\implies\sum_i y_i = 0, \sum_{i}x_{i,j} = 0, j=1,2,...,p$.} 
Then we have 
$$\hat{\beta}_i = \frac{ \ip{X_i,Y}}{\ip{X_i,X_i}},$$
where $X_i$ is the column $i$ of $X$.
\end{corollary}
\begin{proof}

\end{proof}	


\begin{remark}[orthogonalization for linear regression]
We can use QR decomposition to make the input orthogonal to each other; such idea gives the following successive regression method.
\end{remark}



\begin{method}[successive regression method]
Consider a multiple linear regression problem consisting of column input vectors $X_1,...,X_p$ and output vector $Y$.	
\begin{itemize}
	\item Initialize $Z_0 = 1$.
	\item Regress $X_1$ on $Z_0$, and denote $Z_1$ as the residual vector,
	$$Z_1 = X_1 - \frac{\ip{X_1,Z_0}}{\ip{Z_0,Z_0}}Z_0.$$
	\item Similarly, regress $X_i$ on $Z_0,Z_1,...,Z_{i-1}$ for $i=2,...,p$, and denote $Z_i$ as the residual vector,
	$$Z_i = X_i - \sum_{j=0}^{i-1}\frac{\ip{X_1,Z_j}}{\ip{Z_j,Z_j}}Z_j.$$
	
	\item Since $Z_0,Z_1,...,Z_p$ are orthogonal, then 
	$$\hat{\beta}_i =  \frac{ \ip{Z_i,Y}}{\ip{Z_i,Z_i}}.$$
\end{itemize}
\end{method}


\subsubsection{Frisch-Waugh-Lovell(FWL) theorem and partial regression}


\begin{theorem}[Frisch-Waugh-Lovell theorem]\index{Frisch-Waugh-Lovell theorem}\label{ch:regression-analysis:th:LinearRegressionPartialRegressionFrisch-Waugh-LovellTheorem}
Consider a linear regression formulation with standard assumptions
$$Y = X_1\beta_1 + X_2\beta_2 + \epsilon,$$
where $Y\in \R^N, X_1\in \R^{N\times k_1}, X_2\in \R^{N\times k_2}, \beta_1\in\R^{k_1}, \beta_2\in \R^{k_2}$. We assume $X_1,X_2$ matrices have full column rank. Let $\hat{\beta}_1,\hat{\beta}_2$ be the least square estimators.
It follows that
\begin{itemize}
	\item $$\hat{\beta}_2 = (X_2^TM_1X_2)^{-1}X_2M_1Y,$$
	where $M_1 = I - H_1, H_1 = X_1(X_1^TX_1)^{-1}X_1^T$.
	\item The estimator $\hat{\beta}_2$ can be viewed as the least square estimator from a modified linear regression problem given by
	$$M_1Y = M_1X_2 \beta + M_1\epsilon.$$
	\item $$Var[\hat{\beta}_2] = \sigma^2 (X_2^TM_1X_2)^{-1}. $$
\end{itemize}	
\end{theorem}
\begin{proof}

(2) If we multiply the null orthogonal projector $M_1$ to both sides of 
$$Y = X_1\beta_1 + X_2\beta_2 + \epsilon$$
and get
$$M_1Y = M1X_2 \beta + M_1\epsilon.$$

Alternatively, directly apply least square solution to $M_1Y = M1X_2 \beta + M_1\epsilon$ we also get
\begin{align*}
\hat{\beta}_2 &= (X_2^TM_1^TM_1X_2)^{-1}X_2^TM_1^TM_1Y \\
			  &= (X_2^TM_1X_2)^{-1}X_2^TM_1Y
\end{align*}
where we use $M_1^T = M_1, M^T_1M_1 = M_1.$
(3)
\begin{align*}
Var{\beta}_2 &= (X_2^TM_1X_2)^{-1}X_2^TM_1Var[Y] ((X_2^TM_1X_2)^{-1}X_2^TM_1Y)^T \\
			 &= (X_2^TM_1X_2)^{-1}X_2^TM_1\sigma^2 I ((X_2^TM_1X_2)^{-1}X_2^TM_1Y)^T \\
			 &= \sigma^2 (X_2^TM_1X_2)^{-1}.
\end{align*}
\end{proof}



\subsubsection{Gauss-Markov theorem}



\begin{theorem}[Gauss-Markov theorem, best linear unbiased estimator]\label{ch:regression-analysis:th:BestLinearUnbiasedEstimator}
	Given the statistical model
	$$Y= X\beta + \epsilon, E[\epsilon] = 0, Cov(\epsilon) = \sigma^2 I$$
	with $\beta$ being the model parameter, $y$ being the observations, the \textbf{uniformly minimum variance estimators among all linear unbiased estimators} is given by
	$$\hat{\beta} = (X^TX)^{-1}X^TY.$$
	
	As a summary, we have
	\begin{itemize}
		\item $E[\hat{\beta}] = \beta$.
		\item If $Cov[Y] = \sigma^2I$, then $Cov[\hat{\beta}] = \sigma^2(X^TX)^{-1}$.
	\end{itemize}

Furthermore, if $\epsilon$ is Gaussian noise, i.e., $\epsilon \sim MN(0, \sigma^2 I)$, and $Y,X_1,X_2,...,X_n$ are multivariate Gaussian, then $\hat{\beta}$ is the uniformly minimum variance estimator among all estimators.
\end{theorem}
\begin{proof}
	
	(1) (unbiasedness) $E[\beta] = (X^TX)^{-1}X^T E[Y] = (X^TX)^{-1}X^TX\beta = \beta.$
	(2) Let $\theta' = AY $ be any other unbiased linear estimator, and assume $\theta' = (X^TX)^{-1}X^T + D $ for some matrix $D$. The unbiasedness requires that $$E\theta' = \theta(I + DX) = \theta \Rightarrow DX = 0.$$
	The variance of the estimator is given as
	\begin{align*}
	E[(\theta' - \theta)(\theta' - \theta)^T] &= E[(D + (X^TX)^{-1}X^T)\epsilon\epsilon^T(D+(X^TX)^{-1}X^T)^T]\\
	&=DE[\epsilon\epsilon^T]D^T + (X^TX)^{-1} \sigma^2 I \\
	&=\sigma^2 (DD^T + (X^TX)^{-1}) = \sigma^2 DD^T + Var(\theta) \geq Var(\theta).
	\end{align*} 
	Here the $\geq$ sign is in the semi-positive matrix sense.  
	(3) See \autoref{ch:theory-of-probability:th:BestLinearPredictorForRandomVariables}.
\end{proof}

\begin{remark}[the distribution of noise and its consequence]\hfill
\begin{itemize}
	\item The noise $\epsilon$ does not need to be Gaussian, but required to have zero mean and $\sigma^2$ variance. In this case, we get the best linear estimator among all the linear estimators.
	\item If the noise follows Gaussian distribution and random variable $Y,X_1,X_2,...,X_n$ are multivariate Gaussian, then we get the best estimator among all the estimators.
\end{itemize}	
	
\end{remark}




\subsubsection{The residual and variance estimation}
\begin{definition}[residual of a linear regression]
	The residual of linear regression is defined as
	$$SSE = \sum_{i=1} (y_i - \hat{y}_i)^2 = Y^T(I-H)Y$$
	where $H=X(X^TX)^{-1}X^T$ is the orthogonal projector associated with the linear regression.
\end{definition}

\begin{remark}[derivation]
	The residual is given in matrix form as $$SSE = (Y - HY)^T(Y-HY) = Y^TY - YH^THY - 2Y^THY = Y^TIY - Y^THY$$
	where the orthogonality property of $H$,$H^T = H,H^2 = H$ is used. 
\end{remark}


\begin{theorem}[variance decomposition and property in linear regression]\index{variance decomposition}\label{ch:theory-of-statistics:th:variancedecompositionlinearregression}\cite[76]{montgomery2012introduction}
	In the linear regression(with $p$ coefficients), let $Y$ be the vector of the observations, and let $H$ be the orthogonal projector of rank $p$ and let $\epsilon \sim MN(0,\sigma^2 I)$. Then we have
	\begin{itemize}
		\item $I-\frac{1}{n}J$ is orthogonal projector, and 
		\begin{align*}
		SSTotal &= \sum_{i=1}^{n} (y_i - \mean{y})^2 \\
		&= Y^T(I-\frac{1}{n}J)^T(I-\frac{1}{n}J)Y\\
		&=Y^T(I-\frac{1}{n}J)Y
		\end{align*}
	In particular, if $\beta_i = 0,i=1,2,...,p-1$, then
	$$SSTotal \sim \sigma^2 \chi^2(n-1).$$	
		
		\item $I-H$ is orthogonal projector, and 
		\begin{align*}
		SSE&= \sum_{i=1} (y_i - \hat{y}_i)^2\\
		&=Y^T(I-H)^T(I - H)Y\\
		&=Y^T(I - H)Y \sim \sigma^2 \chi^2(n-p)
		\end{align*}
		and the unbiased variance estimator is given by
		$$\hat{\sigma}^2 = \frac{SSE}{n-p}.$$
		\item $H-\frac{1}{n}J$ is orthogonal projector, and 
		\begin{align*}
		SSRegress &= \sum_{i=1} (\hat{y}_i - \mean{y})^2\\
		&=Y^T(H - \frac{1}{n}J)^T(H - \frac{1}{n}J) Y \\
		&=Y^T(H - \frac{1}{n}J) Y 
		\end{align*}
		In particular, if $\beta_i = 0,i=1,2,...,p-1$, then
		$$SSRegress \sim \sigma^2 \chi^2(p-1).$$	
		\item (independence) If $\beta_i = 0,i=1,2,...,p-1$, then
		$SSE$ and $SSRegress$ are independent.
		\item (partition identity)
		$$SSTotal = SSE + SSRegress$$
		\item $\hat{\beta} = (X^TX)^{-1}X^TY$ and $(I-H)Y$ are mutually independent from each other; moreover, $\hat{\beta}$ and $SSE = Y^T(I-H)Y$ are independent from each other.
	\end{itemize}
\end{theorem}
\begin{proof}
	(1) This is just sample variance when $\beta_i=0,i=1,2,...,p-1$. See \autoref{ch:theory-of-statistics:th:samplevariancedistribution} the distribution of sample variance.
	(2) $H$ has rank $p$(\autoref{ch:theory-of-statistics:th:propertylinearregression}). And $Y^TY = Y^T(I - H)Y + Y^THY$. Use \autoref{ch:theory-of-statistics:th:cochrantheorem}. 
	(3) $\frac{1}{n}J$ has rank 1(\autoref{ch:linearalgebra:th:spectralpropertyorthogonalprojector}). 
	And $Y^TY = Y^T(I - H)Y + Y^T(H -\frac{1}{n}J) Y + Y^T(\frac{1}{n}J) Y$. Use \autoref{ch:theory-of-statistics:th:cochrantheorem}.\\
	(4) We only show $H-\frac{1}{n}J$ is orthogonal projector. Note that $(H-\frac{1}{n}J)(H-\frac{1}{n}J) = H-\frac{2}{n}HJ + \frac{1}{n}J$, and $HJ = J$(since the vector $1$ is one vector in the subspace spanned by the basis of $H$).Then $ H-\frac{2}{n}HJ + \frac{1}{n}J = H-\frac{2}{n}J + \frac{1}{n}J$.
	(5) Note that
	$$Y^T(I-\frac{1}{nJ})Y = Y^T(I-H)Y + Y^T(H-\frac{1}{n}J)Y.$$
	(6) From \autoref{ch:theory-of-statistics:th:independenceOfChiSquareQuadraticForms}, we have
	\begin{align*}
	(I-H)(H-\frac{1}{n}J) &= H - \frac{1}{n}J - H^2 - \frac{1}{n}HJ \\
	&= H - \frac{1}{n}J - H - \frac{1}{n}J \\
	&= 0
	\end{align*}
	where we used the fact that $H\frac{1}{n}J = \frac{1}{n}J$ because the subspace associated with $H$ contains the subspace associated with $\frac{1}{n}J$. 
	(7) Note that under the assumption of $\epsilon\sim MN(0,I)$, $\hat{\beta}$ and $(I-H)Y$ are multivariate normal random vectors. To show independence, we have
	\begin{align*}
	&Cov((X^TX)^{-1}X^TY,(I-X(X^TX)^{-1}X^T)Y)\\
	=& (X^TX)^{-1}X^T Cov(Y,Y)(I-X(X^TX)^{-1}X^T)
	=& (X^TX)^{-1}X^T - (X^TX)^{-1}X^T = 0.
	\end{align*}
	
	Since $\hat{\beta}$ and $(I-H)Y$ are independent, we can directly see
	$\hat{\beta}$ and $Y^T(I-H)(I-H)Y$ are independent.
\end{proof}

\begin{corollary}[residuals and estimation of variance for simple regressions]\label{ch:statistical-models:th:residualAndEstimationVarianceSimpleRegression}
	Let $SSE$ be the residual of a first-order simple regression with $n$ samples, let $\epsilon \sim MN(0,\sigma^2 I)$,  then
	$$ \sum_{i=1} (y_i - \hat{y}_i)^2/\sigma^2 =  \frac{SSE}{\sigma^2} \sim \chi^2(n-2)$$
	from which, we can estimate $\sigma$ via
	$$\hat{\sigma}^2 = \frac{SSE}{n-2},$$
	where we have 
	$$SSE =  \sum_{i=1} (y_i - \hat{y}_i)^2, E[\hat{\sigma}^2] = E[\frac{SSE}{n-2}] = \sigma^2.$$	
\end{corollary}
\begin{proof}
	(1) use \autoref{ch:theory-of-statistics:th:variancedecompositionlinearregression};
	(2) Use the properties of $\chi^2$ (\autoref{ch:theory-of-statistics:th:chi_expectationvariance}).
\end{proof}



\subsubsection{Forecasting analysis}


\begin{lemma}[prediction for simple linear regression]\cite[30]{montgomery2012introduction}\label{ch:statistical-models:th:predictionForSimpleLinearRegression}Assume $\epsilon\sim MN(0,\sigma^2 I)$
\begin{itemize}
	\item Given the regressors value $x_0$, the unbiased mean response is defined to be
	$$\hat{\mu}(y|x_0) = \hat{\beta}_0 + \hat{\beta}_1x_0.$$
	such that $E[\hat{\mu}(y|x_0)] = \beta_0 + \beta_1 x_0.$
	And if $\sigma^2$ is known,
		$$\frac{\hat{\mu}(y|x_0) - E[\hat{\mu}(y|x_0)]}{\sqrt{\sigma^2(1/n + (x_0-\mean{x})^2/S_{XX})}} \sim N(0,1);$$
	If $\sigma^2$ is unknown,	
	$$\frac{\hat{\mu}(y|x_0) - E[\hat{\mu}(y|x_0)]}{\sqrt{\hat{\sigma}^2(1/n + (x_0-\mean{x})^2/S_{XX})}} \sim t(n-2).$$
	where 
	$$\hat{\sigma}^2 = (SSE)/(n-2).$$
	\item The estimation of the response $y_0$ given $x_0$ is
	$$\hat{y}_0 = \hat{\beta}_0 + \hat{\beta}_1x_0,$$
	and 
	$$Var[y_0 - \hat{y}_0] = \sigma^2(1 + \frac{1}{n} + \frac{(x_0-\mean{x})^2}{S_{XX}}).$$
	
	If $\sigma^2$ is known, then
	
	$$\frac{\hat{y}_0 - y_0}{\sqrt{\sigma^2(1 + 1/n + (x_0-\mean{x})^2/S_{XX})}} \sim N(0,1);$$
	
	If $\sigma^2$ is unknown,
	$$\frac{\hat{y}_0 - y_0}{\sqrt{\hat{\sigma}^2(1 + 1/n + (x_0-\mean{x})^2/S_{XX})}} \sim t(n-2).$$
		where 
	$$\hat{\sigma}^2 = (SSE)/(n-2).$$
\end{itemize}	
\end{lemma}
\begin{proof}
(1)(a)Using the results in \autoref{ch:statistical-models:th:residualAndEstimationVarianceSimpleRegression}, we have
\begin{align*}
Var[\hat{\mu}(y|x_0)] &= Var[ \hat{\beta}_0 + \hat{\beta}_1x_0] \\
&=Var[\hat{\beta}_0] + Var[\hat{\beta}_1]x_0^2 + 2x_0Cov(\hat{\beta}_0,\hat{\beta}_1) \\
&=\sigma^2(\frac{1}{n}+ \frac{\mean{x}^2}{S_{XX}})+\frac{\sigma^2x_0^2}{S_{XX}}-2\frac{\sigma^2x_0\mean{x}}{S_{XX}}\\
&=\sigma^2(\frac{1}{n} + \frac{(x_0 - \mean{x})^2}{S_{XX}}).
\end{align*}
(b) Note that
$$\frac{\hat{\sigma}^2}{\sigma^2} \sim \frac{\chi^2(n-2)}{n-2},$$
therefore
$$	\frac{\hat{\mu}(y|x_0) - E[\hat{\mu}(y|x_0)]}{\sqrt{\sigma^2(1/n + (x_0-\mean{x})^2/S_{XX})}}/\frac{\hat{\sigma}^2}{\sigma^2} \sim t(n-2). $$
where we also the independence between $\hat{\beta}$ and $\hat{\sigma}^2$ in \autoref{ch:theory-of-statistics:th:variancedecompositionlinearregression}.
(2) 
Note that $$y_0 - \hat{y}_0 = \beta_0+\beta_1x_0 + \epsilon  - \hat{\beta}_0+\hat{\beta}_1x_0.$$
Then
$$Var[y_0 - \hat{y}_0] = \sigma^2(1 +ã€€\frac{1}{n} + \frac{(x_0 - \mean{x})^2}{S_{XX}}).$$
The rest is similar to (1).
\end{proof}


\begin{lemma}[prediction for multiple linear regression]\cite[94,99]{montgomery2012introduction}
Assume $\epsilon\sim MN(0,\sigma^2 I)$
\begin{itemize}
	\item Given the regressors value $x_0$, the unbiased mean response is defined to be
$$\hat{y}_0=x_0^T\hat{\beta}$$
and
$$Var[\hat{y}_0] = \sigma^2x_0^T(X^TX)^{-1}x_0$$
	If $\sigma^2$ is known, then
$$\frac{\hat{y}_0 - E[y|x_0]}{\sqrt{\sigma^2 x_0^T(X^TX)^{-1}x_0}}\sim N(0,1);$$
	If $\sigma^2$ is unknown,
$$\frac{\hat{y}_0 - E[y|x_0]}{\sqrt{\hat{\sigma}^2 x_0^T(X^TX)^{-1}x_0}}\sim t(n-p),$$
where
	$$\hat{\sigma}^2 = \frac{\sum_i (y_i - \hat{y}_i)}{n-p}.$$

\item The estimation of the response $y_0$ given $x_0$ is
$$\hat{y}_0 = \hat{\beta}_0 + \hat{\beta}_1x_0,$$
and 
	If $\sigma^2$ is known, then
$$\frac{\hat{y}_0 -y_0}{\sqrt{\sigma (1+x_0^T(X^TX)^{-1}x_0)}}\sim N(0,1);$$
	If $\sigma^2$ is unknown,
$$\frac{\hat{y}_0 - y_0}{\sqrt{\hat{\sigma}^2 (1 + x_0^T(X^TX)^{-1}x_0)}}\sim t(n-p).$$
\end{itemize}
\end{lemma}
\begin{proof}
Note that	
$$Var[\hat{y}_0] = Var[x_0^T\hat{\beta}] = x_0^T Cov(\hat{\beta})x_0 =  \sigma^2x_0^T(X^TX)^{-1}x_0.$$
The rest is similar to \autoref{ch:statistical-models:th:predictionForSimpleLinearRegression}.
\end{proof}


\subsection{Maximum likelihood method}
\begin{theorem}[maximum likelihood estimation of parameters in multiple linear regression]\label{ch:statistical-models:th:maximumLikelihoodMethodLinearRegression}
The likelihood function for the multiple linear regression problem is given by	
	$$L(\beta, \sigma^2) = \prod_{i=1}^{N} N(y_i;x_i,\beta, \sigma^2) = (2\pi\sigma^2)^{-N/2}\exp(-\frac{1}{2\sigma^2}(Y-X\beta)^T(Y-X\beta)),$$
and the logarithm of this function is
$$l(\beta, \sigma^2) = -\frac{N}{2}\ln(2\pi)-\frac{N}{2}\ln(\sigma^2) -\frac{1}{2\sigma^2}(Y-X\beta)^T(Y-X\beta).$$

We have the following derivatives
\begin{itemize}
	\item 
	$$\frac{\Pa l}{\Pa \beta} = \frac{1}{\sigma^2}(Y-X\beta)^TX.$$
	\item 
	$$\frac{\Pa l}{\Pa \sigma^2} = -\frac{N}{2\sigma^2} + \frac{1}{2\sigma^4}(Y-X\beta)^T(Y-X\beta).$$
	\item 
	$$\frac{\Pa^2 l}{\Pa \beta\Pa \beta^T} = -\frac{1}{\sigma^2}X^TX.$$
	\item 
	$$\frac{\Pa^2 l}{\Pa \sigma^2\Pa \sigma^2} = \frac{N}{2\sigma^4} -\frac{1}{\sigma^6}(Y-X\beta)^T(Y-X\beta).$$
	\item 
	$$\frac{\Pa^2 l}{\Pa \beta\Pa \sigma^2} = -\frac{1}{\sigma^4}X^T(Y-X\beta).$$
\end{itemize}	

The first order condition gives the following MLE
\begin{itemize}
	\item $$\hat{\beta} = (X^TX)^{-1}X^TY,$$
	\item $$\hat{\sigma}^2 = \frac{1}{N}(Y-X\hat{\beta})^T(Y-X\hat{\beta})$$
\end{itemize}
\end{theorem}
\begin{proof}
(1) Note that 
$$\frac{\Pa l}{\Pa \beta} = 0\implies (Y-X\beta)^TX = 0\implies X^TY=X^TX\beta.$$
(2)$$\frac{\Pa l}{\Pa \sigma^2} = 0\implies -\frac{N}{2\sigma^2} + \frac{1}{2\sigma^4}(Y-X\beta)^T(Y-X\beta) = 0\implies \hat{\sigma}^2 = \frac{1}{N}(Y-X\hat{\beta})^T(Y-X\hat{\beta}).$$
\end{proof}


\begin{corollary}[maximum likelihood estimation of parameters in simple linear regression]
The likelihood function for the multiple linear regression problem is given by	
$$L(\beta_0,\beta_1, \sigma^2) = \prod_{i=1}^{N} N(y_i;x_i,\beta_0,\beta_1, \sigma^2) = (2\pi\sigma^2)^{-N/2}\exp(-\frac{1}{2\sigma^2}\sum_{i=1}^{N}(y_i-\beta_0-\beta_1x_i)^2),$$
and the logarithm of this function is
$$l(\beta_0,\beta_1, \sigma^2) = -\frac{N}{2}\ln(2\pi)-\frac{N}{2}\ln(\sigma^2) -\frac{1}{2\sigma^2}\sum_{i=1}^{N}(y_i-\beta_0-\beta_1x_i)^2.$$

We have the following derivatives
\begin{itemize}
	\item 
	$$\frac{\Pa l}{\Pa \beta_1} = -2\sum_{i=1}^{N}(y_i-\beta_0-\beta_1x_i)x_i.$$
	\item
	$$\frac{\Pa l}{\Pa \beta_0} = -2\sum_{i=1}^{N}(y_i-\beta_0-\beta_1x_i).$$
	\item 
	$$\frac{\Pa l}{\Pa \sigma^2} = -\frac{N}{2\sigma^2} + \frac{1}{2\sigma^4}\sum_{i=1}^{N}(y_i-\beta_0-\beta_1x_i)^2 $$
\end{itemize}	

The first order condition gives the following MLE
\begin{itemize}
	\item $$\hat{\beta}_1 = \frac{\sum_{i=1}^{N} (x_i-\mean{x})(y_i-\mean{y})}{\sum_{i=1}^{N}(x_i-\mean{x})},$$
	\item $$\hat{\beta}_0 = \mean{y} - \hat{\beta}_1\mean{x},$$
	\item $$\hat{\sigma}^2 = \frac{1}{N}\sum_{i=1}^{N}(y_i-(\hat{\beta}_0+\hat{\beta}_1x_i))^2.$$
\end{itemize}	
\end{corollary}

\begin{remark}[biased variance estimator]
Note that the unbiased variance estimator is given by(\autoref{ch:theory-of-statistics:th:variancedecompositionlinearregression})
$$\hat{\sigma}^2 = \frac{1}{N-p}(Y-X\hat{\beta})^T(Y-X\hat{\beta})$$
therefore the MLE variance estimator is biased.
\end{remark}

\subsection{Asymptotic properties of least square solutions}

\begin{remark}[Motivation]
In a standard linear regression with assumptions A1-A5 in \autoref{ch:regression-analysis:assumption:LinearRegressionStandardAssumption} hold and finite samples, the least square estimator is the best linear unibiased estimator based on the Gauss-Markov theorem (\autoref{ch:regression-analysis:th:BestLinearUnbiasedEstimator}). If we further assume the error/noise term has normal distribution (A6 in \autoref{ch:regression-analysis:assumption:LinearRegressionStandardAssumption}), then we derive various hypothesis test statistics, such as t-test and F-test. 

The full set of assumptions in \autoref{ch:regression-analysis:assumption:LinearRegressionStandardAssumption} can be difficult to satisfy in the real world; however, in the large sample limit, we can loose some assumptions (e.g., the normal distribution assumption) can still achieve nice properties (e.g., deriving t and F statistics) using central limit theorem.
\end{remark}


\subsubsection{Asymptotic properties under standard assumptions}


\begin{lemma}[]
Consider a multiple linear regression under standard assumption (\autoref{ch:regression-analysis:assumption:LinearRegressionStandardAssumption}) and its least square estimator (\autoref{ch:statistical-models:th:leastSquareSolution}) given by
$$\hat{\beta} = (X^TX)^{-1}X^TY.$$

Assume the sample of the regressors $X_i$ satisfying $E[X_iX_i^T] = Q$. It follows that  
\begin{itemize}
	\item 
	$$Var[\hat{\beta}] = \sigma^2 E[X^TX]^{-1} = \sigma^2 \frac{Q}{n} \to 0, ~as~N\to \infty.$$
	\item $$\plim \hat{\beta} = \beta;$$
	that is
\end{itemize}
\end{lemma}
\begin{proof}
(1)
Note that
$$Var[\hat{\beta}] = \sigma^2 (\sum_{i=1}^N X_iX_i^T)^{-1} = \sigma^2(NQ)^{-1}.$$
(2) Note that $\hat{\beta}$ is an unbiased estimator(\autoref{ch:statistical-models:th:leastSquareSolution}). Then use convergence in mean square (from (1)) implies convergence in probability.	
\end{proof}


\subsubsection{Asymptotic properties with correlated noise and regressors }



\subsection{Statistics of disturbance}
\subsubsection{Fundamentals}
\begin{definition}[residual vector]\cite[194]{theil1971principles}
The least square residual vector is defined as
$$e = Y - X\beta = MY,$$
where $M = I-H, H = X(X^TX)^{-1}X^T.$	
\end{definition}

\begin{lemma}[Basic property of residual vector]
Let $e = Y-X\beta$ be the residual vector. 
We have
\begin{itemize}
	\item $$X^Te = 0;$$
	That is, residual vector is orthogonal to the subspace of observations.
	\item $$\hat{Y}^Te = 0;$$
	That is, residual vector is orthogonal to the projection of $Y$.
\end{itemize}	
\end{lemma}
\begin{proof}
(1)	
$$X^Te = X^T(I_H)Y = (X^T - X^TH)Y = (X^T - X^T)Y.$$
(2)
$$\hat{Y}^Te  = (HY)^T(I-H)Y = Y^T(H-H^2)Y = 0.$$
\end{proof}




\subsubsection{Normality test}



\subsection{Partial and multiple correlation}

\subsubsection{Multiple correlation coefficient, $R^2$}

\begin{definition}[coefficient of determination, coefficient of multiple correlation]\index{coefficient of determination}\index{coefficient of multiple correlation}
	The \textbf{coefficient of determination} (or \textbf{coefficient of multiple correlation}) $R^2$ is defined as 
	$$R^2 = \frac{SSRegression}{SST} = \frac{\sum_{i=1}^n (\hat{y}_i - \mean{y})}{\sum_{i=1}^n (y_i - \mean{y})} = \frac{SST - SSE}{SST} = 1 -\frac{SSE}{SST},$$
where	
\begin{align*}
SST &= \sum_{i=1}^{n} (y_i - \mean{y})^2 \\
SSRegress &= \sum_{i=1} (\hat{y}_i - \mean{y})^2\\
SSE&= \sum_{i=1} (y_i - \hat{y}_i)^2
\end{align*}
and we use $SST = SSRegression + SSE$ from \autoref{ch:theory-of-statistics:th:variancedecompositionlinearregression}. 
\end{definition}


\begin{lemma}[properties of coefficient of multiple correlation]\cite[164]{theil1971principles}\label{ch:regression-analysis:th:PropertiesLinearRegressionCoefficientMultipleCorrelation}
The \textbf{coefficient of multiple correlation} of the linear regression is given by
$$R^2 = \frac{\hat{\beta}^T X^TX \hat{\beta}}{Y^TY} =\frac{Y^THY}{Y^TY};$$
Or equivalently,  $$R^2 = 1-\frac{\hat{e}^T\hat{e}}{Y^TY}.$$
where $\hat{e}$ is the residual given by
$$\hat{e} = Y - X^T\hat{\beta} = (I - H)Y,H = X(X^TX)^{-1}X^T.$$	
\end{lemma}
\begin{proof}
\begin{align*}
R^2 &= \frac{\hat{\beta}^T X^TX \hat{\beta}}{Y^TY} \\
	&= \frac{Y^THHY}{Y^TY} \\
	&= \frac{Y^T(I - (I-H))Y}{Y^TY} \\
	&=  1 - \frac{\hat{e}^T\hat{e}}{Y^TY}
\end{align*}	
where $X\hat{\beta} = X(X^TX)^{-1}X^TY = HY.$
\end{proof}

\begin{remark}[interpretations and basic properties]\hfill
	\begin{itemize}
		\item $R^2$ is  the proportion of variation in $Y$ explained by the predicator $X$. Obviously, 
		$$0\leq R^2 \leq 1.$$
		\begin{itemize}
			\item $R^2 = 0$ is the case of $\beta_1=0$ in simple regression.
			\item $R^2 = 1$ is all variations is explained by $X$.
			\item Large $R^2$ implies small residual, or good fit.
			\item $R^2$ can be increase by increasing the number of predictors. 
		\end{itemize}
	\end{itemize}
\end{remark}


\subsubsection{Partial correlation coefficient}
\begin{definition}[partial correlation coefficients]\cite[173]{theil1971principles}
The partial correlation coefficient $r_h$ for predictor variable $X_h$ is defined by	
	$$r_h = 1 - \frac{1-R^2}{1-R^2_h},$$
	where $R^2_h$ is the coefficent of multiple correlation without using $X_h$.	
\end{definition}


\begin{remark}\hfill
\begin{itemize}
	\item $r_h = 0$ means that $R_h^2 = R^2$ and $X_h$ is not useful.
	\item $r_h = 1 \implies R_h^2 = 1$, and $X_h$ alone can fully explain $Y$.
	\item Larger $r_h$, more important $X_h$ in explaining $Y$.
\end{itemize}	
\end{remark}



\begin{lemma}[calculating partial correlation coefficients]\cite[173]{theil1971principles}
Consider a standard multiple linear problem. The partial correlation coefficient associated with first regressor $X_1$ is given by
\begin{itemize}
	\item 
	$$R_1^2 = 1 - \frac{Y^TNY}{Y^TY},$$
	where  $X$ is decomposed by $X = [X_1 ~ W], N = I - W(W^TW)^{-1}W^T.$
	\item $$r_1 = \frac{Y^TNX_1}{\sqrt{(Y^TNy)(X_1^TNX_1)}}.$$
\end{itemize}	
\end{lemma}
\begin{proof}
Note that 
$$\frac{1-R^2}{1-R_1^2} = \frac{\frac{Y^TMY}{Y^TY}}{\frac{Y^TNY}{Y^TY}} = \frac{Y^TMY}{Y^TNY} = \frac{Y^T(N - \frac{(NX_1)(NX_1)^T}{X_1NX_1})Y}{Y^TNY} = 1 -\frac{(Y^TNX_1)^2}{(Y^TNy)(X_1^TNX_1)} = 1-r_1^2 $$	
where we use the fact that
$M = X(X^TX)^{-1}X^T = N - \frac{(NX_1)(NX_1)^T}{X_1NX_1}$ from \autoref{ch:linearalgebra:th:lowRankUpdateOfOrthogonalProjector}.
\end{proof}

\begin{remark}[interpretation]\hfill
\begin{itemize}
	\item From the expression of $r_1$, it is clear that if $X_1$ lies in the space spanned by the columns of $W$ then $r_1 = 0$, indicating that $X_1$ is not useful.	
	\item On the other hand, if $X_1$ has some component lies in the space spanned by the columns of $N$ and $X_1$ and $Y$ has some level of correlation, then $r_1 > 0$.
\end{itemize}

\end{remark}


\subsection{Handling various types of regressors}

\subsubsection{When regressors are random}


\begin{note}\cite[49]{montgomery2012introduction}
Suppose that $x$ and $y$ are jointly distributed random variables but the form of this joint distribution is unknown.

Then all of our previous regression results hold if the following conditions are satisfied.
\begin{itemize}
	\item The conditional distribution of $y$ given $x$ is normal with conditional mean $\beta_0 + \beta_1 x$ and conditional variance $\sigma^2$.
	\item The $x$ are independent random variables whose probability distribution does not involve $\beta_0,\beta_1,$ and $\sigma^2$.
\end{itemize}	
\end{note}


\begin{theorem}[multivariate Gaussian distribution and multiple linear regression]
Suppose $(x_1,x_2,...,x_p)$ and $y$ are jointly distributed	according to multivariate Gaussian distribution with parameter $(\mu,\Sigma)$.
Then 
\begin{itemize}
	\item the conditional distribution of $y$ given $(x_1,x_2,...,x_n)$ is given by	
		$$f(y|x_1,x_2,...,x_n)= \frac{1}{\sqrt{2\pi(\Sigma_{yy} - \Sigma_{yx}\Sigma_{xx}^{-1}\Sigma_{xy})}}\exp(-\frac{1}{2}(\frac{(y - \mu_y - \Sigma_{yx}\Sigma_{xx}^{-1}(x-\mu_x))^2}{(\Sigma_{yy} - \Sigma_{yx}\Sigma_{xx}^{-1}\Sigma_{xy})}),$$
	where we decompose $$\mu = [\mu_y^T ,\mu_x^T]^T,\Sigma = 
	\begin{pmatrix}
	\Sigma_{yy} & \Sigma_{yx} \\
	\Sigma_{xy} & \Sigma_{xx}
	\end{pmatrix}
	,$$ 
	with $\mu_1 \in \R^k,\mu_2\in \R^{n-k}$.
	\item The conditional mean and conditional variance are given by
	$$E[y|x] = \mu_y + \Sigma_{yx}\Sigma_{xx}^{-1}(x-\mu_x) = \mu_y + \beta(x - \mu_x),\beta = \Sigma_{yx}\Sigma_{xx}^{-1}$$
	and, $$Var[y|x]=\Sigma_{yy} - \Sigma_{yx}\Sigma_{xx}^{-1}\Sigma_{xy}.$$	
\end{itemize}
\end{theorem}
\begin{proof}
See 	
\autoref{ch:theory-of-statistics:th:multivariatenormalconditionaldistribution}
\end{proof}

\begin{remark}[connection to the best linear predictor]
The result is closely related best linear predictor(\autoref{ch:theory-of-probability:th:BestLinearPredictorForRandomVariables}). 	
\end{remark}


\begin{corollary}[bivariate Gaussian distribution and linear regression]\cite[49]{montgomery2012introduction}
	Suppose $x$ and $y$ are jointly distributed	according to bivariate distribution with parameter $(\mu_1,\mu_2,\sigma^2_1,\sigma_2^2,\rho)$ such that
	$$f(x,y) = \frac{1}{2\pi \sigma_1\sigma_2 \sqrt{1-\rho^2}}\exp(-\frac{1}{2(1-\rho^2)}[(\frac{y-\mu_1}{\sigma_1})^2 + (\frac{y-\mu_1}{\sigma_1})^2 - 2\rho(\frac{y-\mu_1}{\sigma_1})(\frac{x-\mu_2}{\sigma_2})]). $$
	
	Then 
	\begin{itemize}
		\item the conditional distribution of $y$ given $x$ is	
		$$f(y|x) = \frac{1}{\sqrt{2\pi}\sigma_{1,2}}\exp(-\frac{1}{2}(\frac{(y-\beta_0-\beta_1x)^2}{\sigma_{1,2}^2})$$
		where
		$$\beta_0=\mu_1 - \mu_2\rho\frac{\sigma_1}{\sigma_2},\beta_1 = \rho\frac{\sigma_1}{\sigma_2}, \sigma_{1,2}^2=\sigma_1^2(1-\rho^2),\rho = \frac{\sigma_{1,2}}{\sigma_1\sigma_2}.$$
		\item The conditional mean and conditional variance are given by
		$$E[y|x] = \beta_0 + \beta_1x, Var[y|x]=\sigma_{1,2}^2.$$	
	\end{itemize}
\end{corollary}


\subsubsection{When regressors are categorical types}

\begin{note}[regressor is taking binary value]
	Suppose we have regressor characterizing whether tomorrow is rainy or not. Then we can design a regressor with the following rule
	
	$$x_i = \begin{cases*}
	1, rainy\\
	0, not~rainy
	\end{cases*}$$	
\end{note}


\begin{note}[regressor is taking $K,K\geq 3$ discrete value]
\begin{itemize}
	\item 	Suppose we have regressor taking values from 1 to 6. Then we can design five regressor with the following rule
	$$x_i = \begin{cases*}
	1, if~value~i\\
	0, not~rainy
	\end{cases*}, i=1,2,...,5.$$
	If $x_i=0,\forall i=1,2,...,5$, then the regressor value is 6.
	\item Note that we cannot design six regressors since they are not linearly independent; that is, when five regressor value is known, the six is known.
\end{itemize}	
	
\end{note}





\section{Linear regression analysis: diagnostics \& solutions}



\subsection{Multi-collinearity}
\subsubsection{Detection and characterization}
\begin{definition}[multi-collinearity, collinearity]\index{collinearity}
	Multi-collinearity, or collinearity is the phenomenon that some predictors are linear combinations of the other predictors, or some predictors are highly correlated. As a direct consequence, $X^TX$ is singular or has a large conditional number.
\end{definition}


\begin{remark}[detrimental effects of collinearity]\hfill
	\begin{itemize}
		\item A unique $\hat{\beta}$ cannot be found.
		\item In the case of near singularity, $Var[\hat{\beta}] = \sigma^2(X^TX)^{-1}$ is huge.
		\item The estimation $SSE = Y^T(I-H)Y$ will increase as $H$ becomes rank deficient.
	\end{itemize}
\end{remark}


\begin{remark}[connection with machine learning training procedure]\hfill
	\begin{itemize}
		\item In machine learning, if the training examples are highly linear dependent, then the variance of the model is big, leading to large test error.
		\item Correlated examples do not provide too much new information, and therefore training examples should be uncorrelated examples. 
	\end{itemize}
\end{remark}


\subsubsection{Variance inflation factor method}

\begin{definition}[variance inflation factor]\index{variance inflation factor}\cite[335]{montgomery2012introduction}
	In multivariable linear regression, the \textbf{variance inflation factor}(VIF) associated with predictor $X_i$ is defined by
	$$VIF_i = \frac{1}{1-R_{i}^2}$$
	where $R^2_{i}$ is the coefficient of determination from regression of $X_i$ on the rest of other predicators.
\end{definition}


\begin{lemma}[basic properties and relations]
Let $R^2_1$ be the coefficient of determination from regression of $X_1$ on the rest of other predicators.	
It follows that
\begin{itemize}
	\item $$R_1^2 = \frac{X_1^TH_{-1}X_1}{X_1^TX_1}.$$
	\item $$Var[\hat{\beta}_1] = \frac{\sigma^2}{X_1^TX_1}VIF_1 = \frac{\sigma^2}{X_1^TX_1}\frac{1}{1-R_1^2}.$$
\end{itemize}
\end{lemma}
\begin{proof}
(1) From \autoref{ch:regression-analysis:th:PropertiesLinearRegressionCoefficientMultipleCorrelation}.
(2) From \autoref{ch:statistical-models:th:leastSquareSolution}, we have $$\hat{\beta}_1 = \frac{X_1^T(I - H_{-1})Y}{X_1^T(I-H_{-1})X_1},$$
where $H_{-1} = X_{-1}(X_{-1}^TX_{-1})^{-1} X_{-1}^T$, $X_{-1}$ is the matrix without column $i$.
Then
\begin{align*}
Var[\hat{\beta}_1] &= \frac{X_1^T(I - H_{-1})YY^T(I - H_{-1})X_1}{(X_1^T(I-H_{-1})X_1)^2} \\
&= \frac{X_1^T(I - H_{-1})X_1 \sigma^2}{(X_1^T(I-H_{-1})X_1)^2} \\
&=\frac{\sigma^2}{X_1^T(I-H_{-1})X_1} \\
&=\frac{\sigma^2}{X_1^TX_1 - X_1^TX_1R_1^2} \\
&=\frac{\sigma^2}{X_1^TX_1(1 - R_1^2)} \\
&=\frac{\sigma^2}{X_1^TX_1}VIF_1
\end{align*}
\end{proof}


\begin{remark}[interpretation]\hfill
	\begin{itemize}
		\item VIF provides a quantitative method to examine how the linear dependence among predicators can affect the estimator variance.  For example, $VIF_i$ is proportional to the variance of $\hat{\beta}_1$.
		\item $VIF > 5 $ is considered bad.
	\end{itemize}
	
\end{remark}



\begin{example}\cite[326]{montgomery2012introduction}
$$y = \beta_1 x_1 + \beta_2 x_2 + \epsilon$$
and the least-square normal equation are
$$(X^TX)\hat{\beta} = X^Ty,$$
or equivalently
$$\begin{bmatrix}
1 & r_{12} \\
r_{12} & 1
\end{bmatrix}\begin{bmatrix}
\hat{\beta}_1\\
\hat{\beta}_2
\end{bmatrix} = \begin{bmatrix}
r_{1y}\\
r_{2y}
\end{bmatrix}$$
where $r_{12}$ is the correlation between $x_1$ and $x_2$, and $r_{1y}$ is the correlation between $x_1$ and $y$. 

Then the inverse $(X^TX)$ is
$$(X^TX)^{-1}=\begin{bmatrix}
\frac{1}{1-r_{12}^2} & \frac{-r_{12}}{1-r_{12}^2}  \\
\frac{-r_{12}}{1-r_{12}^2} & \frac{1}{1-r_{12}^2}  
\end{bmatrix},$$
and the estimates of the regression coefficients are
$$\hat{\beta}_1 = \frac{r_{1y} - r_{12}r_{2y}}{1 - r_{12}^2},\hat{\beta}_2 = \frac{r_{2y} - r_{12}r_{1y}}{1 - r_{12}^2}.$$
The variance of estimator $\hat{\beta}_1$ and $\hat{\beta}_2$ are
$$Var[\hat{\beta}_i] = \sigma^2\frac{1}{1-r_{12}^2}.$$

If there is a strong multicollinearity between $x_1$ and $x_2$, then the correlation coefficient $r_12$ will be close to 1.	

As the consequence, 
\begin{itemize}
	\item the variance of estimator $\hat{\beta}$ is large; in other words, the result will be unstable, different sample input will give very different $\hat{\beta}$.
	\item the magnitude of estimator $\hat{\beta}$ also tends to be large. 
\end{itemize}
\end{example}

\subsubsection{Principal component linear regression}


\begin{lemma}[principal component linear regression]\cite[355]{montgomery2012introduction}
	$$y = X\beta + \epsilon$$
is equivalent to transformed model
$$y = Z\alpha + \epsilon,$$
where $Z = XU, \alpha = U^T\beta, U^TX^TXU = Z^TZ = \Lambda$, and $X^TX$ has the eigendecomposition given by
$$X^TX = U\Lambda U^T.$$	
	
In the transformed model, we have the least square estimator $\hat{\alpha}$ given by
$$\hat{\alpha}=(Z^TZ)^{-1}Z^Ty = \lambda^{-1}Z^Ty,$$
and the covariance matrix of $\hat{\alpha}$ is
$$Var[\hat{\alpha}] = \sigma^2(Z^TZ)^{-1} = \sigma^2 \Lambda^{-1}.$$
	
\end{lemma}
\begin{proof}
Note that from eigendecomposition we have
$$X^TX = U\Lambda U^T \implies U^TX^TXU = U^TU\Lambda U^TU = \Lambda.$$	
\end{proof}

\begin{remark}[principal component regression for detecting multi-collinearity]

\end{remark}



\begin{lemma}[principal component regression to handle multicollinearity]\cite[355]{montgomery2012introduction}
	$$y = X\beta + \epsilon$$
	is equivalent to

	
	Suppose take the $q$ eigenvectors with the largest eigenvalues and form matrix	$$U_q = [u_1,u_2,...,u_q].$$
	
	And in the projected model
		$$y = Z_q\alpha_q + \epsilon,$$
	where $Z_q = XU_q, \alpha = U^T_q\beta, U^T_qX^TXU_ = Z^T_qZ_q = \Lambda_q$, and $X^TX$ has the eigendecomposition given by
	$$X^TX = U\Lambda U^T.$$	
	
	
	$$\hat{\alpha}_q = \begin{bmatrix}
	\hat{\alpha}_1\\
	\hat{\alpha}_2\\
	\vdots \\
	\hat{\alpha}_q\\
	\vdots
	\end{bmatrix},$$
	
	and $\hat{\beta}_q = U_q \hat{\alpha}_q$
\end{lemma}
\begin{proof}
	
\end{proof}


\subsection{Heteroscedasticity and weighted least square}
\begin{definition}[multiple linear regression model with structural error]
	The multiple linear regression model \textbf{assumes} that a random variable $Y$ has a linear dependency on a non-random vector $X \in \R$ given as
	$$Y = \beta_0 + \beta_1 X_1 +\beta_2 X_2 + ... +\beta_{p-1} X_1 + \epsilon$$
	where $\beta_0,\beta_1, ...$ are unknown model parameters, and $\epsilon$ is a random variable. 
	Given the observed sample pairs $(x_1,y_1),(x_2,y_2),..., (x_n,y_n), x\in \R^{p-1}, y\in \R$ as $y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + ... + \epsilon_i$ and we \textbf{further make the following assumptions on $\epsilon$} as
	\begin{itemize}
		\item $E[\epsilon_i] = 0,\forall i$
		\item $cov(\epsilon_i,\epsilon_j) = \sigma^2 \Sigma,\forall i,j$
	\end{itemize} 	
\end{definition}

\begin{remark}[compared with standard linear regression model]
The covariance matrix the error is $\sigma^2 \Sigma$ instead of $\sigma^2I$.	
\end{remark}

\begin{note}[properties of ordinary least square estimator under structural error]
Note that the ordinary least square estimator is given by
$$\hat{\beta} = (X^TX)^{-1}X^TY,$$
which is still unbiased, i.e., $$E[\hat{\beta}] = E[(X^TX)^{-1}X^TX\beta] = \beta.$$

However, it is no longer the minimum variance estimator because
	
\end{note}



\begin{theorem}[generalized least square solution: general case ]\index{generalized least square}\label{ch:statistical-models:th:GeneralizedLeastSquareSolution}
	The multiple linear regression with n samples can be written as
	\begin{align*}
	\begin{bmatrix}
	y_1\\
	y_2\\
	\vdots\\
	y_n
	\end{bmatrix} = \begin{bmatrix}
	1 & x_{11} & x_{12} & \dots & x_{1(p-1)}\\
	1 & x_{21} & x_{22} & \dots & x_{2(p-1)}\\
	\vdots & \vdots & \vdots & \vdots & \vdots \\
	1 & x_{n1} & x_{n2} & \dots & x_{n(p-1)}\\
	\end{bmatrix}
	\begin{bmatrix}
	\beta_0\\
	\beta_1\\
	\vdots\\
	\beta_{p-1}
	\end{bmatrix}
	+ \begin{bmatrix}
	\epsilon_0\\
	\epsilon_1\\
	\vdots\\
	\epsilon_{p-1}
	\end{bmatrix}
	\end{align*}
	with matrix form
	$$Y = X\beta + \epsilon$$
	The \textbf{unique minimizer} to the problem 
	$$\min_\beta J(\beta) = (Y-X\beta)^T\Sigma^{-1}(Y - X\beta)$$
	is given as
	$$\hat{\beta} = (X^T\Sigma^{-1}X)^{-1}X^T\Sigma^{-1}Y,$$
	in particular, $$\hat{\beta}_0 = \mean{y} - \sum_{i=1}^p \hat{\beta}_i \mean{x}$$
	$$\hat{y}_i - \mean{y} =\sum_{i=1}^p \hat{\beta}_i (x_i-\mean{x}) $$
	Moreover, we have
	\begin{itemize}
		\item $E[\hat{\beta}] = \beta$.
		\item If $Cov[Y] = \sigma^2\Sigma$, then $Cov[\hat{\beta}] = \sigma^2(X^T\Sigma X)^{-1}$.($\hat{\beta}$ is not necessarily normal)
	\end{itemize}
\end{theorem}
\begin{proof}
	(1) $J(\beta) = Y^T\Sigma^{-1}Y - \beta^TX^T\Sigma^{-1}X\beta - 2\beta^TX^T\Sigma^{-1}Y$. Set $dJ/d\beta = 0$, we can get the result.
	(2) (unbiasedness) $E[\beta] = (X^T\Sigma^{-1}X)^{-1}X^T\Sigma^{-1} E[Y] = (X^T\Sigma^{-1}X)^{-1}X^T\Sigma^{-1}X\beta = \beta.$
	(3) (variance) $Cov[\beta] = (X^T\Sigma^{-1}X)^{-1}X^T\Sigma^{-1}Cov[Y]((X^T\Sigma^{-1}X)^{-1}X^T\Sigma^{-1})^T = \sigma^2(X^T\Sigma^{-1}X)^{-1}$
\end{proof}


\begin{note}[connection to ordinary least square]
	Let $\Sigma^{-1}$ be symmetric positive definite, and let $\Sigma^{-1} = U\Lambda U^T$. Let $S^{-1} = U\sqrt{\Lambda}, S^{-1}S^{-T}=\Sigma^{-1}$, then the transformed model
	\begin{align*}
	S^{-1}Y &= S^{-1}X\beta + S^{-1}\epsilon \\
	Y^* &= X^*\beta + \epsilon^*
	\end{align*}
	becomes the canonical linear regression model such that
	$$E[\epsilon^*] = E[S^{-1}\epsilon] = S^{-1}E[\epsilon] = 0$$
	and
	$$Var[\epsilon^*] = P^{-1}Var[\epsilon]P^{-T} = S^{-1}\sigma^2 \Sigma S^{-T} = \sigma^2 I.$$
	
	Then we can apply the ordinary least square to the transformed data $Y^*,X^*$.
\end{note}


\begin{remark}[weighted least square as a special generalized least square]\index{weighted least square}
	Weighted least square is the special case that $\Sigma^{-1}=diag(w_1,...,w_p)$.
\end{remark}



\begin{algorithm}[H]
	\SetAlgoLined
	\KwIn{Data set consists of $X,Y$}
	Start with initial weight $w_i\geq 0,i=1,...,p$ and the error model, for example $var(\epsilon_i) = \gamma_0 + \gamma_1x_1$.\\
	Use generalized least square to estimate $\beta$.\\
	Use the residuals to estimate $\gamma$, by regressing $x$ on the residual $\hat{\epsilon}^2$.\\
	Re-compute the weights and go to step 2.\\
	\KwOut{The coefficients $\beta$}
	\caption{EM algorithm for least square with nonconstant variance}
\end{algorithm}

\subsection{Residual distribution test}

\subsubsection{Jarque-Bera test}\index{Jarque-Bera test}

\begin{definition}[Jarqueâ€“Bera test]
Jarqueâ€“Bera test is a goodness-of-fit test of whether sample data have the skewness and kurtosis matching a normal distribution. The test statistic JB is defined as
	$$JB = \frac{n-k-1}{6}(S^2 + \frac{1}{4}(C-3)^2),$$
where $n$ is the number of observations, $S$ is the sample skewness, $C$ is the sample kurtosis, and $k$ is the number of regressors (excluding the constant regressor). Note that for a normal distribution, $S = 0$ and $C = 3$. 	
\end{definition}

\begin{remark}
If the data comes from a normal distribution, the JB statistic asymptotically has a $\chi^2(2)$ distribution, so the statistic can be used to test the hypothesis that the data are from a normal distribution.
\end{remark}


\subsubsection{D'Agostino's $K^2$ test}

\begin{definition}[D'Agostino's $K^2$ test]
D'Agostino's $K^2$ test is a goodness-of-fit test of whether the sample data's distribution deviate from normality. The statistic $K^2$ is defined by
$$K^2 = Z_1(g_1)^2 + Z_2(g_2)^2,$$
where $Z_1, Z_2$ are some transformation functions and $g_1, g_2$ are the sample skewness and sample kurtosis.	
\end{definition}

\begin{remark}
	If the data comes from a normal distribution, the $K^2$ statistic asymptotically has a $\chi^2(2)$ distribution, so the statistic can be used to test the hypothesis that the data are from a normal distribution.
\end{remark}



\subsection{Autocorrelation of errors and models}
\subsubsection{Test of autocorrelation of errors}
\begin{definition}[Durbin-Watson test]\index{Durbin-Watson test}
	$$d = \frac{\sum_{t=2}^T (e_t - e_{t-1})^2}{\sum_{t=1}^T e_t^2}$$
	where $e_t = y_t - \hat{y}_t$.
	The decision rule is given as
	\begin{itemize}
		\item 	If $d < d_{L,\alpha}$, there is statistical evidence that the error terms are positively autocorrelated.
		\item 	If $d > d_{U,\alpha}$, there is no evidence of autocorrelation.
		\item 	If $d_{L,\alpha} < d < d_{U,\alpha}$, the test is inconclusive.
	\end{itemize}
\end{definition}

\begin{remark}[noise/error model]
	Durbin-Waston test assumes that the errors
	in the regression model are generated by AR(1) as, that is,
	$$e_{t} = \phi e_{t-1} + \eta,\eta\sim WN(0,\sigma^2),\abs{\phi}<1.$$
\end{remark}

\begin{remark}[interpretation]\hfill
	\begin{itemize}
		\item Durbin Watson test is a test that tests  autocorrelation in the residuals from a statistical regression analysis.
		\item The Durbin Watson statistic has the following approximation:
		\begin{align*}
		d &= \frac{\sum_{t=2}^T (e_t - e_{t-1})^2}{\sum_{t=1}^T e_t^2} \\
		  &= \frac{\sum_{t=2}^T e_t^2}{\sum_{t=1}^T e_t^2}+\frac{\sum_{t=2}^T e_t^2}{\sum_{t=1}^T e_t^2} - 2\frac{\sum_{t=2}^T e_te_{t-1}}{\sum_{t=1}^T e_t^2} \\
		  &\approx 1 + 1 - 2\frac{\sum_{t=2}^T e_te_{t-1}}{\sum_{t=1}^T e_t^2}\approx 2(1 - \phi).
		\end{align*}
		where $\phi$ is the AR(1) coefficient.
		\item The Durbin-Watson statistic is always between 0 and 4. A value of 2 means that there is no autocorrelation in the sample. Values approaching 0 indicate positive autocorrelation and values toward 4 indicate negative autocorrelation.
	\end{itemize}
\end{remark}

\begin{remark}[higher order model of errors]
	There are tests for higher order model of errors, such as Breusch-Pagan test.
\end{remark}


\subsubsection{Modeling autocorrelations}



\begin{definition}[linear regression model with AR(1) error]\cite[361]{hill2010principles}
The linear regression model for random variable $y_t$ depending on non-random observation $x$ and correlated error $e_t$ is given by 	
\begin{align*}
y_t &= \beta_0 + \beta_1 x_t + e_t \\
e_t & = \rho e_{t-1} + v_t
\end{align*}
where	
\begin{itemize}
	\item $-1< \rho < 1$
	\item $E[v_t] = 0, Var[v_t] = \sigma_v^2, Cov(v_s,v_t) = 0, \forall t\neq s$.
\end{itemize}
\end{definition}

\begin{remark}[View $y_t$ as a random variable and a random process]

\end{remark}


\begin{lemma}[equivalent form for auto-correlated noise model]\cite[361]{hill2010principles}
The model 	
\begin{align*}
y_t &= \beta_0 + \beta_1 x_t + e_t \\
e_t & = \rho e_{t-1} + v_t
\end{align*}
has the following equivalent forms:
\begin{itemize}
	\item $$y_t = \beta_0 + \beta_1 x_t + \sum_{i=0}^\infty \rho^i v_{t-i};$$
	\item $$(1 - \rho B)y_t = \beta_0 + \beta_1(1 - \rho B) x_t  + v_t,$$
	where $B$ is the lag operator;
	\item $$y_t = \beta_0(1 - \rho) + \beta_1 x_t + \rho y_{t-1} - \rho \beta_1 x_{t-1} + v_t.$$
\end{itemize} 
\end{lemma}
\begin{proof}
	(1) Note that 
\begin{align*}
	(1 - \rho B) e_t= v_t \implies e_t  &=(1 - \rho B)^{-1} v_t \\
	& =  \sum_{i=0}^\infty \rho^i v_{t-i}
\end{align*}
(2)(3) Multiply both sides by $(1 - \rho B)$ will get the result.	
\end{proof}


\begin{remark}[alternative models]
We can also model the correlated $e_t$ using MA(q) and ARMA(p,q) model.	
\end{remark}


\subsubsection{Autoregressive transformation}

\begin{lemma}[autoregressive transformation to weighted least square]\cite[254]{theil1971principles}

If the error term satisfying 
$$\epsilon_i = \rho \epsilon_{i-1} + \xi_i, i = 2,3,...,n$$
where $n$ is the number of samples, $\abs{\rho}\leq 1$ is \textbf{known} and $E[\xi_i] = 0, E[\xi_i,\xi_j] = \sigma_0^2\delta_{ij}$. Then
\begin{itemize}
	\item $$E[\epsilon_i] = 0$$
	\item $$E[\epsilon_i\epsilon_j] = \sigma_0^2 \frac{\rho^{\abs{i-j}}}{1- \rho^2}.$$
\end{itemize}
Or equivalently, the $Cov[\epsilon,\epsilon] = \sigma_0^2 V$, where
$$V = \begin{pmatrix}
1 & \rho & \rho^2 & \cdots & \rho^{n-1}\\ 
\rho & 1 & \rho & \cdots & \rho^{n-2} \\ 
\rho^2 & \rho & 1 &  & \rho^{n-3}\\ 
\vdots &  &  & \ddots & \\ 
\rho^{n-1} & \rho^{n-2} & \rho^{n-3} & \cdots & 1 
\end{pmatrix}$$
	
\end{lemma}
\begin{proof}
Use the basic statistical properties in \autoref{ch:time-series-analysis:th:AR(1)processBasicProperties}.
\end{proof}



\begin{remark}[property of $V$ matrix]\hfill
Here we list some properties of $V$, which will be useful when we do weighted least square.	
\begin{itemize}
	\item $$V^{-1} = \frac{1}{1-\rho^2}\begin{pmatrix}
	1 & -\rho & 0 & \cdots & 0 & 0\\ 
	-\rho & 1+\rho^2 & -\rho & \cdots & 0 & 0\\ 
	0 & -\rho & 1+\rho^2 &  & 0 & 0\\ 
	\vdots & \vdots & \vdots &  & \vdots & \vdots\\ 
	0 & 0 & 0 & \cdots & 1+\rho^2 & -\rho\\ 
	0 & 0 & 0 & \cdots & -\rho & 1
	\end{pmatrix}$$
	\item $$V^{-1} = P^TP, P =\frac{1}{\sqrt{1-\rho^2}} \begin{pmatrix}
	\sqrt{1-\rho^2} & 0 & 0 & \cdots & 0 & 0\\ 
	-\rho & 1 & 0 & \cdots & 0 & 0\\ 
	0 & -\rho & 1 & \cdots & 0 & 0\\ 
	\vdots &  &  &  &  & \\ 
	0 & 0 & 0 & \cdots & 1 & 0 \\ 
	0 & 0 & 0 & \cdots & -\rho & 1
	\end{pmatrix}$$
	
\end{itemize}	
	
	
\end{remark}



\begin{remark}[when $\rho$ is unknown]\cite[254]{theil1971principles}
If we do not know $\rho$, we can use least square to estimate  the correlation]\autoref{ch:time-series-analysis:th:leastSquareEstimationOfCorrelationAR(1)}.
		The correlation parameter $\rho$ as a solution to	
		$$\min_{\rho} \sum_{i=2}^N (e_i - \rho e_{i-1})^2$$
		is given by
		$$\hat{\rho} = \frac{\sum_{t=1}^{N-1} e_t e_{t+1}}{\sum_{t=1}^{N-1} e_t^2},$$
		where $e_i$ is the residue from the standard least square solution.	
\end{remark}



\subsection{Rank deficiency and penalized linear regression}

If the number of predicators is exceeding the number of observations, the ordinary linear regression will break down since the matrix $X$ does not have full column rank and $X^TX$ has rank deficiency. In such case, we can use penalized linear regression discussed in \autoref{ch:statistical-learning-linear-models:sec:PenalizedLinearRegression}.

\subsection{Outliers and Robust linear regression}

\subsubsection{Outliers and influential points}

\begin{definition}[outlier, high leverage point, influential point]\hfill
\begin{itemize}
	\item An \textbf{outlier} is a data point whose response y does not follow the general trend of the rest of the data.
	\item A data point has \textbf{high leverage} if it has "extreme" predictor x values.
	\item A data point is \textbf{influential} if it affects the predicted responses significantly via the estimated slope coefficients Outliers and high leverage data points have the potential to be influential, but not necessarily so.
\end{itemize}	
\end{definition}


\begin{definition}[leverage]
Let $H$ be the orthogonal projector matrix in the multiple linear regression. $H_{ii}$ is called the \textbf{leverage point} because $H_ii$ quantifies the influence that the observed response $y_i$ has on the predicted response $\hat{y}_i$ since
$$\hat{y}_i = H_{i1}y_1 + \cdots + H_{ii}y_i + \cdots + H_{in}y_n.$$	
\end{definition}

\begin{figure}[H]
	\centering
	\includegraphics[width=1\linewidth]{../figures/statisticalModeling/influencePointsLinearRegression}
	\caption{Illustration of an outlier, a high-leverage point, and a influential point. Left subfigure shows a red-colored outlier, which does not have high leverage and large influence on the regression result. Middle subfigure shows a red-colored high-leverage point, which is not an outlier or influential point due to its weak influence on the regression result. Right subfigure shows an influential point that is both an outlier and a high-leverage point.}
	\label{fig:influencepointslinearregression}
\end{figure}


\begin{note}[interpretation and usage of leverage to identify influential points]\hfill
\begin{itemize}
	\item That is, if $H_{ii}$ is small, then the observed response $y_i$ plays only a small role in the value of the predicted response $\hat{y}_i$; On the other hand, if $H_{ii}$ is large, then the observed response $y_i$ plays a large role in the value of the predicted response $\hat{y}_i$.
	\item $H_{ii}$ is between 0 and 1, inclusively; and $\sum_i H_{ii} = p$.(\autoref{ch:linearalgebra:th:spectralpropertyorthogonalprojector})
	\item If $$H_{ii} > 3\frac{\sum_i H_{ii}}{n} = 3\frac{p}{n},$$
	that is, $H_{ii}$ is more than 3 times larger than the mean leverage value, we identify the pair $(x_i,y_i)$ as the influential points. 
\end{itemize}	
\end{note}






\subsubsection{Robust regression}



\begin{lemma}[robust estimator]
The robust estimator for the parameter $\beta$ is obtained by solving the following optimization problem	
$$\min_{\beta \in \R^p} \sum_{i=1}^{n} \rho(e_i) = \min_{\beta} \sum_{i=1}^{n} \rho(y_i - x_i^T\beta),$$
where the function $\rho$ is related the likelihood function for an appropriate choice of the error distribution.

An alternative scale-invariant optimization formulation is given by
$$\min_{\beta \in \R^p} \sum_{i=1}^{n} \rho(\frac{e_i}{s}) = \min_{\beta} \sum_{i=1}^{n} \rho(\frac{y_i - x_i^T\beta}{s}),$$
where $s$ is a robust estimate of scale. A popular choice for $s$ is the median absolute deviation(\autoref{ch:theory-of-statistics:def:medianabsoluteDeviation})
$$s = median[e_i - median[e_i]]/0.6745.$$	
\end{lemma}


\begin{example}[example functions]\hfill
	\begin{itemize}
		\item (least square) $$\rho(z) = \frac{1}{2}z^2, \psi(z) = z.$$
		\item (Huber's t function)
		$$\rho(z) = 
		\begin{cases*}
		\frac{1}{2}z^2, \abs{z} \leq t \\
		0, otherwise
		\end{cases*}.$$
		\item (Cauchy)
		$$\rho(z) = \frac{1}{1+z^2}.$$ 
	\end{itemize}
\end{example}



\begin{lemma}[iteratively reweighted least square approach for parameter estimation]\cite[373]{montgomery2012introduction}
Consider a robust estimator by solving the following problem given by
$$\min_{\beta \in \R^p} \sum_{i=1}^{n} \rho(\frac{e_i}{s}) = \min_{\beta} \sum_{i=1}^{n} \rho(\frac{y_i - x_i^T\beta}{s}).$$
\begin{itemize}
	\item The first order necessary condition for the minimum is given by
	$$\sum_{i=1}^{n} x_{ij}\psi((y_i-x_i^T\beta)/s),$$
where $\psi = \rho'$ and $x_{ij}$ is the i observation on the j regressor and $x_{i0}=1$.	
\item (iterative reweighted algorithm) The iterative reweighted algorithm is formulated using the old estimated $\hat{\beta}_0$ and solve for the new iterate $\beta$ via
	$$\sum_{i=1}^{n} x_{ij}w_{i0}\cdot (y_i-x_i^T\beta) = 0, j=0,1,...,k; (*)$$
where 
$$w_{i0} = \begin{cases*}
\frac{\psi[(y_i-x_i^T\hat{\beta}_0)/s]}{(y_i-x_i^T\hat{\beta}_0)/s}, if~y_i\neq x_i^T\hat{\beta}_0 \\
1
\end{cases*}$$	

In matrix form, $(*)$ is given by
$$X^TW_0X\beta = X^TW_0y,$$
where $W_0$ is an $n\times n$ diagonal matrix of weights with diagonal elements $w_{10},w_{20},...,w_{n0}$. 
And the new minimizer iterate is given by
$$\hat{\beta}_1 = (X^TW_0X)^{-1}X^TW_0y.$$
\end{itemize}
\end{lemma}


\subsubsection{Least absolute deviation (LAD) regression}

\begin{definition}[multiple linear regression with least absolute deviations]\label{ch:statistical-models:th:multipleLinearRegressionLeastAbsoluteDeviation}	The multiple linear regression model \textbf{assumes} that a random variable $Y$ has a linear dependency on a non-random vector $X = (X_1,X_2,...,X_{p-1}) \in \R^{p-1}$ given as
	$$Y = \beta_0 + \beta_1 X_1 +\beta_2 X_2 + ... +\beta_{p-1} X_{p-1} + \epsilon.$$
	Given the observed sample pairs $(x_1,y_1),(x_2,y_2),..., (x_n,y_n), x\in \R^{p-1}, y\in \R$ as $y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + ... + \epsilon_i$ and we \textbf{further make the following assumptions on $\epsilon$} as
	\begin{itemize}
		\item $E[\epsilon_i] = 0,\forall i$
		\item $cov(\epsilon_i,\epsilon_j) = \sigma^2\delta_{ij}$ and $\sigma^2$ is unknown.
	\end{itemize} 	
	
	The least absolute deviation estimation of the coefficient $\beta_0,\beta_1,...,\beta_p$ is from the optimization
	$$\min_{\beta_0,\beta_1,...,\beta_p} \sum_{i=1}^{n} \abs{y_i - \beta_0 - \beta_1 x_{i1} - \beta_2 x_{i2} - \cdots -\beta_p x_{ip}}.$$
\end{definition}

\begin{remark}[motivation and issues with least absolute deviation optimization]\hfill
\begin{itemize}
	\item LAD regression is robust to outliers, whereas least square regression is not.
	\item LAD regression might give multiple solution due to its linear programming nature.
\end{itemize}	
\end{remark}


\begin{lemma}[solution via linear programming]
The optimization problem given by
$$\min_{a_0,a_1,...,a_k} \sum_{i=1}^{n} \abs{y_i - a_0 - a_1 x_{i1} - a_2 x_{i2} - \cdots -a_k x_{ik}},$$
can be transformed to the following linear programming problem
\begin{align*}
&\min_{a_0,a_1,...,a_k, u_1,...,u_n} \sum_{i=1}^{n} u_i ,\\
s.t. & u_i \geq y_i - a_0 -a_1x_{i1} -a_2x_{i2} - \cdots -a_kx_{ik}, i=1,...,n. \\
& u_i \geq -(y_i - a_0 -a_1x_{i1} -a_2x_{i2} - \cdots -a_kx_{ik}), i=1,...,n. 
\end{align*}
\end{lemma}
\begin{proof}
Todo
\end{proof}


\subsection{Quantile regression}\index{quantile regression}

\begin{lemma}[sample quantile as the solution to an optimization problem]\cite[87]{cameron2005microeconometrics} 
The sample $q$th quantile, denoted by $\hat{\mu}_q$, is also the solution to the following optimization problem
$$\min_{\beta \in \R} \sum_{i:y_i\geq \beta}^{N} q\abs{y_i-\beta} + \sum_{i:y_i < \beta}^{max}(1-q)\abs{y_i-\beta};$$
or equivalently
$$\min_{\beta \in \R} \sum_{i=1}^{N} (q-\bm{1}_{y_i<\beta})\abs{y_i-\beta}.$$

In particular, if $q=0.5$,i.e., median, then the median is the minimum of the following optimization
$$\min_{\beta \in \R} \sum_{i:y_i\geq \beta}^{N} \abs{y_i-\beta}.$$
\end{lemma}


\begin{remark}[the intuition for median]\hfill
\begin{itemize}
	\item 
	Suppose in a sample of 99 observations that the 50th smallest observations,i.e. the median, equals 10 and the 51st smallest observation equals 12. 
	\item 
	If we let $\beta = 12$, then the first 50 ordered observations will increase by 2 and the remaining 49 observations will decrease by 2, leading to an overall net increase of $$50\times 2 - 49\times 2 = 2.$$
	\item Similarly, the 49th smallest observation can be shown to be a worse choice compared with the 50th observation.
\end{itemize}	 	
\end{remark}

\begin{definition}\cite[87]{cameron2005microeconometrics} 
	The multiple linear regression model \textbf{assumes} that a random variable $Y$ has a linear dependency on a non-random vector $X = (X_1,X_2,...,X_{p-1}) \in \R^{p-1}$ given as
	$$Y = \beta_0 + \beta_1 X_1 +\beta_2 X_2 + ... +\beta_{p-1} X_{p-1} + \epsilon.$$
	Given the observed sample pairs $(x_1,y_1),(x_2,y_2),..., (x_n,y_n), x\in \R^{p-1}, y\in \R$ as $y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + ... + \epsilon_i$ and we \textbf{further make the following assumptions on $\epsilon$} as
	\begin{itemize}
		\item $E[\epsilon_i] = 0,\forall i$
		\item $cov(\epsilon_i,\epsilon_j) = \sigma^2\delta_{ij}$ and $\sigma^2$ is unknown.
	\end{itemize} 	
	
The $q$th \textbf{quantile multiple linear regression estimator $\hat{\beta}_q$} minimizes the following the optimization problem
$$\min_{\beta \in \R^q} \sum_{i:y_i\geq x_i^T\beta}^{N} q\abs{y_i-x_i^T\beta} + \sum_{i:y_i < \beta}^{max}(1-q)\abs{y_i-x_i^T\beta};$$
or equivalently
$$\min_{\beta \in \R^n} \sum_{i=1}^{N} (q-\bm{1}_{y_i<\beta^Tx_i})\abs{y_i-\beta^Tx_i}.$$

\end{definition}

\begin{remark}[special case of least absolute deviation estimator]
When $q = 0.5$, we will get the median, the regression problem becomes the least absolute deviation regression(\autoref{ch:statistical-models:th:multipleLinearRegressionLeastAbsoluteDeviation}). 
\end{remark}


\subsection{Regressor correlation with error}

\subsubsection{Problems with ordinary least square estimator}

\begin{lemma}[ordinary least square estimator is biased]
In the multiple linear regression, the ordinary least square estimator given by
$$\hat{\beta} = (X^TX)^{-1}X^TY,$$
will be biased if $Cov(X,\epsilon) > 0.$
That is
$$E[\hat{\beta}] \neq \beta.$$	
\end{lemma}
\begin{proof}
$$E[\hat{\beta}] = (X^TX)^{-1}X^T(X\beta+\epsilon) = \beta + (X^TX)^{-1}X^T\epsilon\neq \beta.$$	
\end{proof}

\subsubsection{Reasons leading to correlation}

\begin{remark}[correlation due to omitted variables]
\cite[407]{hill2008principles}
Suppose the true model between $Y$ and $X_1,X_2,...,X_n$ is given by
$$Y = \beta_1X_1+\beta_2X_2+...+\beta_nX_n + \epsilon,$$
where $\epsilon$ is independent of $Y$ and $X$s, and $X_1,X_2,...,X_n$ be correlated. If we propose a model of 
$$Y = \alpha_1X_1+\alpha_2X_2+...+\alpha_{n-1}X_{n-1} + \eta,$$
then the $\eta$ will be correlated with $X_1,X_2,...,X_n$.	
\end{remark}


\begin{remark}[correlation due to measurement error]\cite[406]{hill2008principles}
Suppose our true model is given by
$$Y = \beta_1 + \beta_2 X^* + \epsilon,$$
where $\epsilon$ is independent of $X$. Now suppose
our model due to measurement error is given by
$$Y = \beta_1 + \beta_2 X + \eta,$$
where $X = X^* + u$, $u$ is the measurement error independent of $X$, and $\eta = \epsilon - \beta_2 u$.

Then $$Cov(X,\eta) = Cov(X^*+u,\epsilon-\beta_2 u) = -\beta Var[u].$$	
\end{remark}



\subsubsection{Instrument variable estimator}

\begin{lemma}
$$\hat{\beta} = (Z^TX)^{-1}Z^TY$$
is unbiased.

$$Var[\hat{\beta}] = (Z^TX)^{-1}Z^T\Omega Z(Z^TX)^{-1}.$$
\end{lemma}
\begin{align*}
\hat{\beta} &= (Z^TX)^{-1}Z^TY \\
&=(Z^TX)^{-1}Z^T(X\beta + \epsilon) \\
&=(Z^TX)^{-1}Z^TX\beta + (Z^TX)^{-1}Z^T\epsilon \\
&=\beta + 0\\
&=\beta
\end{align*}



\begin{lemma}
The two-stage least squares estimator can be compactly written as
$$\hat{\beta} = (X^TP_ZX)^{-1}(X^TP_ZY),$$
where $$P_Z = Z(Z^TZ)^{-1}Z^T$$
is the orthogonal projectors such that $P_Z^T=P_Z, P_Z^2 = P_Z.$	
\end{lemma}




\section{Hypothesis Testing and Model selection}

\subsection{Hypothesis testing and analysis of variance}
\begin{mdframed}
	\textbf{notations}
	\begin{itemize}
		\item $SSE = \sum_{i=1}^n (y_i - \mean{y})^2$, where $\mean{y}$ is the sample mean of $y$ values.
		\item $MSE$
		\item $MSR$
		\item $S_{XX} = \sum_i (x_{i}-\mean{x})^2$, where $\mean{x}$ is the sample mean of $x$ values.
	\end{itemize}
\end{mdframed}

\subsubsection{Distribution of coefficients}

\begin{theorem}[distribution of coefficients in linear regression]\cite[93]{montgomery2012introduction}\label{ch:statistical-models:th:distributionOfCoefficientsLinearRegression}
	Assume $\epsilon \sim MN(0,\sigma^2 I)$. Then we have
	\begin{itemize}
		\item For multiple linear regression with $\sigma$ known, $$\hat{\beta} \sim MN(\beta,\sigma^2 X^TX).$$
		\item If $\sigma^2$ is known, then
		$$\frac{\hat{\beta}_j - \beta_j}{\sqrt{\sigma^2C_{jj}}} \sim N(0,1), j=0,1,...,p-1;$$
		if $\sigma^2$ is unknown, then
		$$\frac{\hat{\beta}_j - \beta_j}{\sqrt{\hat{\sigma}^2C_{jj}}}\sim t(n-p),j=0,1,...,p-1,$$
		where $C_{jj}$ is the j diagonal element of $(X^TX)^{-1}$, and $$\hat{\sigma}^2 = \frac{SSE}{n-p}.$$
		\item For simple linear regression with $\sigma$ known, $$\hat{\beta}_1 \sim N(\beta_1,\sigma^2/S_{XX}),$$ where
		$S_{XX} = \sum_{i=1}^n (x_i - \mean{x})^2.$
		\item For simple linear regression with $\sigma$ unknown, $$  \frac{\hat{\beta}_1 - \beta_{1}}{\sqrt{\hat{\sigma}^2/S_{XX}}} = \sqrt{(n-2)S_{XX}}\frac{\hat{\beta}_1 - \beta_{1}}{SSE} = \sim t(n-2),$$
		where $t(n-2)$ is the standard $t$ distribution with $n-2$ degree of freedom, $E[\hat{\beta}_1] = \beta_1$ and 
		$$\hat{\sigma}^2 = \frac{SSE}{n-2}.$$
	\end{itemize}	
\end{theorem}
\begin{proof}
	(1)When $\epsilon \sim MN(0,\sigma^2 I)$, then $Cov(\hat{\beta}) = Cov(HY) = \sigma^2 (X^TX)^{-1}$(\autoref{ch:theory-of-statistics:th:propertylinearregression}). Since $\hat{\beta}$ is the affine transformation of multivariate Gaussian $Y$, $\hat{\beta}$ is also multivariate Gaussian.\\
	Note that 
	$$\hat{\beta}_1 = \frac{\sum_i (x_{i}-\mean{x})(y_i - \mean{y})}{\sum_i (x_{i}-\mean{x})^2}  $$
	and
	$$Var[\hat{\beta}_1] = \sum_{i=1}^n \frac{ (x_{i}-\mean{x})^2}{(\sum_i (x_{i}-\mean{x})^2)^2} Var[y_i] = \sigma^2 \frac{S_{xx}}{S_{xx}^2}= \sigma^2 \frac{1}{S_{xx}}$$
	where $S_{xx} = \sum_i (x_{i}-\mean{x})^2$.
	Therefore, $\hat{\beta}_1 \sim N(\beta_{1}, \sigma^2/S_{XX})$.\\
	(2) Note that
	$$\hat{\beta}_i/\sqrt{\sigma^2 C_{ii}} \sim N(0,1),$$
	(3) Note that $$\frac{SSE}{\sigma^2}\sim \chi^2(n-2).$$
	therefore
	$$ \frac{\hat{\beta}_1 - \beta_{1}}{\sqrt{\sigma^2/S_{XX}}}/\sqrt{\frac{SSE}{\sigma^2(n-2)}} = \sqrt{(n-2)S_{XX}}\frac{\hat{\beta}_1 - \beta_{1}}{SSE} \sim t(n-2)$$
	based of t distribution definition(\autoref{ch:theory-of-statistics:def:tdistribution}).
\end{proof}

\subsubsection{t test and normality test of single coefficients}



\begin{lemma}[t-Test of regression slope for simple linear regression]\cite[261]{chinese2008probability}
	\textbf{Assume $\sigma$ is unknown.}	Assume $\epsilon \sim MN(0,\sigma^2 I)$. Consider the hypothesis that the slope of a regression line :
	\begin{align*}
	&H_0: \beta_1 = \beta_{10} (~often ~ \beta_{10} = 0)\\
	&H_1: \beta_1 \neq \beta_{10}
	\end{align*}
	And t-test statistic is
	$$t_0 = \sqrt{(n-2)S_{XX}}\frac{\hat{\beta}_1 - \beta_{10}}{SSE}$$
	is a Student t-distribution with $n-2$ degrees of freedom. 
\end{lemma}
\begin{proof}
	\autoref{ch:statistical-models:th:distributionOfCoefficientsLinearRegression}.
\end{proof}

\begin{lemma}[t-Test of single coefficient for multiple linear regression]\cite[261]{chinese2008probability}
	\textbf{Assume $\sigma$ is unknown.}	Assume $\epsilon \sim MN(0,\sigma^2 I)$. Consider the hypothesis that the slope of a regression line :
	\begin{align*}
	&H_0: \beta_1 = \beta_{10} (~often ~ \beta_{10} = 0)\\
	&H_1: \beta_1 \neq \beta_{10}
	\end{align*}
	And t-test statistic:
	$$t_0 = \frac{\hat{\beta}_j - \beta_j}{\sqrt{\hat{\sigma}^2C_{jj}}} $$
	where $C_{jj}$ is the j diagonal element of $(X^TX)^{-1}$, and $$\hat{\sigma}^2 = \frac{SSE}{n-p},$$
	
	is a Student t-distribution with $n-p$ degrees of freedom. 
\end{lemma}
\begin{proof}
	\autoref{ch:statistical-models:th:distributionOfCoefficientsLinearRegression}.
\end{proof}

\begin{lemma}[normality-Test of regression slope for simple linear regression]\cite[261]{chinese2008probability}
	\textbf{Assume $\sigma$ is known.}	Assume $\epsilon \sim MN(0,\sigma^2 I)$. Consider the hypothesis that the slope of a regression line :
	\begin{align*}
	&H_0: \beta_1 = \beta_{10} (~often ~ \beta_{10} = 0)\\
	&H_1: \beta_1 \neq \beta_{10}
	\end{align*}
	And the normality-test statistic is
	$$N_0 = \frac{\hat{\beta}_1 - \beta_{10}}{\sqrt{\sigma^2/S_{XX}}}$$
	is a standard normal distribution $N(0,1)$. 
\end{lemma}
\begin{proof}
	\autoref{ch:statistical-models:th:distributionOfCoefficientsLinearRegression}.
\end{proof}

\begin{lemma}[normality-Test of single coefficient for multiple linear regression]\cite[261]{chinese2008probability}
	\textbf{Assume $\sigma$ is known.}	Assume $\epsilon \sim MN(0,\sigma^2 I)$. Consider the hypothesis that the slope of a regression line :
	\begin{align*}
	&H_0: \beta_1 = \beta_{10} (~often ~ \beta_{10} = 0)\\
	&H_1: \beta_1 \neq \beta_{10}
	\end{align*}
	And the normality test statistic is
	$$N_0 = \frac{\hat{\beta}_j - \beta_{10}}{\sqrt{\sigma^2/C_{jj}}}$$
	is a standard normal distribution $N(0,1)$,	where $C_{jj}$ is the j diagonal element of $(X^TX)^{-1}$,
\end{lemma}
\begin{proof}
	\autoref{ch:statistical-models:th:distributionOfCoefficientsLinearRegression}.
\end{proof}

\subsubsection{$\chi^2$ test}



\subsubsection{F lack-of-fit test}


\begin{lemma}[F-Test of simple regression slope]
	\textbf{Assume $\sigma$ is unknown.}	
	The hypothesis that the slope of a regression line a constant, $\beta_{10}$:
	\begin{align*}
	&H_0: \beta_1 = \beta_{10} (~often ~ \beta_{10} = 0)\\
	&H_1: \beta_1 \neq \beta_{10}
	\end{align*}
	We have:\\
	(1)
	The F-test statistic is
	$$F_0 = \frac{\sum_{i=1}^n (\hat{y}_i - \mean{y})}{SSR/(n-2)} = \frac{SSRegression}{SSR/(n-2)}$$
	is a F-distribution with $(1,n-2)$ degrees of freedom.\\
	(2)  $$E[\sum_{i=1}^n (\hat{y}_i - \mean{y})^2] = \sigma^2 + \beta_1^2 \sum_{i=1}^n (x_i - \mean{x})^2 = \sigma^2 + \beta_1^2S_{XX}, E[\frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{n-2}]=\sigma^2$$
\end{lemma}
\begin{proof}
	(1) From \autoref{ch:theory-of-statistics:th:variancedecompositionlinearregression},  we know that, under null hypothesis,  $SSRegression \sim \chi^2(1), SSR\sim \chi^2(n-2)$. \\
	(2) 
	\begin{align*}
	E[\sum_{i=1}^n (\hat{y}_i - \mean{y})^2] &= E[\sum_{i=1}^n (\hat{\beta_1} (x_i - \mean{x})^2]\\
	&=\sum_{i=1}^n (x_i - \mean{x})^2E[\hat{\beta_1}^2] \\
	&= S_{XX} E[\hat{\beta}^2_1] \\
	&= S_{XX}(Var[\hat{\beta}_1^2] + E[\hat{\beta}_1]^2) \\
	&= S_{XX}(\sigma^2/S_{XX} + \beta_1) \\
	&= \sigma^2 + \beta_1^2S_{XX}.
	\end{align*}
	
\end{proof}

\begin{remark}[Interpretation and hypothesis testing procedure]\hfill
	\begin{itemize}
		\item If $H_0$ is true, then $\beta_0 = 0 $, then $F_0 \approx 1$.
		\item If $\beta_0 \neq 0$, $F_0$ tends to have large values, then $H_0$ should be rejected. 
	\end{itemize}
\end{remark}



\begin{lemma}[lack-of-fit in F-Test of multiple regression slope]\cite[80]{montgomery2012introduction}\label{ch:statistical-models:th:LackOfFitFtestMultipleRegression}
	\textbf{Assume $\sigma$ is unknown.}
	The hypothesis that if there is a linear relationship between response $y$ and any of the regressor variables($x_1,x_2,...,x_{p-1}$) is given by:
	\begin{align*}
	&H_0: \beta_1 = \beta_2 = ... = \beta_{p-1} = 0\\
	&H_1: \text{at least one }\beta_j \neq 0
	\end{align*}
	We have:\\
	(1)
	The F-test statistic is
	$$F_0 = \frac{\sum_{i=1}^n (\hat{y}_i - \mean{y})^2/p-1}{\sum_{i=1}^n (y_i-\hat{y}_i)/(n-p)} = \frac{SSRegression/p-1}{SSR/(n-p)}$$
	is a F-distribution with $(p-1,n-p)$ degrees of freedom.\\
	(2)  $$E[\sum_{i=1}^n (\hat{y}_i - \mean{y})^2] = (p-1)\sigma^2 + \beta^T(X-\mean{X})^T(X-\mean{X})\beta, E[\frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{n-2}]=\sigma^2$$ 
	(where $\hat{\beta}$ not includes $\hat{\beta}_0$)
\end{lemma}
\begin{proof}
	(1)  From \autoref{ch:theory-of-statistics:th:variancedecompositionlinearregression}, we know that, under null hypothesis,  $$SSRegression \sim \chi^2(p-1), SSR\sim \chi^2(n-p).$$ 
	(2)	Use the fact that
	$$\hat{y}_i - \mean{y} =\sum_{i=1}^{p-1} \hat{\beta}_i (x_i-\mean{x}) $$
	and 
	$$E[\sum_{i=1}^n (\hat{y}_i - \mean{y})^2]=E[\hat{\beta}^TX^TX\hat{\beta}^T] = E[(\hat{\beta}-\beta)^TX^TX(\hat{\beta}-\beta)] + E[\hat{\beta}]^TX^TXE[\hat{\beta}]^T = \sigma^2 + \beta X^TX \beta^T.$$
	Also note that
	$$E[(\hat{\beta}-\beta)^TX^TX(\hat{\beta}-\beta)] = E[Tr[X^TX(\hat{\beta}-\beta)(\hat{\beta}-\beta)^T]] = \sigma^2$$
	where $X$ is demean data matrix and cycle rule of trace is used(\autoref{appendix:th:matrixtraceproperty}).
\end{proof}

\subsection{Model selection methods}
\subsubsection{Adjusted R square method}
\begin{definition}[adjusted R square]
	
	$$R^2_{adj} = 1 - \frac{(1-R^2)(N-1)}{N-K-1},$$
	where \begin{itemize}
		\item $N$ is the number of points in the data sample.
		\item $K$ is the number of independent regressors, excluding the constant.
	\end{itemize}	
\end{definition}


\begin{remark}[interpretation]\hfill
	\begin{itemize}
		\item Increasing number of regressors will increase $R^2$; however, it might decrease $\R^2_{adj}$.
		\item We can choose the optimal number of regressors that has the maximum $\R_{adj}^2$.
	\end{itemize}	
\end{remark}

\subsubsection{F test method}
\begin{theorem}[F-Test of different linear models]
	Consider the hypothesis that a smaller model is better: 
	\begin{align*}
	&M_0(small~model, null~hypothesis): y = \beta_0 + \beta_1x_1 + ... + \beta_{q}x_q, \beta_{q+1} = ... = \beta_{p-1} = 0\\
	&M_1(large~model, alternative~hypothesis): \beta_j \neq 0,\forall j=1,...,p.
	\end{align*}
	We have:\\
	(1)  $$SSR(M_1)\sim \sigma^2\chi^2(n-p),SSR(M_0) - SSR(M_1) \sim \sigma^2\chi^2(n-p).$$
	(2)
	The F-test statistic is
	$$F_0 = \frac{(SSR(M_0)-SSR(M_1))/(p-q)}{SSR(M_1)/(n-p)}$$
	is a F-distribution with $(p-q,n-2)$ degrees of freedom.\\
	
\end{theorem}
\begin{proof}
	Note that $$SSR(M_0) - SSR(M_1) = Y^T(I-H_0)Y - Y^T(I-H_1)Y = Y^T(H_1-H_0)Y,$$
	where $H_0$ and $H_1$ are the hat matrix associated with the model $M_0$ and $M_1$. Then we can show that
	$$(H_1-H_0)^2=H_1^2-H_1H_0-H_0H_1 + H_0^2 = H_1 - 2H_0 + H_0 = H_1-H_0,$$
	therefore $H_1-H_0$ is a orthogonal projector with rank equals $Tr(H_1-H_0) = Tr(H_1)-Tr(H_0) = p-q$. Then, we can use Cochran's theorem(\autoref{ch:theory-of-statistics:th:cochrantheorem}).
\end{proof}



\begin{remark}[Decision rule and implications]\hfill
	\begin{itemize}
		\item If $H_0$ is true, then $F_0 \approx 0$. So we will reject $F_0$ when $F_0$ exceeds certain critical value(for example, 95\% percentile of the F-distribution).
		\item Using larger model will always reduce the residual; however, if the error reduction is insignificant, the increase $p$ will make $F_0$ smaller via $$\frac{n-p}{p-q},$$ indicating that smaller model is better. On the other hand, if $n$ is large, then we are more easy to accept larger model. 
	\end{itemize}	
\end{remark}


\begin{example}[testing all parameters are zero]
	Consider the 3-parameter full linear regression model given by
	$$y_i = (\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3}) + \epsilon_i;$$
	And consider the following hypothesis
	\begin{itemize}
		\item $M_0: \beta_1 = \beta_2 = \beta_3 = 0$.
		\item $M_1: at~least~one~\beta_j \neq 0, j=1,2,3$.
	\end{itemize}
	Then the statistic
	$$F = \frac{(SSE(M_0) - SSE(M_1))/(3)}{SSE(M_1)/(n-4)} = \frac{(SST - SSE(M_1))/(3)}{SSE(M_1)/(n-4)} =  \frac{(SSRegression)/(3)}{SSE(M_1)/(n-4)},$$
	which recovers results in \autoref{ch:statistical-models:th:LackOfFitFtestMultipleRegression}.	
\end{example}

\begin{example}[testing one slope parameters is zero]
	Consider the 3-parameter full linear regression model given by
	$$y_i = (\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3}) + \epsilon_i;$$
	And consider the following hypothesis
	\begin{itemize}
		\item $M_0: \beta_1 =  0$.
		\item $M_1: \beta_1 \neq 0$.
	\end{itemize}
	Then the statistic
	$$F = \frac{(SSE(M_0) - SSE(M_1))/(1)}{SSE(M_1)/(n-4)}$$
	which has F-distribution with $(1,n-4)$ degrees of freedom. 	
\end{example}


\subsubsection{Information criterion methods}
\begin{definition}[Akaike's Infomration Criterion(AIC)]\index{Akaike's Infomration Criterion(AIC)}
	The Akaike's Infomration Criterion(AIC) is defined as
	$$AIC = \log(\hat{\sigma}_k^2) + \frac{n+2k}{n}$$
	where $\hat{\sigma}_k^2 = SSR/(n-k)$ and $k$ is the number of parameters in the model.
\end{definition}

\begin{definition}[Bayesian information criterion(BIC)]\index{Bayesian Infomration Criterion(BIC)}
	The Bayesian Infomration Criterion(BIC)(\autoref{ch:theory-of-statistics:th:BICFormultipleLinearRegression}) is defined as
	$$BIC = \log(\hat{\sigma}_k^2) + \frac{k\log(n)}{n}$$
	where $\hat{\sigma}_k^2 = SSR/(n-k)$ and $k$ is the number of parameters in the model.
\end{definition}

\begin{remark}[decision rule]\hfill
	\begin{itemize}
		\item We will choose the model that maximize the BIC value.
		\item If $n$ is large, then we tend to choose larger model.
		\item When $n$ is fixed, increase $k$ will decrease the first term but increase the second term.
	\end{itemize}
\end{remark}


\section{Model mis-specification}
\subsection{Omission of relevant regressors}


\begin{lemma}[least square estimator with omission of relevant regressors]
Suppose the correct model is given by
$$y = X_1\beta_1 + X_2\beta_2 + \epsilon,$$
where $X_1,X_2$ are the data matrices.
But the assumed model is $y = X_1\beta_1 + \epsilon$. 
Suppose all other assumptions in \autoref{ch:regression-analysis:assumption:LinearRegressionStandardAssumption} hold.
Then
\begin{itemize}
	\item The least square estimator is biased and given by$$\hat{\beta}_1 = \beta_1 +(X_1^TX_1)^{-1}X_1^TX_2.$$
	\item Let $\hat{\beta}_1*$ be the least square estimator of coefficient $\beta_1$ with correctly specified model. Then 
	 $$\hat{\beta}_1* = (X_1^TM_2X_1)^{-1}X_1M_2Y,$$
	where $M_2 = I - H_2, H_2 = X_2(X_2^TX_2)^{-1}X_2^T$.
	\item Small model gives a biased estimator with smaller variance; that is,
	$$\sigma^2(X_1^TX_1)^{-1} = Var[\hat{\beta}_1] \leq Var[\hat{\beta}_1*] = \sigma^2(X_1^TM_2X_1)^{-1}.$$
\end{itemize}	
\end{lemma}
\begin{proof}
(1)	
\begin{align*}
\hat{\beta_1} &= (X_1^TX_1)^{-1}X_1^TY \\
              &= (X_1^TX_1)^{-1}X_1^T(X_1\beta_1 + X_2\beta_2 + \epsilon) \\
              &= \beta_1 + (X_1^TX_1)^{-1}X_1^TX_2\beta_2 + (X_1^TX_1)^{-1}X_1^T\epsilon \\
              &= \beta_1 + (X_1^TX_1)^{-1}X_1^TX_2\beta_2
\end{align*}	
(2) From \autoref{ch:regression-analysis:th:LinearRegressionPartialRegressionFrisch-Waugh-LovellTheorem}.
(3) 
\begin{align*}
& Var[\hat{\beta}_1]^{-1} - Var[\hat{\beta}_1*]^{-1} \\
& = \frac{1}{\sigma^2}(X_1^TX_1 - X_1^TM_2X_1) \\
& = \frac{1}{\sigma^2}(X_1^T(I-M_2)X_1) \\
& = \frac{1}{\sigma^2}(X_1^TH_2X_1) \\
& \geq 0
\end{align*}
where it is clear that $$(X_1^TH_2X_1) = (X_1^TH_2^TH_2X_1) = (H_2X_1)^T(H_2X_1)\geq 0.$$
\end{proof}


\begin{example}[a two-variable regression problem]
	
\end{example}


\subsection{Inclusion of irrelevant regressors}

\begin{lemma}[least square estimator with inclusion of relevant regressors]
	Suppose the correct model is given by
	$$Y = X_1\beta_1 + \epsilon,$$
	where $X_1$ is the data matrix.
	But the assumed model is $Y = X_1\beta_1 + X_2\beta_2 + \epsilon$, where the data generation process for $X_2$ is independent/irrevelent to $X_1, Y, \epsilon$. 
	Suppose all other assumptions in \autoref{ch:regression-analysis:assumption:LinearRegressionStandardAssumption} hold.
	Then
	\begin{itemize}
		\item The least square estimator is given by $$\hat{\beta}_1 = (X_1^TM_2X_1)^{-1}X_1^TM_2Y,$$
		where $M_2 = I - H_2, H_2 = X_2(X_2^TX_2)^{-1}X_2^T$. And it is unbiased
		$$E[\hat{\beta}_1] = \beta_1.$$
		\item The variance of the estimator is same as the variance of estimator in a correctly specified model. That is
		$$Var[\hat{\beta}_1] = \sigma^2 E[(X_1^TM_2X_1)^{-1}] = \sigma^2. $$		
	\end{itemize}	
\end{lemma}
\begin{proof}
(1) From \autoref{ch:regression-analysis:th:LinearRegressionPartialRegressionFrisch-Waugh-LovellTheorem}.
$$E[\hat{\beta}_1] = E[(X_1^TM_2X_1)^{-1}X_1^TM_2Y] = E[(X_1^TX_1)^{-1}X_1^T(X_1\beta_1 + \epsilon)] = \beta_1.$$
where we use the fact that $E[M_2Y] = Y, E[M_2X_1] = X_1$ (the expectation is taken with respect to $X_2$ distribution) since the data generation process for $X_2$ is independent/irrevelent to $X_1, Y, \epsilon$. 
(2) Similar to (1) that $E[M_2X_1] = X_1$ (the expectation is taken with respect to $X_2$ distribution).
\end{proof}



\section{Linear regression case studies}

\subsection{Standard linear regression}



\begin{verbatim}
OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.878
Model:                            OLS   Adj. R-squared:                  0.877
Method:                 Least Squares   F-statistic:                     707.4
Date:                Tue, 30 Oct 2018   Prob (F-statistic):           1.29e-46
Time:                        23:08:39   Log-Likelihood:                 77.109
No. Observations:                 100   AIC:                            -150.2
Df Residuals:                      98   BIC:                            -145.0
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                coef      std err       t        P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          1.9841      0.022     88.410      0.000       1.940       2.029
x1             1.0312      0.039     26.596      0.000       0.954       1.108
==============================================================================
Omnibus:                        0.886   Durbin-Watson:                   2.028
Prob(Omnibus):                  0.642   Jarque-Bera (JB):                0.914
Skew:                           0.080   Prob(JB):                        0.633
Kurtosis:                       2.560   Cond. No.                         4.35
==============================================================================
\end{verbatim}

basic information about the model fit:
\begin{description}
	\item[Dep. Variable] Which variable is the response in the model
	\item[Model]	What model you are using in the fit
	\item[Method]	How the parameters of the model were calculated
	\item[No. Observations]	The number of observations (examples)
	\item[DF Residuals]	Degrees of freedom of the residuals. Number of observations - number of parameters
	\item[DF Model]	Number of parameters in the model (not including the constant term if present)
\end{description}	

 the goodness of fit
\begin{description}
	\item[R-squared] The coefficient of determination. A statistical measure of how well the regression line approximates the real data points
	\item[Adj. R-squared]	The above value adjusted based on the number of observations and the degrees-of-freedom of the residuals
	\item[F-statistic]	A measure how significant the fit is. The mean squared error of the model divided by the mean squared error of the residuals
	\item[Prob (F-statistic)]	The probability that you would get the above statistic, given the null hypothesis that they are unrelated
	\item[Log-likelihood]	The log of the likelihood function.
	\item[AIC]	The Akaike Information Criterion. Adjusts the log-likelihood based on the number of observations and the complexity of the model.
	\item[BIC]	The Bayesian Information Criterion. Similar to the AIC, but has a higher penalty for models with more parameters.
\end{description}	

each of the coefficients
\begin{description}
	\item[coef]	The estimated value of the coefficient
	\item[std err]	The basic standard error of the estimate of the coefficient. More sophisticated errors are also available.
	\item[t]	The t-statistic value. This is a measure of how statistically significant the coefficient is.
	\item[P > |t|]	P-value that the null-hypothesis that the coefficient = 0 is true. If it is less than the confidence level, often 0.05, it indicates that there is a statistically significant relationship between the term and the response.
	\item[95.0\% Conf. Interval]	
	The lower and upper values of the 95\% confidence interval
\end{description}	
	
	
	Finally, there are several statistical tests to assess the distribution of the residuals
	
\begin{description}
	\item[Skewness]	A measure of the symmetry of the data about the mean. Normally-distributed errors should be symmetrically distributed about the mean (equal amounts above and below the line).
	\item[Kurtosis]	A measure of the shape of the distribution. Compares the amount of data close to the mean with those far away from the mean (in the tails).
	\item[Omnibus]	D'Angostino's test. It provides a combined statistical test for the presence of skewness and kurtosis.
	\item[Prob(Omnibus)]	The above statistic turned into a probability
	\item[Jarque-Bera]	A different test of the skewness and kurtosis
	\item[Prob (JB)]	The above statistic turned into a probability
	\item[Durbin-Watson]	A test for the presence of autocorrelation (that the errors are not independent.) Often important in time-series analysis
	\item[Cond. No]	A test for multicollinearity (if in a fit with multiple parameters, the parameters are related with each other).
\end{description}


\section{Linear regression application examples}


\subsection{Categories of analysis}
\subsubsection{Time series data}

\begin{definition}[time series data]\index{time series data}
Time series data has the following characteristics:
\begin{itemize}
	\item Observations of features (weight, height, revenue, population, GDP) of a single entity (person, firm, country) are collected at multiple time periods.
	\item Order of the data is important and observations are typically not independent over time.
\end{itemize}		
\end{definition}


\begin{example}
Examples of time series data include:
\begin{itemize}
	\item GDP data of China from 1978 to 2008.
	\item Price levels (CPI) of the US from 1900 to 2016.
	\item Daily value of SP500  index from 1990 to 2018.  
\end{itemize}
\end{example}


\subsubsection{Cross-section data}


\begin{definition}[Cross-sectional data]
	Panel data has the following characteristics:
	\begin{itemize}
		\item Observations of features of multiple entities (individuals,firms, countries) at the same point of time (or observed at different point of time but the time difference is not important).
		\item No time dimension, and order of the data does not matter.
	\end{itemize}	
\end{definition}

\begin{example}
	Examples of cross-sectional data include:
	\begin{itemize}
		\item The weights and heights data from a random sample of 1000 people which is used to analyze the obesity level in a population. 
	\end{itemize}
\end{example}



\subsubsection{Panel data analysis}

\begin{definition}[panel data]
Panel data has the following characteristics:
\begin{itemize}
	\item Observations of features of multiple entities (individuals,firms, countries) at multiple points in time.
	\item Can be viewed as the combination of time series data and cross-sectional data.
\end{itemize}	
\end{definition}

\begin{example}
	Examples of panel data include:
	\begin{itemize}
		\item The daily stock price data of 500 preselected NYSE companies from 1990 to 2016.
		\item Price levels (CPI) of G8 countries from 1900 to 2016.
	\end{itemize}
\end{example}


\subsection{Cross-section analysis}



\subsection{Panel data analysis}



\section{Generalized linear model}
\subsubsection{Poisson regression}

\begin{definition}[Poisson regression]
In \textbf{Poisson regression}, the dependent variable $Y$ is an observed count that follows the Poisson distribution.
$$Pr(Y=y|\lambda) = \frac{\exp(-\lambda)\lambda^y}{y!},$$
for $y=0,1,2,...$
 The rate $\lambda$ is determined by a set $p-1$ predictors $X_1,X_2,...,X_{p-1}$. The expression relating these quantities is
 $$\lambda = \exp(X^T\beta),$$
where $X=(1,X_1,X_2,...,X_{p-1}).$ 	
\end{definition}


\begin{lemma}[likelihood function]
Consider a set of random sample given by $(X_1,y_1),(X_2,y_2),...,(X_n,y_n)$.
the likelihood function is given by
$$L(\beta;y,X) = \prod_i^n \frac{\exp(-\exp(X_i^T\beta))\exp(X_i^T\beta)^y_i}{y_i!}.$$

The log-likelihood function is given by
$$l(\beta;y,X) = \sum_{i=1}^n y_iX_i^T\beta - \sum_{i=1}^n \exp(X_i\beta) - \sum_{i=1}^{n}\log(y_i!).$$	
\end{lemma}



\begin{lemma}[goodness-of-fit chi-square test]
Consider a set of random sample given by $(X_1,y_1),(X_2,y_2),...,(X_n,y_n)$.
Let $\hat{\beta}$ be the estimated coefficient.
The \textbf{Person statistic} defined by
$$X^2 = \sum_{i=1}^{n}=\frac{y_i - \exp(X_i^T\hat{\beta})^2}{\exp(X_i\hat{\beta})},$$
is approximately chi-square distributed with $n-p$ degrees of freedom.	
\end{lemma}
\begin{proof}
Use Pearson's theorem in \autoref{ch:theory-of-statistics:th:PearsonTheorem}. To do: show that how $n-p$ arises?
\end{proof}







\section{Notes on Bibliography}

For linear regression models, see \cite{kutner2003applied}\cite{seber2012linear}. For, linear models with $R$ resources, see \cite{faraway2014linear}.


For multivariate statistical analysis, see \cite{johnson2007applied}\cite{anderson2009introduction}.

For copula, see \cite{Ruschendorf2013mathematical}\cite{lindskog2000modelling}\cite{mcneil2015quantitative}\cite{cherubini2004copula}.


Computation software and libraries include:
\begin{itemize}
	\item Python linear model estimation library, \href{https://bashtage.github.io/linearmodels/doc/}{linearmodels}.
\end{itemize}

\printbibliography
\end{refsection}
